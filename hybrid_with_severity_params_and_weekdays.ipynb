{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/I88N-processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample dates, split train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_day(date):\n",
    "    # date: a date string in the format of \"yyyy-mm-dd\"\n",
    "    # return: an int w/t Monday being 0 and Sunday being 6.\n",
    "    if date.find('-') != -1:\n",
    "        y, m, d = date.split('-')\n",
    "    else:\n",
    "        m, d, y = date.split('/')\n",
    "    return dt.datetime(int(y), int(m), int(d)).weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.read_csv(base_path + 'available_dates.csv')\n",
    "dates = np.array(dates['0'].values.tolist())\n",
    "dates = np.array(list(map(lambda x: x.split('-')[1] + '/' + x.split('-')[2] + '/' + x.split('-')[0], dates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to sample dates in June."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = dates[(dates > '05/31/2017') & (dates < '07/01/2017')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_weekend = [date for date in dates if date_to_day(date) >= 5]\n",
    "dates_weekday = list(set(dates).difference(set(dates_weekend)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 21)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dates_weekend), len(dates_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_train = dates_weekday[:14] + dates_weekend[:3]\n",
    "dates_test = dates_weekday[14:] + dates_weekend[3:]\n",
    "dates_train.sort()\n",
    "dates_test.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,\n",
       " 17,\n",
       " 12,\n",
       " ['06/01/2017', '06/02/2017', '06/03/2017'],\n",
       " ['06/05/2017', '06/07/2017', '06/11/2017'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dates), len(dates_train), len(dates_test), dates_train[0:3], dates_test[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading severity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_data = pd.read_csv(base_path + 'severity_params.csv')\n",
    "severity_data.rename(columns={'Unnamed: 0':'datetime'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sev_datetimes = severity_data['datetime'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sev_dates = []\n",
    "sev_times = []\n",
    "for x in sev_datetimes:\n",
    "    d, t = x.split(' ')\n",
    "    sev_dates.append(d)\n",
    "    sev_times.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_data['Date'] = sev_dates\n",
    "severity_data['Time'] = sev_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "severity_data = severity_data.loc[~severity_data['Date'].isin(['2017-06-15'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_max = severity_data['LambdaMax'].values\n",
    "sigma = severity_data['Sigma'].values\n",
    "tau = severity_data['Tau'].values\n",
    "impact = severity_data['Impact'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>ID</th>\n",
       "      <th>LambdaMax</th>\n",
       "      <th>Sigma</th>\n",
       "      <th>Tau</th>\n",
       "      <th>Impact</th>\n",
       "      <th>Incident</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-01 00:00:00</td>\n",
       "      <td>408907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-06-01 00:05:00</td>\n",
       "      <td>408907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>00:05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-06-01 00:10:00</td>\n",
       "      <td>408907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>00:10:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime      ID  LambdaMax  Sigma  Tau    Impact  Incident  \\\n",
       "0  2017-06-01 00:00:00  408907        NaN    NaN  NaN  0.021267       0.0   \n",
       "1  2017-06-01 00:05:00  408907        NaN    NaN  NaN  0.017058       0.0   \n",
       "2  2017-06-01 00:10:00  408907        NaN    NaN  0.0  0.015338       0.0   \n",
       "\n",
       "         Date      Time  \n",
       "0  2017-06-01  00:00:00  \n",
       "1  2017-06-01  00:05:00  \n",
       "2  2017-06-01  00:10:00  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "severity_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_max = [0 if np.isnan(x) else x for x in lambda_max]\n",
    "sigma = [0 if np.isnan(x) else x for x in sigma]\n",
    "tau = [0 if np.isnan(x) else x for x in tau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "severity_data['LambdaMax'] = lambda_max\n",
    "severity_data['Sigma'] = sigma\n",
    "severity_data['Tau'] = tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading speed, flow, occupancy, and stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(base_path + 'concat_no_holes/concat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select raw that is sampled\n",
    "raw_all = raw.loc[raw['Date'].isin(dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hylei/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/hylei/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/hylei/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/hylei/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "raw_all['LambdaMax'] = lambda_max\n",
    "raw_all['Sigma'] = sigma\n",
    "raw_all['Tau'] = tau\n",
    "raw_all['Impact'] = impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test = raw_all.loc[raw['Date'].isin(dates_test)]\n",
    "stations = np.array(raw_all['Station ID'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station ID</th>\n",
       "      <th>datetime</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>Flow</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>idx</th>\n",
       "      <th>LambdaMax</th>\n",
       "      <th>Sigma</th>\n",
       "      <th>Tau</th>\n",
       "      <th>Impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8640</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-01 00:00:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>06/01/2017</td>\n",
       "      <td>00:00</td>\n",
       "      <td>42322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8641</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-01 00:05:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>69.1</td>\n",
       "      <td>06/01/2017</td>\n",
       "      <td>00:05</td>\n",
       "      <td>42323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8642</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-01 00:10:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>68.9</td>\n",
       "      <td>06/01/2017</td>\n",
       "      <td>00:10</td>\n",
       "      <td>42324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Station ID             datetime  Occupancy  Flow  Speed        Date  \\\n",
       "8640      408907  2017-06-01 00:00:00        0.5  22.0   69.5  06/01/2017   \n",
       "8641      408907  2017-06-01 00:05:00        0.5  22.0   69.1  06/01/2017   \n",
       "8642      408907  2017-06-01 00:10:00        0.5  23.0   68.9  06/01/2017   \n",
       "\n",
       "       Time    idx  LambdaMax  Sigma  Tau    Impact  \n",
       "8640  00:00  42322        0.0    0.0  0.0  0.021267  \n",
       "8641  00:05  42323        0.0    0.0  0.0  0.017058  \n",
       "8642  00:10  42324        0.0    0.0  0.0  0.015338  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special construction of raw_train, because the dates are sampled with replacement\n",
    "raw_train = pd.DataFrame()\n",
    "duplicate_id = 0\n",
    "for i in range(0, len(dates_train)):\n",
    "    if i > 0:\n",
    "        if dates_train[i] == dates_train[i-1]:\n",
    "            duplicate_id += 1\n",
    "        else:\n",
    "            duplicate_id = 0\n",
    "    df_date = raw_all.loc[raw_all['Date'] == dates_train[i]]\n",
    "    df_date = df_date.assign(duplicateIdx=duplicate_id)\n",
    "    raw_train = raw_train.append(df_date)\n",
    "sorterIdx = dict( zip(stations, range(len(stations))) )\n",
    "raw_train['stationSorterIdx'] = raw_train['Station ID'].map(sorterIdx)\n",
    "raw_train = raw_train.sort_values(['stationSorterIdx', 'duplicateIdx', 'datetime'], ascending=[True, True, True])\n",
    "raw_train.drop(['duplicateIdx', 'stationSorterIdx', 'idx'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499392"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['Speed', 'Flow', 'Occupancy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct road segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_segments = list()\n",
    "for i in range(len(stations) - 1):\n",
    "    road_segments.append(tuple([stations[i], stations[i+1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_incidents = pd.read_csv(base_path + 'valid_incidents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_incidents_all = raw_incidents.loc[raw_incidents['Date'].isin(dates)]\n",
    "raw_incidents_train = raw_incidents_all.loc[raw_incidents_all['Date'].isin(dates_train)]\n",
    "raw_incidents_test = raw_incidents_all.loc[raw_incidents_all['Date'].isin(dates_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pos_timestamps = pd.read_csv(base_path + 'svm_pos_instances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pos_timestamps_train = svm_pos_timestamps.loc[svm_pos_timestamps['Date'].isin(dates_train)]\n",
    "svm_pos_timestamps_test = svm_pos_timestamps.loc[svm_pos_timestamps['Date'].isin(dates_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Upstream</th>\n",
       "      <th>Downstream</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>408907</td>\n",
       "      <td>400951</td>\n",
       "      <td>01/22/2017</td>\n",
       "      <td>20:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>408907</td>\n",
       "      <td>400951</td>\n",
       "      <td>01/22/2017</td>\n",
       "      <td>20:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>408907</td>\n",
       "      <td>400951</td>\n",
       "      <td>01/22/2017</td>\n",
       "      <td>20:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Upstream  Downstream        Date   Time\n",
       "0    408907      400951  01/22/2017  20:30\n",
       "1    408907      400951  01/22/2017  20:35\n",
       "2    408907      400951  01/22/2017  20:40"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_pos_timestamps.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_incident_dates_train = svm_pos_timestamps_train['Date'].unique().tolist()\n",
    "svm_normal_dates_train = list(set(dates_train).difference(svm_incident_dates_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svm_incident_dates_train), len(svm_normal_dates_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress message formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraction_msg(present, total):\n",
    "    return '[{}/{}]'.format(present, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train: TSA-DES forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DES_rmse(alpha, var_series):\n",
    "    len_series = len(var_series)\n",
    "    \n",
    "    beta = round(1. - alpha, 3)\n",
    "\n",
    "    sse = 0.\n",
    "    s1 = np.mean(var_series[:10])\n",
    "    s2 = s1\n",
    "    \n",
    "    for i in range(11, len_series - 1):\n",
    "        s1 = alpha * var_series[i] + beta * s1\n",
    "        s2 = alpha * s1 + beta * s2\n",
    "        y_next = 2 * s1 - s2 + alpha / beta * (s1 - s2)\n",
    "        sse += (var_series[i+1] - y_next) ** 2\n",
    "    \n",
    "    return np.sqrt( sse / (len_series - 12) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune best alphas for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: stations, raw training data (includ. incidents), rule to update alphas\n",
    "# output: a dictionary containing stations, and stations' best alphas\n",
    "def compute_best_alphas(stations, raw_train, raw_incidents_train, dates_train, num_grids, DES_rmse, fraction_msg):\n",
    "    best_alphas = {\n",
    "    'Station ID': [],\n",
    "    'Speed': [],\n",
    "    'Flow': [],\n",
    "    'Occupancy': []\n",
    "    }\n",
    "    pid = mp.current_process().pid\n",
    "    for i, station in enumerate(stations):\n",
    "        best_alphas['Station ID'].append(station)\n",
    "\n",
    "        # update current training station dataframe, the training data is normal day's data\n",
    "        abnormal_dates_station = raw_incidents_train.loc[(raw_incidents_train['Upstream'] == station) | (raw_incidents_train['Downstream'] == str(station))]['Date'].unique()\n",
    "        normal_dates_train = np.array(list(set(dates_train).difference(set(abnormal_dates_station))))\n",
    "        df_train_station = raw_train.loc[(raw_train['Station ID'] == station) & (raw_train['Date'].isin(normal_dates_train))]\n",
    "\n",
    "        print(\"{} {} Tuning alphas for station {}...\".format(pid, fraction_msg(i+1, len(stations)), station))\n",
    "        for var_name in var_names:\n",
    "            # print(\"    \" + var_name + \"...\")\n",
    "            var_series = df_train_station[var_name].values\n",
    "            len_series = len(var_series)\n",
    "\n",
    "            # setting up alphas\n",
    "            alphas = np.arange(num_grids) * 1. / num_grids\n",
    "\n",
    "            # save the historical best alpha by rmse\n",
    "            best_rmse = float(\"inf\")\n",
    "            best_alpha = 0.\n",
    "\n",
    "            # for each alpha, perform exponential smoothing, and compute RMSE\n",
    "            for alpha in alphas:\n",
    "                rmse = DES_rmse(alpha, var_series)\n",
    "\n",
    "                # compare, and decide whether to update best alpha\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_alpha = alpha\n",
    "\n",
    "            # finally, save the best alpha for the variable at this station\n",
    "            best_alphas[var_name].append(best_alpha)\n",
    "\n",
    "        # print trained alphas for each station\n",
    "        # print(best_alphas['Station ID'][i], best_alphas['Speed'][i], best_alphas['Flow'][i], best_alphas['Occupancy'][i])\n",
    "    print(\"Process {} has finished alpha tuning.\".format(pid))\n",
    "    return best_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28966 [1/13] Tuning alphas for station 408907...\n",
      "28967 [1/13] Tuning alphas for station 400088...\n",
      "28968 [1/13] Tuning alphas for station 408755...\n",
      "28969 [1/13] Tuning alphas for station 400137...\n",
      "28970 [1/13] Tuning alphas for station 400611...\n",
      "28971 [1/13] Tuning alphas for station 400275...\n",
      "28972 [1/13] Tuning alphas for station 400333...\n",
      "28973 [1/11] Tuning alphas for station 400980...\n",
      "28967 [2/13] Tuning alphas for station 402288...\n",
      "28966 [2/13] Tuning alphas for station 400951...\n",
      "28968 [2/13] Tuning alphas for station 402802...\n",
      "28969 [2/13] Tuning alphas for station 400716...\n",
      "28971 [2/13] Tuning alphas for station 400939...\n",
      "28970 [2/13] Tuning alphas for station 400928...\n",
      "28972 [2/13] Tuning alphas for station 410363...\n",
      "28973 [2/11] Tuning alphas for station 401333...\n",
      "28966 [3/13] Tuning alphas for station 400057...\n",
      "28967 [3/13] Tuning alphas for station 413026...\n",
      "28969 [3/13] Tuning alphas for station 401545...\n",
      "28968 [3/13] Tuning alphas for station 408756...\n",
      "28973 [3/11] Tuning alphas for station 404746...\n",
      "28971 [3/13] Tuning alphas for station 400180...\n",
      "28970 [3/13] Tuning alphas for station 400284...\n",
      "28972 [3/13] Tuning alphas for station 400360...\n",
      "28966 [4/13] Tuning alphas for station 400147...\n",
      "28967 [4/13] Tuning alphas for station 401464...\n",
      "28969 [4/13] Tuning alphas for station 401011...\n",
      "28970 [4/13] Tuning alphas for station 400041...\n",
      "28968 [4/13] Tuning alphas for station 400189...\n",
      "28973 [4/11] Tuning alphas for station 401142...\n",
      "28972 [4/13] Tuning alphas for station 400955...\n",
      "28971 [4/13] Tuning alphas for station 400529...\n",
      "28966 [5/13] Tuning alphas for station 400343...\n",
      "28969 [5/13] Tuning alphas for station 400674...\n",
      "28967 [5/13] Tuning alphas for station 401489...\n",
      "28968 [5/13] Tuning alphas for station 400309...\n",
      "28970 [5/13] Tuning alphas for station 408133...\n",
      "28973 [5/11] Tuning alphas for station 400218...\n",
      "28966 [6/13] Tuning alphas for station 401560...\n",
      "28972 [5/13] Tuning alphas for station 400495...\n",
      "28971 [5/13] Tuning alphas for station 400990...\n",
      "28969 [6/13] Tuning alphas for station 400539...\n",
      "28967 [6/13] Tuning alphas for station 401538...\n",
      "28968 [6/13] Tuning alphas for station 400417...\n",
      "28970 [6/13] Tuning alphas for station 408135...\n",
      "28966 [7/13] Tuning alphas for station 400045...\n",
      "28972 [6/13] Tuning alphas for station 400608...\n",
      "28973 [6/11] Tuning alphas for station 400983...\n",
      "28969 [7/13] Tuning alphas for station 400534...\n",
      "28971 [6/13] Tuning alphas for station 400515...\n",
      "28967 [7/13] Tuning alphas for station 402290...\n",
      "28968 [7/13] Tuning alphas for station 400249...\n",
      "28966 [8/13] Tuning alphas for station 400122...\n",
      "28973 [7/11] Tuning alphas for station 400765...\n",
      "28970 [7/13] Tuning alphas for station 417665...\n",
      "28972 [7/13] Tuning alphas for station 400949...\n",
      "28969 [8/13] Tuning alphas for station 401062...\n",
      "28967 [8/13] Tuning alphas for station 402292...\n",
      "28971 [7/13] Tuning alphas for station 400252...\n",
      "28968 [8/13] Tuning alphas for station 401639...\n",
      "28972 [8/13] Tuning alphas for station 400678...\n",
      "28970 [8/13] Tuning alphas for station 412637...\n",
      "28966 [9/13] Tuning alphas for station 401541...\n",
      "28973 [8/11] Tuning alphas for station 400844...\n",
      "28971 [8/13] Tuning alphas for station 400788...\n",
      "28972 [9/13] Tuning alphas for station 400341...\n",
      "28967 [9/13] Tuning alphas for station 401643...\n",
      "28968 [9/13] Tuning alphas for station 400662...\n",
      "28969 [9/13] Tuning alphas for station 401529...\n",
      "28970 [9/13] Tuning alphas for station 417666...\n",
      "28973 [9/11] Tuning alphas for station 400923...\n",
      "28966 [10/13] Tuning alphas for station 402281...\n",
      "28971 [9/13] Tuning alphas for station 401517...\n",
      "28967 [10/13] Tuning alphas for station 402800...\n",
      "28972 [10/13] Tuning alphas for station 400607...\n",
      "28968 [10/13] Tuning alphas for station 400141...\n",
      "28969 [10/13] Tuning alphas for station 401613...\n",
      "28966 [11/13] Tuning alphas for station 402283...\n",
      "28970 [10/13] Tuning alphas for station 408134...\n",
      "28973 [10/11] Tuning alphas for station 401143...\n",
      "28971 [10/13] Tuning alphas for station 401871...\n",
      "28967 [11/13] Tuning alphas for station 402828...\n",
      "28968 [11/13] Tuning alphas for station 400761...\n",
      "28972 [11/13] Tuning alphas for station 400094...\n",
      "28969 [11/13] Tuning alphas for station 400536...\n",
      "28966 [12/13] Tuning alphas for station 402285...\n",
      "28970 [11/13] Tuning alphas for station 400685...\n",
      "28971 [11/13] Tuning alphas for station 400574...\n",
      "28973 [11/11] Tuning alphas for station 401471...\n",
      "28967 [12/13] Tuning alphas for station 407219...\n",
      "28972 [12/13] Tuning alphas for station 400682...\n",
      "28968 [12/13] Tuning alphas for station 400490...\n",
      "28969 [12/13] Tuning alphas for station 400488...\n",
      "28966 [13/13] Tuning alphas for station 402286...\n",
      "28971 [12/13] Tuning alphas for station 401629...\n",
      "Process 28973 has finished alpha tuning.\n",
      "28970 [12/13] Tuning alphas for station 401003...\n",
      "28967 [13/13] Tuning alphas for station 402789...\n",
      "28972 [13/13] Tuning alphas for station 408138...\n",
      "28968 [13/13] Tuning alphas for station 401888...\n",
      "28969 [13/13] Tuning alphas for station 401561...\n",
      "28971 [13/13] Tuning alphas for station 400422...\n",
      "Process 28966 has finished alpha tuning.\n",
      "Process 28972 has finished alpha tuning.\n",
      "28970 [13/13] Tuning alphas for station 400898...\n",
      "Process 28967 has finished alpha tuning.\n",
      "Process 28969 has finished alpha tuning.\n",
      "Process 28968 has finished alpha tuning.\n",
      "Process 28970 has finished alpha tuning.\n",
      "Process 28971 has finished alpha tuning.\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(processes=8)\n",
    "num_grids = 100\n",
    "results = [pool.apply_async(compute_best_alphas, args=(stations[13 * i: 13 * i + 13], raw_train, raw_incidents_train, dates_train, num_grids, DES_rmse, fraction_msg)) for i in range(0, 8)]\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_results = []\n",
    "for proc in results:\n",
    "    exec_results.append(proc.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_alphas = {\n",
    "    'Station ID': [],\n",
    "    'Speed': [],\n",
    "    'Flow': [],\n",
    "    'Occupancy': []\n",
    "}\n",
    "for dict_best_alphas in exec_results:\n",
    "    for key in best_alphas.keys():\n",
    "        best_alphas[key].extend(dict_best_alphas[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Station ID': [408907,\n",
       "  400951,\n",
       "  400057,\n",
       "  400147,\n",
       "  400343,\n",
       "  401560,\n",
       "  400045,\n",
       "  400122,\n",
       "  401541,\n",
       "  402281,\n",
       "  402283,\n",
       "  402285,\n",
       "  402286,\n",
       "  400088,\n",
       "  402288,\n",
       "  413026,\n",
       "  401464,\n",
       "  401489,\n",
       "  401538,\n",
       "  402290,\n",
       "  402292,\n",
       "  401643,\n",
       "  402800,\n",
       "  402828,\n",
       "  407219,\n",
       "  402789,\n",
       "  408755,\n",
       "  402802,\n",
       "  408756,\n",
       "  400189,\n",
       "  400309,\n",
       "  400417,\n",
       "  400249,\n",
       "  401639,\n",
       "  400662,\n",
       "  400141,\n",
       "  400761,\n",
       "  400490,\n",
       "  401888,\n",
       "  400137,\n",
       "  400716,\n",
       "  401545,\n",
       "  401011,\n",
       "  400674,\n",
       "  400539,\n",
       "  400534,\n",
       "  401062,\n",
       "  401529,\n",
       "  401613,\n",
       "  400536,\n",
       "  400488,\n",
       "  401561,\n",
       "  400611,\n",
       "  400928,\n",
       "  400284,\n",
       "  400041,\n",
       "  408133,\n",
       "  408135,\n",
       "  417665,\n",
       "  412637,\n",
       "  417666,\n",
       "  408134,\n",
       "  400685,\n",
       "  401003,\n",
       "  400898,\n",
       "  400275,\n",
       "  400939,\n",
       "  400180,\n",
       "  400529,\n",
       "  400990,\n",
       "  400515,\n",
       "  400252,\n",
       "  400788,\n",
       "  401517,\n",
       "  401871,\n",
       "  400574,\n",
       "  401629,\n",
       "  400422,\n",
       "  400333,\n",
       "  410363,\n",
       "  400360,\n",
       "  400955,\n",
       "  400495,\n",
       "  400608,\n",
       "  400949,\n",
       "  400678,\n",
       "  400341,\n",
       "  400607,\n",
       "  400094,\n",
       "  400682,\n",
       "  408138,\n",
       "  400980,\n",
       "  401333,\n",
       "  404746,\n",
       "  401142,\n",
       "  400218,\n",
       "  400983,\n",
       "  400765,\n",
       "  400844,\n",
       "  400923,\n",
       "  401143,\n",
       "  401471],\n",
       " 'Speed': [0.75,\n",
       "  0.74,\n",
       "  0.83,\n",
       "  0.71,\n",
       "  0.62,\n",
       "  0.47,\n",
       "  0.71,\n",
       "  0.74,\n",
       "  0.61,\n",
       "  0.4,\n",
       "  0.37,\n",
       "  0.51,\n",
       "  0.52,\n",
       "  0.56,\n",
       "  0.59,\n",
       "  0.61,\n",
       "  0.6,\n",
       "  0.68,\n",
       "  0.32,\n",
       "  0.51,\n",
       "  0.66,\n",
       "  0.72,\n",
       "  0.61,\n",
       "  0.69,\n",
       "  0.6,\n",
       "  0.58,\n",
       "  0.69,\n",
       "  0.61,\n",
       "  0.69,\n",
       "  0.55,\n",
       "  0.57,\n",
       "  0.59,\n",
       "  0.74,\n",
       "  0.74,\n",
       "  0.52,\n",
       "  0.42,\n",
       "  0.5,\n",
       "  0.52,\n",
       "  0.62,\n",
       "  0.67,\n",
       "  0.58,\n",
       "  0.5,\n",
       "  0.69,\n",
       "  0.66,\n",
       "  0.61,\n",
       "  0.68,\n",
       "  0.62,\n",
       "  0.61,\n",
       "  0.53,\n",
       "  0.63,\n",
       "  0.71,\n",
       "  0.71,\n",
       "  0.65,\n",
       "  0.69,\n",
       "  0.67,\n",
       "  0.68,\n",
       "  0.61,\n",
       "  0.62,\n",
       "  0.68,\n",
       "  0.73,\n",
       "  0.7,\n",
       "  0.64,\n",
       "  0.61,\n",
       "  0.59,\n",
       "  0.48,\n",
       "  0.58,\n",
       "  0.68,\n",
       "  0.53,\n",
       "  0.71,\n",
       "  0.64,\n",
       "  0.54,\n",
       "  0.61,\n",
       "  0.65,\n",
       "  0.69,\n",
       "  0.68,\n",
       "  0.62,\n",
       "  0.72,\n",
       "  0.75,\n",
       "  0.69,\n",
       "  0.38,\n",
       "  0.69,\n",
       "  0.75,\n",
       "  0.71,\n",
       "  0.75,\n",
       "  0.72,\n",
       "  0.71,\n",
       "  0.77,\n",
       "  0.71,\n",
       "  0.64,\n",
       "  0.56,\n",
       "  0.53,\n",
       "  0.54,\n",
       "  0.39,\n",
       "  0.31,\n",
       "  0.42,\n",
       "  0.4,\n",
       "  0.29,\n",
       "  0.54,\n",
       "  0.75,\n",
       "  0.54,\n",
       "  0.56,\n",
       "  0.38],\n",
       " 'Flow': [0.19,\n",
       "  0.2,\n",
       "  0.22,\n",
       "  0.24,\n",
       "  0.24,\n",
       "  0.12,\n",
       "  0.11,\n",
       "  0.22,\n",
       "  0.23,\n",
       "  0.19,\n",
       "  0.19,\n",
       "  0.18,\n",
       "  0.17,\n",
       "  0.15,\n",
       "  0.19,\n",
       "  0.2,\n",
       "  0.19,\n",
       "  0.22,\n",
       "  0.2,\n",
       "  0.21,\n",
       "  0.2,\n",
       "  0.22,\n",
       "  0.4,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.24,\n",
       "  0.23,\n",
       "  0.16,\n",
       "  0.21,\n",
       "  0.23,\n",
       "  0.22,\n",
       "  0.24,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.23,\n",
       "  0.2,\n",
       "  0.21,\n",
       "  0.2,\n",
       "  0.14,\n",
       "  0.21,\n",
       "  0.21,\n",
       "  0.19,\n",
       "  0.19,\n",
       "  0.21,\n",
       "  0.23,\n",
       "  0.2,\n",
       "  0.18,\n",
       "  0.2,\n",
       "  0.19,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.23,\n",
       "  0.22,\n",
       "  0.23,\n",
       "  0.23,\n",
       "  0.23,\n",
       "  0.23,\n",
       "  0.22,\n",
       "  0.23,\n",
       "  0.24,\n",
       "  0.26,\n",
       "  0.29,\n",
       "  0.23,\n",
       "  0.22,\n",
       "  0.27,\n",
       "  0.26,\n",
       "  0.23,\n",
       "  0.26,\n",
       "  0.27,\n",
       "  0.23,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.26,\n",
       "  0.22,\n",
       "  0.28,\n",
       "  0.32,\n",
       "  0.21,\n",
       "  0.24,\n",
       "  0.15,\n",
       "  0.26,\n",
       "  0.26,\n",
       "  0.22,\n",
       "  0.25,\n",
       "  0.26,\n",
       "  0.24,\n",
       "  0.27,\n",
       "  0.21,\n",
       "  0.92,\n",
       "  0.17,\n",
       "  0.17,\n",
       "  0.17,\n",
       "  0.14,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.18,\n",
       "  0.12,\n",
       "  0.22,\n",
       "  0.18,\n",
       "  0.22,\n",
       "  0.17,\n",
       "  0.49],\n",
       " 'Occupancy': [0.58,\n",
       "  0.51,\n",
       "  0.54,\n",
       "  0.36,\n",
       "  0.35,\n",
       "  0.16,\n",
       "  0.19,\n",
       "  0.42,\n",
       "  0.21,\n",
       "  0.18,\n",
       "  0.17,\n",
       "  0.26,\n",
       "  0.27,\n",
       "  0.29,\n",
       "  0.32,\n",
       "  0.34,\n",
       "  0.4,\n",
       "  0.44,\n",
       "  0.28,\n",
       "  0.35,\n",
       "  0.38,\n",
       "  0.39,\n",
       "  0.4,\n",
       "  0.32,\n",
       "  0.28,\n",
       "  0.29,\n",
       "  0.32,\n",
       "  0.24,\n",
       "  0.32,\n",
       "  0.26,\n",
       "  0.27,\n",
       "  0.33,\n",
       "  0.41,\n",
       "  0.47,\n",
       "  0.23,\n",
       "  0.21,\n",
       "  0.25,\n",
       "  0.37,\n",
       "  0.28,\n",
       "  0.32,\n",
       "  0.32,\n",
       "  0.32,\n",
       "  0.41,\n",
       "  0.33,\n",
       "  0.32,\n",
       "  0.3,\n",
       "  0.33,\n",
       "  0.25,\n",
       "  0.24,\n",
       "  0.41,\n",
       "  0.43,\n",
       "  0.43,\n",
       "  0.25,\n",
       "  0.34,\n",
       "  0.32,\n",
       "  0.34,\n",
       "  0.41,\n",
       "  0.44,\n",
       "  0.46,\n",
       "  0.39,\n",
       "  0.4,\n",
       "  0.42,\n",
       "  0.36,\n",
       "  0.31,\n",
       "  0.23,\n",
       "  0.33,\n",
       "  0.37,\n",
       "  0.3,\n",
       "  0.41,\n",
       "  0.4,\n",
       "  0.38,\n",
       "  0.42,\n",
       "  0.4,\n",
       "  0.43,\n",
       "  0.39,\n",
       "  0.35,\n",
       "  0.43,\n",
       "  0.44,\n",
       "  0.32,\n",
       "  0.22,\n",
       "  0.38,\n",
       "  0.46,\n",
       "  0.42,\n",
       "  0.4,\n",
       "  0.36,\n",
       "  0.38,\n",
       "  0.39,\n",
       "  0.32,\n",
       "  0.79,\n",
       "  0.25,\n",
       "  0.2,\n",
       "  0.22,\n",
       "  0.14,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.2,\n",
       "  0.13,\n",
       "  0.42,\n",
       "  0.52,\n",
       "  0.47,\n",
       "  0.45,\n",
       "  0.42]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alphas_df = pd.DataFrame(best_alphas)\n",
    "best_alphas_df.to_csv(base_path + 'smaller_sample/best_alphas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the tuned alphas to predict training traffic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/102] Start time series prediction (DES) at station 408907...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 408907.\n",
      "[2/102] Start time series prediction (DES) at station 400951...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400951.\n",
      "[3/102] Start time series prediction (DES) at station 400057...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400057.\n",
      "[4/102] Start time series prediction (DES) at station 400147...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400147.\n",
      "[5/102] Start time series prediction (DES) at station 400343...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400343.\n",
      "[6/102] Start time series prediction (DES) at station 401560...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401560.\n",
      "[7/102] Start time series prediction (DES) at station 400045...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400045.\n",
      "[8/102] Start time series prediction (DES) at station 400122...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400122.\n",
      "[9/102] Start time series prediction (DES) at station 401541...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401541.\n",
      "[10/102] Start time series prediction (DES) at station 402281...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402281.\n",
      "[11/102] Start time series prediction (DES) at station 402283...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402283.\n",
      "[12/102] Start time series prediction (DES) at station 402285...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402285.\n",
      "[13/102] Start time series prediction (DES) at station 402286...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402286.\n",
      "[14/102] Start time series prediction (DES) at station 400088...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400088.\n",
      "[15/102] Start time series prediction (DES) at station 402288...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402288.\n",
      "[16/102] Start time series prediction (DES) at station 413026...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 413026.\n",
      "[17/102] Start time series prediction (DES) at station 401464...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401464.\n",
      "[18/102] Start time series prediction (DES) at station 401489...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401489.\n",
      "[19/102] Start time series prediction (DES) at station 401538...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401538.\n",
      "[20/102] Start time series prediction (DES) at station 402290...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402290.\n",
      "[21/102] Start time series prediction (DES) at station 402292...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402292.\n",
      "[22/102] Start time series prediction (DES) at station 401643...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401643.\n",
      "[23/102] Start time series prediction (DES) at station 402800...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402800.\n",
      "[24/102] Start time series prediction (DES) at station 402828...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402828.\n",
      "[25/102] Start time series prediction (DES) at station 407219...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 407219.\n",
      "[26/102] Start time series prediction (DES) at station 402789...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402789.\n",
      "[27/102] Start time series prediction (DES) at station 408755...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 408755.\n",
      "[28/102] Start time series prediction (DES) at station 402802...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402802.\n",
      "[29/102] Start time series prediction (DES) at station 408756...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 408756.\n",
      "[30/102] Start time series prediction (DES) at station 400189...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400189.\n",
      "[31/102] Start time series prediction (DES) at station 400309...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400309.\n",
      "[32/102] Start time series prediction (DES) at station 400417...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400417.\n",
      "[33/102] Start time series prediction (DES) at station 400249...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400249.\n",
      "[34/102] Start time series prediction (DES) at station 401639...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401639.\n",
      "[35/102] Start time series prediction (DES) at station 400662...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400662.\n",
      "[36/102] Start time series prediction (DES) at station 400141...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400141.\n",
      "[37/102] Start time series prediction (DES) at station 400761...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400761.\n",
      "[38/102] Start time series prediction (DES) at station 400490...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400490.\n",
      "[39/102] Start time series prediction (DES) at station 401888...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401888.\n",
      "[40/102] Start time series prediction (DES) at station 400137...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400137.\n",
      "[41/102] Start time series prediction (DES) at station 400716...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400716.\n",
      "[42/102] Start time series prediction (DES) at station 401545...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401545.\n",
      "[43/102] Start time series prediction (DES) at station 401011...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401011.\n",
      "[44/102] Start time series prediction (DES) at station 400674...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400674.\n",
      "[45/102] Start time series prediction (DES) at station 400539...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400539.\n",
      "[46/102] Start time series prediction (DES) at station 400534...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400534.\n",
      "[47/102] Start time series prediction (DES) at station 401062...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401062.\n",
      "[48/102] Start time series prediction (DES) at station 401529...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401529.\n",
      "[49/102] Start time series prediction (DES) at station 401613...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401613.\n",
      "[50/102] Start time series prediction (DES) at station 400536...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400536.\n",
      "[51/102] Start time series prediction (DES) at station 400488...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400488.\n",
      "[52/102] Start time series prediction (DES) at station 401561...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401561.\n",
      "[53/102] Start time series prediction (DES) at station 400611...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400611.\n",
      "[54/102] Start time series prediction (DES) at station 400928...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400928.\n",
      "[55/102] Start time series prediction (DES) at station 400284...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400284.\n",
      "[56/102] Start time series prediction (DES) at station 400041...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400041.\n",
      "[57/102] Start time series prediction (DES) at station 408133...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 408133.\n",
      "[58/102] Start time series prediction (DES) at station 408135...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 408135.\n",
      "[59/102] Start time series prediction (DES) at station 417665...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 417665.\n",
      "[60/102] Start time series prediction (DES) at station 412637...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End prediction at station 412637.\n",
      "[61/102] Start time series prediction (DES) at station 417666...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 417666.\n",
      "[62/102] Start time series prediction (DES) at station 408134...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 408134.\n",
      "[63/102] Start time series prediction (DES) at station 400685...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400685.\n",
      "[64/102] Start time series prediction (DES) at station 401003...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401003.\n",
      "[65/102] Start time series prediction (DES) at station 400898...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400898.\n",
      "[66/102] Start time series prediction (DES) at station 400275...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400275.\n",
      "[67/102] Start time series prediction (DES) at station 400939...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400939.\n",
      "[68/102] Start time series prediction (DES) at station 400180...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400180.\n",
      "[69/102] Start time series prediction (DES) at station 400529...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400529.\n",
      "[70/102] Start time series prediction (DES) at station 400990...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400990.\n",
      "[71/102] Start time series prediction (DES) at station 400515...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400515.\n",
      "[72/102] Start time series prediction (DES) at station 400252...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400252.\n",
      "[73/102] Start time series prediction (DES) at station 400788...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400788.\n",
      "[74/102] Start time series prediction (DES) at station 401517...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401517.\n",
      "[75/102] Start time series prediction (DES) at station 401871...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401871.\n",
      "[76/102] Start time series prediction (DES) at station 400574...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400574.\n",
      "[77/102] Start time series prediction (DES) at station 401629...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401629.\n",
      "[78/102] Start time series prediction (DES) at station 400422...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400422.\n",
      "[79/102] Start time series prediction (DES) at station 400333...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400333.\n",
      "[80/102] Start time series prediction (DES) at station 410363...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 410363.\n",
      "[81/102] Start time series prediction (DES) at station 400360...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400360.\n",
      "[82/102] Start time series prediction (DES) at station 400955...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400955.\n",
      "[83/102] Start time series prediction (DES) at station 400495...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400495.\n",
      "[84/102] Start time series prediction (DES) at station 400608...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400608.\n",
      "[85/102] Start time series prediction (DES) at station 400949...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400949.\n",
      "[86/102] Start time series prediction (DES) at station 400678...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400678.\n",
      "[87/102] Start time series prediction (DES) at station 400341...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400341.\n",
      "[88/102] Start time series prediction (DES) at station 400607...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400607.\n",
      "[89/102] Start time series prediction (DES) at station 400094...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400094.\n",
      "[90/102] Start time series prediction (DES) at station 400682...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400682.\n",
      "[91/102] Start time series prediction (DES) at station 408138...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 408138.\n",
      "[92/102] Start time series prediction (DES) at station 400980...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400980.\n",
      "[93/102] Start time series prediction (DES) at station 401333...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401333.\n",
      "[94/102] Start time series prediction (DES) at station 404746...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 404746.\n",
      "[95/102] Start time series prediction (DES) at station 401142...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401142.\n",
      "[96/102] Start time series prediction (DES) at station 400218...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400218.\n",
      "[97/102] Start time series prediction (DES) at station 400983...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400983.\n",
      "[98/102] Start time series prediction (DES) at station 400765...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400765.\n",
      "[99/102] Start time series prediction (DES) at station 400844...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400844.\n",
      "[100/102] Start time series prediction (DES) at station 400923...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400923.\n",
      "[101/102] Start time series prediction (DES) at station 401143...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401143.\n",
      "[102/102] Start time series prediction (DES) at station 401471...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401471.\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "# initialize prediction dictionary\n",
    "pred_dict_train = dict()\n",
    "for var_name in var_names:\n",
    "    pred_dict_train[var_name] = []\n",
    "\n",
    "for i, station in enumerate(stations):\n",
    "    print(\"{} Start time series prediction (DES) at station {}...\".format(fraction_msg(i+1, len(stations)), station))\n",
    "    df_train_station = raw_train.loc[raw_train[\"Station ID\"] == station]\n",
    "    \n",
    "    # formulate predictions of speed, flow and occupancy for the station\n",
    "    for var_name in var_names:\n",
    "        print(\"    {}...\".format(var_name))\n",
    "        var_series = df_train_station[var_name].values\n",
    "        len_series = len(var_series)\n",
    "        # initialize s1, s2, and y\n",
    "        s1 = np.mean(var_series[:10])\n",
    "        s2 = s1\n",
    "        y = [0.] * len_series\n",
    "        # get the best alpha\n",
    "        var_best_alpha = best_alphas_df.loc[best_alphas_df[\"Station ID\"] == station][var_name].values[0]\n",
    "        beta = 1. - var_best_alpha\n",
    "\n",
    "        for t in range(11, len_series - 1):\n",
    "            s1 = var_best_alpha * var_series[t] + beta * s1\n",
    "            s2 = var_best_alpha * s1 + beta * s2\n",
    "            y[t+1] = round(2 * s1 - s2 + var_best_alpha / beta * (s1 - s2), 2)\n",
    "\n",
    "        # save the predictions to a dictionary\n",
    "        pred_dict_train[var_name].extend(y)\n",
    "    print(\"End prediction at station {}.\".format(station))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = raw_train.assign(Pred_Speed=pred_dict_train['Speed'], Pred_Flow=pred_dict_train['Flow'], Pred_Occupancy=pred_dict_train['Occupancy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['Diff_Speed'] = raw_train['Speed'] - raw_train['Pred_Speed']\n",
    "raw_train['Diff_Flow'] = raw_train['Flow'] - raw_train['Pred_Flow']\n",
    "raw_train['Diff_Occupancy'] = raw_train['Occupancy'] - raw_train['Pred_Occupancy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the tuned alphas to predict testing traffic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/102] Start time series prediction (DES) at station 408907...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 408907.\n",
      "[2/102] Start time series prediction (DES) at station 400951...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400951.\n",
      "[3/102] Start time series prediction (DES) at station 400057...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400057.\n",
      "[4/102] Start time series prediction (DES) at station 400147...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400147.\n",
      "[5/102] Start time series prediction (DES) at station 400343...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400343.\n",
      "[6/102] Start time series prediction (DES) at station 401560...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401560.\n",
      "[7/102] Start time series prediction (DES) at station 400045...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400045.\n",
      "[8/102] Start time series prediction (DES) at station 400122...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400122.\n",
      "[9/102] Start time series prediction (DES) at station 401541...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401541.\n",
      "[10/102] Start time series prediction (DES) at station 402281...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402281.\n",
      "[11/102] Start time series prediction (DES) at station 402283...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402283.\n",
      "[12/102] Start time series prediction (DES) at station 402285...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402285.\n",
      "[13/102] Start time series prediction (DES) at station 402286...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402286.\n",
      "[14/102] Start time series prediction (DES) at station 400088...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400088.\n",
      "[15/102] Start time series prediction (DES) at station 402288...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402288.\n",
      "[16/102] Start time series prediction (DES) at station 413026...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 413026.\n",
      "[17/102] Start time series prediction (DES) at station 401464...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401464.\n",
      "[18/102] Start time series prediction (DES) at station 401489...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401489.\n",
      "[19/102] Start time series prediction (DES) at station 401538...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401538.\n",
      "[20/102] Start time series prediction (DES) at station 402290...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402290.\n",
      "[21/102] Start time series prediction (DES) at station 402292...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402292.\n",
      "[22/102] Start time series prediction (DES) at station 401643...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401643.\n",
      "[23/102] Start time series prediction (DES) at station 402800...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402800.\n",
      "[24/102] Start time series prediction (DES) at station 402828...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402828.\n",
      "[25/102] Start time series prediction (DES) at station 407219...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 407219.\n",
      "[26/102] Start time series prediction (DES) at station 402789...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402789.\n",
      "[27/102] Start time series prediction (DES) at station 408755...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 408755.\n",
      "[28/102] Start time series prediction (DES) at station 402802...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402802.\n",
      "[29/102] Start time series prediction (DES) at station 408756...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 408756.\n",
      "[30/102] Start time series prediction (DES) at station 400189...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400189.\n",
      "[31/102] Start time series prediction (DES) at station 400309...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400309.\n",
      "[32/102] Start time series prediction (DES) at station 400417...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400417.\n",
      "[33/102] Start time series prediction (DES) at station 400249...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400249.\n",
      "[34/102] Start time series prediction (DES) at station 401639...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401639.\n",
      "[35/102] Start time series prediction (DES) at station 400662...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400662.\n",
      "[36/102] Start time series prediction (DES) at station 400141...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400141.\n",
      "[37/102] Start time series prediction (DES) at station 400761...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400761.\n",
      "[38/102] Start time series prediction (DES) at station 400490...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400490.\n",
      "[39/102] Start time series prediction (DES) at station 401888...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401888.\n",
      "[40/102] Start time series prediction (DES) at station 400137...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400137.\n",
      "[41/102] Start time series prediction (DES) at station 400716...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400716.\n",
      "[42/102] Start time series prediction (DES) at station 401545...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401545.\n",
      "[43/102] Start time series prediction (DES) at station 401011...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401011.\n",
      "[44/102] Start time series prediction (DES) at station 400674...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400674.\n",
      "[45/102] Start time series prediction (DES) at station 400539...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400539.\n",
      "[46/102] Start time series prediction (DES) at station 400534...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400534.\n",
      "[47/102] Start time series prediction (DES) at station 401062...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401062.\n",
      "[48/102] Start time series prediction (DES) at station 401529...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401529.\n",
      "[49/102] Start time series prediction (DES) at station 401613...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401613.\n",
      "[50/102] Start time series prediction (DES) at station 400536...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400536.\n",
      "[51/102] Start time series prediction (DES) at station 400488...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400488.\n",
      "[52/102] Start time series prediction (DES) at station 401561...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401561.\n",
      "[53/102] Start time series prediction (DES) at station 400611...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400611.\n",
      "[54/102] Start time series prediction (DES) at station 400928...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400928.\n",
      "[55/102] Start time series prediction (DES) at station 400284...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400284.\n",
      "[56/102] Start time series prediction (DES) at station 400041...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400041.\n",
      "[57/102] Start time series prediction (DES) at station 408133...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 408133.\n",
      "[58/102] Start time series prediction (DES) at station 408135...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 408135.\n",
      "[59/102] Start time series prediction (DES) at station 417665...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished forecasting at station 417665.\n",
      "[60/102] Start time series prediction (DES) at station 412637...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 412637.\n",
      "[61/102] Start time series prediction (DES) at station 417666...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 417666.\n",
      "[62/102] Start time series prediction (DES) at station 408134...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 408134.\n",
      "[63/102] Start time series prediction (DES) at station 400685...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400685.\n",
      "[64/102] Start time series prediction (DES) at station 401003...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401003.\n",
      "[65/102] Start time series prediction (DES) at station 400898...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400898.\n",
      "[66/102] Start time series prediction (DES) at station 400275...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400275.\n",
      "[67/102] Start time series prediction (DES) at station 400939...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400939.\n",
      "[68/102] Start time series prediction (DES) at station 400180...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400180.\n",
      "[69/102] Start time series prediction (DES) at station 400529...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400529.\n",
      "[70/102] Start time series prediction (DES) at station 400990...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400990.\n",
      "[71/102] Start time series prediction (DES) at station 400515...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400515.\n",
      "[72/102] Start time series prediction (DES) at station 400252...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400252.\n",
      "[73/102] Start time series prediction (DES) at station 400788...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400788.\n",
      "[74/102] Start time series prediction (DES) at station 401517...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401517.\n",
      "[75/102] Start time series prediction (DES) at station 401871...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401871.\n",
      "[76/102] Start time series prediction (DES) at station 400574...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400574.\n",
      "[77/102] Start time series prediction (DES) at station 401629...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401629.\n",
      "[78/102] Start time series prediction (DES) at station 400422...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400422.\n",
      "[79/102] Start time series prediction (DES) at station 400333...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400333.\n",
      "[80/102] Start time series prediction (DES) at station 410363...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 410363.\n",
      "[81/102] Start time series prediction (DES) at station 400360...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400360.\n",
      "[82/102] Start time series prediction (DES) at station 400955...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400955.\n",
      "[83/102] Start time series prediction (DES) at station 400495...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400495.\n",
      "[84/102] Start time series prediction (DES) at station 400608...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400608.\n",
      "[85/102] Start time series prediction (DES) at station 400949...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400949.\n",
      "[86/102] Start time series prediction (DES) at station 400678...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400678.\n",
      "[87/102] Start time series prediction (DES) at station 400341...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400341.\n",
      "[88/102] Start time series prediction (DES) at station 400607...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400607.\n",
      "[89/102] Start time series prediction (DES) at station 400094...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400094.\n",
      "[90/102] Start time series prediction (DES) at station 400682...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400682.\n",
      "[91/102] Start time series prediction (DES) at station 408138...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 408138.\n",
      "[92/102] Start time series prediction (DES) at station 400980...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400980.\n",
      "[93/102] Start time series prediction (DES) at station 401333...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401333.\n",
      "[94/102] Start time series prediction (DES) at station 404746...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 404746.\n",
      "[95/102] Start time series prediction (DES) at station 401142...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401142.\n",
      "[96/102] Start time series prediction (DES) at station 400218...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400218.\n",
      "[97/102] Start time series prediction (DES) at station 400983...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400983.\n",
      "[98/102] Start time series prediction (DES) at station 400765...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400765.\n",
      "[99/102] Start time series prediction (DES) at station 400844...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400844.\n",
      "[100/102] Start time series prediction (DES) at station 400923...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400923.\n",
      "[101/102] Start time series prediction (DES) at station 401143...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401143.\n",
      "[102/102] Start time series prediction (DES) at station 401471...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401471.\n",
      "Finished forecasting for the test dataset.\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "# initialize prediction dictionary\n",
    "pred_dict_test = dict()\n",
    "for var_name in var_names:\n",
    "    pred_dict_test[var_name] = []\n",
    "\n",
    "for i, station in enumerate(stations):\n",
    "    print(\"{} Start time series prediction (DES) at station {}...\".format(fraction_msg(i+1, len(stations)), station))\n",
    "    df_test_station = raw_test.loc[raw_test[\"Station ID\"] == station]\n",
    "    \n",
    "    # formulate predictions of speed, flow and occupancy for the station\n",
    "    for var_name in var_names:\n",
    "        print(\"    {}...\".format(var_name))\n",
    "        var_series = df_test_station[var_name].values\n",
    "        len_series = len(var_series)\n",
    "        # initialize s1, s2, and y\n",
    "        s1 = np.mean(var_series[:10])\n",
    "        s2 = s1\n",
    "        y = [0.] * len_series\n",
    "        # get the best alpha\n",
    "        var_best_alpha = best_alphas_df.loc[best_alphas_df[\"Station ID\"] == station][var_name].values[0]\n",
    "        beta = 1. - var_best_alpha\n",
    "\n",
    "        num_batches = int(len_series / 288)\n",
    "        for j in range(num_batches):\n",
    "            base_idx = 288 * j\n",
    "            for t in range(base_idx + 11, base_idx + 287):\n",
    "                s1 = var_best_alpha * var_series[t] + beta * s1\n",
    "                s2 = var_best_alpha * s1 + beta * s2\n",
    "                y[t+1] = round(2 * s1 - s2 + var_best_alpha / beta * (s1 - s2), 2)\n",
    "\n",
    "        # save the predictions to a dictionary\n",
    "        pred_dict_test[var_name].extend(y)\n",
    "    print(\"Finished forecasting at station {}.\".format(station))\n",
    "print(\"Finished forecasting for the test dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test = raw_test.assign(Pred_Speed=pred_dict_test['Speed'], Pred_Flow=pred_dict_test['Flow'], Pred_Occupancy=pred_dict_test['Occupancy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test.drop(columns=['idx'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test['Diff_Speed'] = raw_test['Speed'] - raw_test['Pred_Speed']\n",
    "raw_test['Diff_Flow'] = raw_test['Flow'] - raw_test['Pred_Flow']\n",
    "raw_test['Diff_Occupancy'] = raw_test['Occupancy'] - raw_test['Pred_Occupancy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training/testing, we want to normalize data for each station.  That is,\n",
    "1. Using **training** data, compute the ***mean*** and ***standard deviation*** for occ, speed, flow, and their predictions at each station;\n",
    "2. Compute the z-scores as the new occ, speed, flow and their time series predictions;\n",
    "3. Keep the means and standard deviations.  After training SVM, compute z-scores using the means and sds from training data for the testing data.  Then perform prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does it make sense to compute z-scores?  Checkout `data_analysis/descriptive_statistics.ipynb` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station ID</th>\n",
       "      <th>datetime</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>Flow</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>LambdaMax</th>\n",
       "      <th>Sigma</th>\n",
       "      <th>Tau</th>\n",
       "      <th>Impact</th>\n",
       "      <th>Pred_Speed</th>\n",
       "      <th>Pred_Flow</th>\n",
       "      <th>Pred_Occupancy</th>\n",
       "      <th>Diff_Speed</th>\n",
       "      <th>Diff_Flow</th>\n",
       "      <th>Diff_Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8640</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-01 00:00:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>06/01/2017</td>\n",
       "      <td>00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8641</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-01 00:05:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>69.1</td>\n",
       "      <td>06/01/2017</td>\n",
       "      <td>00:05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8642</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-01 00:10:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>68.9</td>\n",
       "      <td>06/01/2017</td>\n",
       "      <td>00:10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Station ID             datetime  Occupancy  Flow  Speed        Date  \\\n",
       "8640      408907  2017-06-01 00:00:00        0.5  22.0   69.5  06/01/2017   \n",
       "8641      408907  2017-06-01 00:05:00        0.5  22.0   69.1  06/01/2017   \n",
       "8642      408907  2017-06-01 00:10:00        0.5  23.0   68.9  06/01/2017   \n",
       "\n",
       "       Time  LambdaMax  Sigma  Tau    Impact  Pred_Speed  Pred_Flow  \\\n",
       "8640  00:00        0.0    0.0  0.0  0.021267         0.0        0.0   \n",
       "8641  00:05        0.0    0.0  0.0  0.017058         0.0        0.0   \n",
       "8642  00:10        0.0    0.0  0.0  0.015338         0.0        0.0   \n",
       "\n",
       "      Pred_Occupancy  Diff_Speed  Diff_Flow  Diff_Occupancy  \n",
       "8640             0.0        69.5       22.0             0.5  \n",
       "8641             0.0        69.1       22.0             0.5  \n",
       "8642             0.0        68.9       23.0             0.5  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sd_dict = dict()\n",
    "pred_var_names = []\n",
    "diff_var_names = []\n",
    "for var in var_names:\n",
    "    mean_sd_dict[var + \"_mean\"] = []\n",
    "    mean_sd_dict[var + \"_sd\"] = []\n",
    "    pred_var_names.append(\"Pred_\" + var)\n",
    "    diff_var_names.append(\"Diff_\" + var)\n",
    "for var in pred_var_names:\n",
    "    mean_sd_dict[var + \"_mean\"] = []\n",
    "    mean_sd_dict[var + \"_sd\"] = []\n",
    "for var in diff_var_names:\n",
    "    mean_sd_dict[var + \"_mean\"] = []\n",
    "    mean_sd_dict[var + \"_sd\"] = []\n",
    "mean_sd_dict['station'] = stations\n",
    "\n",
    "for i, s in enumerate(stations):\n",
    "    s_df = raw_train[raw_train['Station ID'] == s]\n",
    "    for j, var in enumerate(var_names):\n",
    "        var_values = s_df[var].values\n",
    "        mean_sd_dict[var + \"_mean\"].append(np.mean(var_values))\n",
    "        mean_sd_dict[var + \"_sd\"].append(np.std(var_values))\n",
    "    for j, var in enumerate(pred_var_names):\n",
    "        var_values = s_df[var].values\n",
    "        mean_sd_dict[var + \"_mean\"].append(np.mean(var_values))\n",
    "        mean_sd_dict[var + \"_sd\"].append(np.std(var_values))\n",
    "    for j, var in enumerate(diff_var_names):\n",
    "        var_values = s_df[var].values\n",
    "        mean_sd_dict[var + \"_mean\"].append(np.mean(var_values))\n",
    "        mean_sd_dict[var + \"_sd\"].append(np.std(var_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Speed_mean', 'Speed_sd', 'Flow_mean', 'Flow_sd', 'Occupancy_mean', 'Occupancy_sd', 'Pred_Speed_mean', 'Pred_Speed_sd', 'Pred_Flow_mean', 'Pred_Flow_sd', 'Pred_Occupancy_mean', 'Pred_Occupancy_sd', 'Diff_Speed_mean', 'Diff_Speed_sd', 'Diff_Flow_mean', 'Diff_Flow_sd', 'Diff_Occupancy_mean', 'Diff_Occupancy_sd', 'station'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_sd_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_train = raw_train['Station ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_idx_dict = dict()\n",
    "for i, s in enumerate(stations):\n",
    "    station_idx_dict[s] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Speed', 'Flow', 'Occupancy', \n",
    "           'Pred_Speed', 'Pred_Flow', 'Pred_Occupancy', \n",
    "           'Diff_Speed', 'Diff_Flow', 'Diff_Occupancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished z-score computation: Speed.\n",
      "Finished z-score computation: Flow.\n",
      "Finished z-score computation: Occupancy.\n",
      "Finished z-score computation: Pred_Speed.\n",
      "Finished z-score computation: Pred_Flow.\n",
      "Finished z-score computation: Pred_Occupancy.\n",
      "Finished z-score computation: Diff_Speed.\n",
      "Finished z-score computation: Diff_Flow.\n",
      "Finished z-score computation: Diff_Occupancy.\n"
     ]
    }
   ],
   "source": [
    "z_dict = dict()\n",
    "for col in columns:\n",
    "    z_scores = []\n",
    "    col_train = raw_train[col].values\n",
    "    for i, val in enumerate(col_train):\n",
    "        sid = stations_train[i]\n",
    "        sidx = station_idx_dict[sid]\n",
    "        z_score = (val - mean_sd_dict[col+'_mean'][sidx]) / mean_sd_dict[col+'_sd'][sidx]\n",
    "        z_scores.append(z_score)\n",
    "    z_dict[col] = z_scores\n",
    "    print(\"Finished z-score computation: {}.\".format(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in z_dict.keys():\n",
    "    raw_train[key] = z_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station ID</th>\n",
       "      <th>datetime</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>Flow</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>LambdaMax</th>\n",
       "      <th>Sigma</th>\n",
       "      <th>Tau</th>\n",
       "      <th>Impact</th>\n",
       "      <th>Pred_Speed</th>\n",
       "      <th>Pred_Flow</th>\n",
       "      <th>Pred_Occupancy</th>\n",
       "      <th>Diff_Speed</th>\n",
       "      <th>Diff_Flow</th>\n",
       "      <th>Diff_Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8640</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-01 00:00:00</td>\n",
       "      <td>-1.008772</td>\n",
       "      <td>-1.404213</td>\n",
       "      <td>0.633301</td>\n",
       "      <td>06/01/2017</td>\n",
       "      <td>00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021267</td>\n",
       "      <td>-10.449204</td>\n",
       "      <td>-1.613481</td>\n",
       "      <td>-1.065</td>\n",
       "      <td>18.058879</td>\n",
       "      <td>0.990930</td>\n",
       "      <td>0.323992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8641</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-01 00:05:00</td>\n",
       "      <td>-1.008772</td>\n",
       "      <td>-1.404213</td>\n",
       "      <td>0.551932</td>\n",
       "      <td>06/01/2017</td>\n",
       "      <td>00:05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017058</td>\n",
       "      <td>-10.449204</td>\n",
       "      <td>-1.613481</td>\n",
       "      <td>-1.065</td>\n",
       "      <td>17.954691</td>\n",
       "      <td>0.990930</td>\n",
       "      <td>0.323992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8642</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-01 00:10:00</td>\n",
       "      <td>-1.008772</td>\n",
       "      <td>-1.393785</td>\n",
       "      <td>0.511248</td>\n",
       "      <td>06/01/2017</td>\n",
       "      <td>00:10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015338</td>\n",
       "      <td>-10.449204</td>\n",
       "      <td>-1.613481</td>\n",
       "      <td>-1.065</td>\n",
       "      <td>17.902598</td>\n",
       "      <td>1.036034</td>\n",
       "      <td>0.323992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Station ID             datetime  Occupancy      Flow     Speed  \\\n",
       "8640      408907  2017-06-01 00:00:00  -1.008772 -1.404213  0.633301   \n",
       "8641      408907  2017-06-01 00:05:00  -1.008772 -1.404213  0.551932   \n",
       "8642      408907  2017-06-01 00:10:00  -1.008772 -1.393785  0.511248   \n",
       "\n",
       "            Date   Time  LambdaMax  Sigma  Tau    Impact  Pred_Speed  \\\n",
       "8640  06/01/2017  00:00        0.0    0.0  0.0  0.021267  -10.449204   \n",
       "8641  06/01/2017  00:05        0.0    0.0  0.0  0.017058  -10.449204   \n",
       "8642  06/01/2017  00:10        0.0    0.0  0.0  0.015338  -10.449204   \n",
       "\n",
       "      Pred_Flow  Pred_Occupancy  Diff_Speed  Diff_Flow  Diff_Occupancy  \n",
       "8640  -1.613481          -1.065   18.058879   0.990930        0.323992  \n",
       "8641  -1.613481          -1.065   17.954691   0.990930        0.323992  \n",
       "8642  -1.613481          -1.065   17.902598   1.036034        0.323992  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished z-score computation: Speed.\n",
      "Finished z-score computation: Flow.\n",
      "Finished z-score computation: Occupancy.\n",
      "Finished z-score computation: Pred_Speed.\n",
      "Finished z-score computation: Pred_Flow.\n",
      "Finished z-score computation: Pred_Occupancy.\n",
      "Finished z-score computation: Diff_Speed.\n",
      "Finished z-score computation: Diff_Flow.\n",
      "Finished z-score computation: Diff_Occupancy.\n"
     ]
    }
   ],
   "source": [
    "stations_test = raw_test['Station ID'].values\n",
    "z_dict_test = dict()\n",
    "for col in columns:\n",
    "    z_scores = []\n",
    "    col_test = raw_test[col].values\n",
    "    for i, val in enumerate(col_test):\n",
    "        sid = stations_test[i]\n",
    "        sidx = station_idx_dict[sid]\n",
    "        z_score = (val - mean_sd_dict[col+'_mean'][sidx]) / mean_sd_dict[col+'_sd'][sidx]\n",
    "        z_scores.append(z_score)\n",
    "    z_dict_test[col] = z_scores\n",
    "    print(\"Finished z-score computation: {}.\".format(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in z_dict.keys():\n",
    "    raw_test[key] = z_dict_test[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station ID</th>\n",
       "      <th>datetime</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>Flow</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>LambdaMax</th>\n",
       "      <th>Sigma</th>\n",
       "      <th>Tau</th>\n",
       "      <th>Impact</th>\n",
       "      <th>Pred_Speed</th>\n",
       "      <th>Pred_Flow</th>\n",
       "      <th>Pred_Occupancy</th>\n",
       "      <th>Diff_Speed</th>\n",
       "      <th>Diff_Flow</th>\n",
       "      <th>Diff_Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9792</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-05 00:00:00</td>\n",
       "      <td>-0.980169</td>\n",
       "      <td>-1.341644</td>\n",
       "      <td>0.511248</td>\n",
       "      <td>06/05/2017</td>\n",
       "      <td>00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>-10.449204</td>\n",
       "      <td>-1.613481</td>\n",
       "      <td>-1.065</td>\n",
       "      <td>17.902598</td>\n",
       "      <td>1.261557</td>\n",
       "      <td>0.388933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9793</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-05 00:05:00</td>\n",
       "      <td>-1.008772</td>\n",
       "      <td>-1.393785</td>\n",
       "      <td>0.185775</td>\n",
       "      <td>06/05/2017</td>\n",
       "      <td>00:05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009436</td>\n",
       "      <td>-10.449204</td>\n",
       "      <td>-1.613481</td>\n",
       "      <td>-1.065</td>\n",
       "      <td>17.485848</td>\n",
       "      <td>1.036034</td>\n",
       "      <td>0.323992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9794</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-05 00:10:00</td>\n",
       "      <td>-0.980169</td>\n",
       "      <td>-1.341644</td>\n",
       "      <td>0.450222</td>\n",
       "      <td>06/05/2017</td>\n",
       "      <td>00:10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010917</td>\n",
       "      <td>-10.449204</td>\n",
       "      <td>-1.613481</td>\n",
       "      <td>-1.065</td>\n",
       "      <td>17.824457</td>\n",
       "      <td>1.261557</td>\n",
       "      <td>0.388933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Station ID             datetime  Occupancy      Flow     Speed  \\\n",
       "9792      408907  2017-06-05 00:00:00  -0.980169 -1.341644  0.511248   \n",
       "9793      408907  2017-06-05 00:05:00  -1.008772 -1.393785  0.185775   \n",
       "9794      408907  2017-06-05 00:10:00  -0.980169 -1.341644  0.450222   \n",
       "\n",
       "            Date   Time  LambdaMax  Sigma  Tau    Impact  Pred_Speed  \\\n",
       "9792  06/05/2017  00:00        0.0    0.0  0.0  0.012451  -10.449204   \n",
       "9793  06/05/2017  00:05        0.0    0.0  0.0  0.009436  -10.449204   \n",
       "9794  06/05/2017  00:10        0.0    0.0  0.0  0.010917  -10.449204   \n",
       "\n",
       "      Pred_Flow  Pred_Occupancy  Diff_Speed  Diff_Flow  Diff_Occupancy  \n",
       "9792  -1.613481          -1.065   17.902598   1.261557        0.388933  \n",
       "9793  -1.613481          -1.065   17.485848   1.036034        0.323992  \n",
       "9794  -1.613481          -1.065   17.824457   1.261557        0.388933  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to scale train and test dataset with the same factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train: feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train: feature vectors - negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_times = raw_train['Time'].unique().tolist()[14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sample_dates = dates_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_incidents_sample = svm_pos_timestamps_train.loc[svm_pos_timestamps_train['Date'].isin(neg_sample_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/17] Negative feature vectors at date 06/01/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/01/2017 01:10 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/01/2017 23:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/01/2017 18:40 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/01/2017 06:50 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/01/2017 17:15 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[2/17] Negative feature vectors at date 06/02/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/02/2017 08:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/02/2017 08:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/02/2017 01:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/02/2017 03:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/02/2017 04:15 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[3/17] Negative feature vectors at date 06/03/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/03/2017 17:10 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/03/2017 10:00 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/03/2017 16:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/03/2017 02:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/03/2017 22:25 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[4/17] Negative feature vectors at date 06/04/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/04/2017 17:05 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/04/2017 07:15 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/04/2017 08:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/04/2017 20:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/04/2017 11:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[5/17] Negative feature vectors at date 06/06/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/06/2017 22:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/06/2017 14:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/06/2017 04:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/06/2017 05:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/06/2017 17:10 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[6/17] Negative feature vectors at date 06/08/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/08/2017 20:10 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/08/2017 08:15 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/08/2017 22:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/08/2017 17:25 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Feature vector at date and time 06/08/2017 12:05 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[7/17] Negative feature vectors at date 06/09/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/09/2017 07:35 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/09/2017 05:15 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/09/2017 08:50 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/09/2017 09:40 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/09/2017 13:50 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[8/17] Negative feature vectors at date 06/10/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/10/2017 05:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/10/2017 21:15 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/10/2017 07:05 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/10/2017 05:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/10/2017 13:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[9/17] Negative feature vectors at date 06/13/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/13/2017 10:15 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/13/2017 10:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/13/2017 10:05 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/13/2017 18:35 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/13/2017 04:00 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[10/17] Negative feature vectors at date 06/19/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/19/2017 11:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/19/2017 17:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/19/2017 04:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/19/2017 19:40 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/19/2017 20:50 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[11/17] Negative feature vectors at date 06/20/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/20/2017 07:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/20/2017 12:10 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/20/2017 01:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/20/2017 07:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/20/2017 09:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[12/17] Negative feature vectors at date 06/21/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/21/2017 09:10 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/21/2017 16:05 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/21/2017 16:35 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/21/2017 17:40 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Feature vector at date and time 06/21/2017 08:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[13/17] Negative feature vectors at date 06/22/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/22/2017 21:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/22/2017 09:40 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/22/2017 10:00 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/22/2017 04:40 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/22/2017 23:50 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[14/17] Negative feature vectors at date 06/23/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/23/2017 07:25 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/23/2017 15:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/23/2017 13:40 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/23/2017 11:35 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/23/2017 04:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[15/17] Negative feature vectors at date 06/27/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/27/2017 16:40 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/27/2017 07:40 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/27/2017 18:40 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/27/2017 16:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/27/2017 15:15 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[16/17] Negative feature vectors at date 06/29/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/29/2017 07:10 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/29/2017 18:00 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/29/2017 18:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/29/2017 04:50 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/29/2017 12:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[17/17] Negative feature vectors at date 06/30/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/30/2017 13:10 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/30/2017 20:15 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/30/2017 11:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/30/2017 21:05 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/30/2017 11:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n"
     ]
    }
   ],
   "source": [
    "X_neg_weekday_train = []\n",
    "X_neg_weekend_train = []\n",
    "num_segments = len(road_segments)\n",
    "count_date = 0\n",
    "for neg_sample_date in neg_sample_dates:\n",
    "    count_date += 1\n",
    "    print(\"{} Negative feature vectors at date {}:\".format(fraction_msg(count_date, len(neg_sample_dates)), neg_sample_date))\n",
    "    \n",
    "    for i, seg in enumerate(road_segments):\n",
    "        B, E = seg\n",
    "        df_neg_train_BE = raw_train.loc[((raw_train[\"Station ID\"] == B) | (raw_train[\"Station ID\"] == E)) & (raw_train[\"Date\"] == neg_sample_date)]\n",
    "        svm_incidents_sample_BE = svm_incidents_sample.loc[svm_incidents_sample['Upstream'] == B]\n",
    "        sample_neg_times = np.random.choice(neg_times, 24)\n",
    "        \n",
    "        if (i+1) % 20 == 0:\n",
    "            print(\"    {} Start constructing feature vectors for road segment s_{},{}...\".format(fraction_msg(i+1, num_segments), B, E))\n",
    "            print(\"        Total number of vectors: {}\".format(len(sample_neg_times)))\n",
    "        \n",
    "        for neg_t in sample_neg_times:\n",
    "            # check if current time is incident time\n",
    "            if len(svm_incidents_sample_BE.loc[svm_incidents_sample_BE['Time'] == neg_t].index) != 0:\n",
    "                continue\n",
    "\n",
    "            feature_t = []\n",
    "            neg_dt_timestamp = pd.Timestamp(neg_sample_date + ' ' + neg_t + ':00')\n",
    "\n",
    "            B_lags = []\n",
    "            for j in range(5):\n",
    "                B_lags.append(neg_dt_timestamp - dt.timedelta(minutes=j*5))\n",
    "            B_lags = list(map(lambda x: x.strftime('%H:%M') , B_lags))\n",
    "            E_lags = B_lags[0:3]\n",
    "\n",
    "            # upstream features\n",
    "            for t_lag in B_lags:\n",
    "                df_dt_lag = df_neg_train_BE.loc[(df_neg_train_BE[\"Station ID\"] == B) & (df_neg_train_BE[\"Time\"] == t_lag)]\n",
    "\n",
    "                speed_B_t = df_dt_lag[\"Speed\"].values[0]\n",
    "                flow_B_t = df_dt_lag[\"Flow\"].values[0]\n",
    "                occ_B_t = df_dt_lag[\"Occupancy\"].values[0]\n",
    "\n",
    "                speed_pred_B_t = df_dt_lag[\"Pred_Speed\"].values[0]\n",
    "                flow_pred_B_t = df_dt_lag[\"Pred_Flow\"].values[0]\n",
    "                occ_pred_B_t = df_dt_lag[\"Pred_Occupancy\"].values[0]\n",
    "                \n",
    "                speed_diff_B_t = df_dt_lag[\"Diff_Speed\"].values[0]\n",
    "                flow_diff_B_t = df_dt_lag[\"Diff_Flow\"].values[0]\n",
    "                occ_diff_B_t = df_dt_lag[\"Diff_Occupancy\"].values[0]\n",
    "                \n",
    "                lambda_max_B_t = df_dt_lag[\"LambdaMax\"].values[0]\n",
    "                sigma_B_t = df_dt_lag[\"Sigma\"].values[0]\n",
    "                tau_B_t = df_dt_lag[\"Tau\"].values[0]\n",
    "                impact_B_t = df_dt_lag[\"Impact\"].values[0]\n",
    "\n",
    "                feature_t.extend([speed_B_t, flow_B_t, occ_B_t, \n",
    "                                  speed_pred_B_t, flow_pred_B_t, occ_pred_B_t, \n",
    "                                  speed_diff_B_t, flow_diff_B_t, occ_diff_B_t, \n",
    "                                  lambda_max_B_t, sigma_B_t, tau_B_t, impact_B_t])\n",
    "\n",
    "            # downstream features\n",
    "            for t_lag in E_lags:\n",
    "                df_dt_lag = df_neg_train_BE.loc[(df_neg_train_BE[\"Station ID\"] == E) & (df_neg_train_BE[\"Time\"] == t_lag)]\n",
    "\n",
    "                speed_E_t = df_dt_lag[\"Speed\"].values[0]\n",
    "                flow_E_t = df_dt_lag[\"Flow\"].values[0]\n",
    "                occ_E_t = df_dt_lag[\"Occupancy\"].values[0]\n",
    "\n",
    "                speed_pred_E_t = df_dt_lag[\"Pred_Speed\"].values[0]\n",
    "                flow_pred_E_t = df_dt_lag[\"Pred_Flow\"].values[0]\n",
    "                occ_pred_E_t = df_dt_lag[\"Pred_Occupancy\"].values[0]\n",
    "\n",
    "                speed_diff_E_t = df_dt_lag[\"Diff_Speed\"].values[0]\n",
    "                flow_diff_E_t = df_dt_lag[\"Diff_Flow\"].values[0]\n",
    "                occ_diff_E_t = df_dt_lag[\"Diff_Occupancy\"].values[0]\n",
    "                \n",
    "                lambda_max_E_t = df_dt_lag[\"LambdaMax\"].values[0]\n",
    "                sigma_E_t = df_dt_lag[\"Sigma\"].values[0]\n",
    "                tau_E_t = df_dt_lag[\"Tau\"].values[0]\n",
    "                impact_E_t = df_dt_lag[\"Impact\"].values[0]\n",
    "\n",
    "                feature_t.extend([speed_E_t, flow_E_t, occ_E_t, \n",
    "                                  speed_pred_E_t, flow_pred_E_t, occ_pred_E_t, \n",
    "                                  speed_diff_E_t, flow_diff_E_t, occ_diff_E_t, \n",
    "                                  lambda_max_E_t, sigma_E_t, tau_E_t, impact_E_t])\n",
    "            \n",
    "            if date_to_day(neg_sample_date) >= 5:\n",
    "                X_neg_weekend_train.append(feature_t)\n",
    "            else:\n",
    "                X_neg_weekday_train.append(feature_t)\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            print(\"        Feature vector at date and time {} {} is done.\".format(neg_sample_date, neg_t))\n",
    "            print(\"    ...Completed construction of feature vectors for road segment s_{},{}.\".format(B, E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31716, 6788)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_neg_weekday_train), len(X_neg_weekend_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_neg_weekday_train = [-1] * len(X_neg_weekday_train)\n",
    "y_neg_weekend_train = [-1] * len(X_neg_weekend_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train: feature vectors - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_time = raw_train['Time'].unique().tolist()[14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pos_timestamps_train = svm_pos_timestamps_train.loc[svm_pos_timestamps_train['Time'].isin(working_time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/101] Start constructing positive feature vectors for road segment s_408907,400951... \n",
      "    Total number of vectors: 0\n",
      "[2/101] Start constructing positive feature vectors for road segment s_400951,400057... \n",
      "    Total number of vectors: 43\n",
      "[3/101] Start constructing positive feature vectors for road segment s_400057,400147... \n",
      "    Total number of vectors: 22\n",
      "[4/101] Start constructing positive feature vectors for road segment s_400147,400343... \n",
      "    Total number of vectors: 21\n",
      "[5/101] Start constructing positive feature vectors for road segment s_400343,401560... \n",
      "    Total number of vectors: 12\n",
      "[6/101] Start constructing positive feature vectors for road segment s_401560,400045... \n",
      "    Total number of vectors: 14\n",
      "[7/101] Start constructing positive feature vectors for road segment s_400045,400122... \n",
      "    Total number of vectors: 0\n",
      "[8/101] Start constructing positive feature vectors for road segment s_400122,401541... \n",
      "    Total number of vectors: 5\n",
      "[9/101] Start constructing positive feature vectors for road segment s_401541,402281... \n",
      "    Total number of vectors: 38\n",
      "[10/101] Start constructing positive feature vectors for road segment s_402281,402283... \n",
      "    Total number of vectors: 0\n",
      "[11/101] Start constructing positive feature vectors for road segment s_402283,402285... \n",
      "    Total number of vectors: 12\n",
      "[12/101] Start constructing positive feature vectors for road segment s_402285,402286... \n",
      "    Total number of vectors: 0\n",
      "[13/101] Start constructing positive feature vectors for road segment s_402286,400088... \n",
      "    Total number of vectors: 38\n",
      "[14/101] Start constructing positive feature vectors for road segment s_400088,402288... \n",
      "    Total number of vectors: 0\n",
      "[15/101] Start constructing positive feature vectors for road segment s_402288,413026... \n",
      "    Total number of vectors: 15\n",
      "[16/101] Start constructing positive feature vectors for road segment s_413026,401464... \n",
      "    Total number of vectors: 0\n",
      "[17/101] Start constructing positive feature vectors for road segment s_401464,401489... \n",
      "    Total number of vectors: 0\n",
      "[18/101] Start constructing positive feature vectors for road segment s_401489,401538... \n",
      "    Total number of vectors: 0\n",
      "[19/101] Start constructing positive feature vectors for road segment s_401538,402290... \n",
      "    Total number of vectors: 8\n",
      "[20/101] Start constructing positive feature vectors for road segment s_402290,402292... \n",
      "    Total number of vectors: 0\n",
      "[21/101] Start constructing positive feature vectors for road segment s_402292,401643... \n",
      "    Total number of vectors: 120\n",
      "    [100/120] Feature vector at date and time 06/23/2017 11:55 is done.\n",
      "[22/101] Start constructing positive feature vectors for road segment s_401643,402800... \n",
      "    Total number of vectors: 4\n",
      "[23/101] Start constructing positive feature vectors for road segment s_402800,402828... \n",
      "    Total number of vectors: 0\n",
      "[24/101] Start constructing positive feature vectors for road segment s_402828,407219... \n",
      "    Total number of vectors: 34\n",
      "[25/101] Start constructing positive feature vectors for road segment s_407219,402789... \n",
      "    Total number of vectors: 0\n",
      "[26/101] Start constructing positive feature vectors for road segment s_402789,408755... \n",
      "    Total number of vectors: 14\n",
      "[27/101] Start constructing positive feature vectors for road segment s_408755,402802... \n",
      "    Total number of vectors: 0\n",
      "[28/101] Start constructing positive feature vectors for road segment s_402802,408756... \n",
      "    Total number of vectors: 0\n",
      "[29/101] Start constructing positive feature vectors for road segment s_408756,400189... \n",
      "    Total number of vectors: 94\n",
      "[30/101] Start constructing positive feature vectors for road segment s_400189,400309... \n",
      "    Total number of vectors: 14\n",
      "[31/101] Start constructing positive feature vectors for road segment s_400309,400417... \n",
      "    Total number of vectors: 0\n",
      "[32/101] Start constructing positive feature vectors for road segment s_400417,400249... \n",
      "    Total number of vectors: 12\n",
      "[33/101] Start constructing positive feature vectors for road segment s_400249,401639... \n",
      "    Total number of vectors: 27\n",
      "[34/101] Start constructing positive feature vectors for road segment s_401639,400662... \n",
      "    Total number of vectors: 3\n",
      "[35/101] Start constructing positive feature vectors for road segment s_400662,400141... \n",
      "    Total number of vectors: 4\n",
      "[36/101] Start constructing positive feature vectors for road segment s_400141,400761... \n",
      "    Total number of vectors: 0\n",
      "[37/101] Start constructing positive feature vectors for road segment s_400761,400490... \n",
      "    Total number of vectors: 3\n",
      "[38/101] Start constructing positive feature vectors for road segment s_400490,401888... \n",
      "    Total number of vectors: 5\n",
      "[39/101] Start constructing positive feature vectors for road segment s_401888,400137... \n",
      "    Total number of vectors: 94\n",
      "[40/101] Start constructing positive feature vectors for road segment s_400137,400716... \n",
      "    Total number of vectors: 88\n",
      "[41/101] Start constructing positive feature vectors for road segment s_400716,401545... \n",
      "    Total number of vectors: 9\n",
      "[42/101] Start constructing positive feature vectors for road segment s_401545,401011... \n",
      "    Total number of vectors: 31\n",
      "[43/101] Start constructing positive feature vectors for road segment s_401011,400674... \n",
      "    Total number of vectors: 12\n",
      "[44/101] Start constructing positive feature vectors for road segment s_400674,400539... \n",
      "    Total number of vectors: 6\n",
      "[45/101] Start constructing positive feature vectors for road segment s_400539,400534... \n",
      "    Total number of vectors: 12\n",
      "[46/101] Start constructing positive feature vectors for road segment s_400534,401062... \n",
      "    Total number of vectors: 0\n",
      "[47/101] Start constructing positive feature vectors for road segment s_401062,401529... \n",
      "    Total number of vectors: 0\n",
      "[48/101] Start constructing positive feature vectors for road segment s_401529,401613... \n",
      "    Total number of vectors: 0\n",
      "[49/101] Start constructing positive feature vectors for road segment s_401613,400536... \n",
      "    Total number of vectors: 9\n",
      "[50/101] Start constructing positive feature vectors for road segment s_400536,400488... \n",
      "    Total number of vectors: 30\n",
      "[51/101] Start constructing positive feature vectors for road segment s_400488,401561... \n",
      "    Total number of vectors: 79\n",
      "[52/101] Start constructing positive feature vectors for road segment s_401561,400611... \n",
      "    Total number of vectors: 0\n",
      "[53/101] Start constructing positive feature vectors for road segment s_400611,400928... \n",
      "    Total number of vectors: 7\n",
      "[54/101] Start constructing positive feature vectors for road segment s_400928,400284... \n",
      "    Total number of vectors: 51\n",
      "[55/101] Start constructing positive feature vectors for road segment s_400284,400041... \n",
      "    Total number of vectors: 0\n",
      "[56/101] Start constructing positive feature vectors for road segment s_400041,408133... \n",
      "    Total number of vectors: 4\n",
      "[57/101] Start constructing positive feature vectors for road segment s_408133,408135... \n",
      "    Total number of vectors: 0\n",
      "[58/101] Start constructing positive feature vectors for road segment s_408135,417665... \n",
      "    Total number of vectors: 35\n",
      "[59/101] Start constructing positive feature vectors for road segment s_417665,412637... \n",
      "    Total number of vectors: 0\n",
      "[60/101] Start constructing positive feature vectors for road segment s_412637,417666... \n",
      "    Total number of vectors: 0\n",
      "[61/101] Start constructing positive feature vectors for road segment s_417666,408134... \n",
      "    Total number of vectors: 0\n",
      "[62/101] Start constructing positive feature vectors for road segment s_408134,400685... \n",
      "    Total number of vectors: 25\n",
      "[63/101] Start constructing positive feature vectors for road segment s_400685,401003... \n",
      "    Total number of vectors: 0\n",
      "[64/101] Start constructing positive feature vectors for road segment s_401003,400898... \n",
      "    Total number of vectors: 9\n",
      "[65/101] Start constructing positive feature vectors for road segment s_400898,400275... \n",
      "    Total number of vectors: 15\n",
      "[66/101] Start constructing positive feature vectors for road segment s_400275,400939... \n",
      "    Total number of vectors: 9\n",
      "[67/101] Start constructing positive feature vectors for road segment s_400939,400180... \n",
      "    Total number of vectors: 0\n",
      "[68/101] Start constructing positive feature vectors for road segment s_400180,400529... \n",
      "    Total number of vectors: 0\n",
      "[69/101] Start constructing positive feature vectors for road segment s_400529,400990... \n",
      "    Total number of vectors: 0\n",
      "[70/101] Start constructing positive feature vectors for road segment s_400990,400515... \n",
      "    Total number of vectors: 0\n",
      "[71/101] Start constructing positive feature vectors for road segment s_400515,400252... \n",
      "    Total number of vectors: 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72/101] Start constructing positive feature vectors for road segment s_400252,400788... \n",
      "    Total number of vectors: 64\n",
      "[73/101] Start constructing positive feature vectors for road segment s_400788,401517... \n",
      "    Total number of vectors: 56\n",
      "[74/101] Start constructing positive feature vectors for road segment s_401517,401871... \n",
      "    Total number of vectors: 0\n",
      "[75/101] Start constructing positive feature vectors for road segment s_401871,400574... \n",
      "    Total number of vectors: 22\n",
      "[76/101] Start constructing positive feature vectors for road segment s_400574,401629... \n",
      "    Total number of vectors: 24\n",
      "[77/101] Start constructing positive feature vectors for road segment s_401629,400422... \n",
      "    Total number of vectors: 64\n",
      "[78/101] Start constructing positive feature vectors for road segment s_400422,400333... \n",
      "    Total number of vectors: 0\n",
      "[79/101] Start constructing positive feature vectors for road segment s_400333,410363... \n",
      "    Total number of vectors: 7\n",
      "[80/101] Start constructing positive feature vectors for road segment s_410363,400360... \n",
      "    Total number of vectors: 53\n",
      "[81/101] Start constructing positive feature vectors for road segment s_400360,400955... \n",
      "    Total number of vectors: 0\n",
      "[82/101] Start constructing positive feature vectors for road segment s_400955,400495... \n",
      "    Total number of vectors: 12\n",
      "[83/101] Start constructing positive feature vectors for road segment s_400495,400608... \n",
      "    Total number of vectors: 0\n",
      "[84/101] Start constructing positive feature vectors for road segment s_400608,400949... \n",
      "    Total number of vectors: 90\n",
      "[85/101] Start constructing positive feature vectors for road segment s_400949,400678... \n",
      "    Total number of vectors: 126\n",
      "    [100/126] Feature vector at date and time 06/30/2017 13:30 is done.\n",
      "[86/101] Start constructing positive feature vectors for road segment s_400678,400341... \n",
      "    Total number of vectors: 0\n",
      "[87/101] Start constructing positive feature vectors for road segment s_400341,400607... \n",
      "    Total number of vectors: 9\n",
      "[88/101] Start constructing positive feature vectors for road segment s_400607,400094... \n",
      "    Total number of vectors: 16\n",
      "[89/101] Start constructing positive feature vectors for road segment s_400094,400682... \n",
      "    Total number of vectors: 0\n",
      "[90/101] Start constructing positive feature vectors for road segment s_400682,408138... \n",
      "    Total number of vectors: 85\n",
      "[91/101] Start constructing positive feature vectors for road segment s_408138,400980... \n",
      "    Total number of vectors: 10\n",
      "[92/101] Start constructing positive feature vectors for road segment s_400980,401333... \n",
      "    Total number of vectors: 55\n",
      "[93/101] Start constructing positive feature vectors for road segment s_401333,404746... \n",
      "    Total number of vectors: 0\n",
      "[94/101] Start constructing positive feature vectors for road segment s_404746,401142... \n",
      "    Total number of vectors: 0\n",
      "[95/101] Start constructing positive feature vectors for road segment s_401142,400218... \n",
      "    Total number of vectors: 0\n",
      "[96/101] Start constructing positive feature vectors for road segment s_400218,400983... \n",
      "    Total number of vectors: 24\n",
      "[97/101] Start constructing positive feature vectors for road segment s_400983,400765... \n",
      "    Total number of vectors: 0\n",
      "[98/101] Start constructing positive feature vectors for road segment s_400765,400844... \n",
      "    Total number of vectors: 13\n",
      "[99/101] Start constructing positive feature vectors for road segment s_400844,400923... \n",
      "    Total number of vectors: 0\n",
      "[100/101] Start constructing positive feature vectors for road segment s_400923,401143... \n",
      "    Total number of vectors: 0\n",
      "[101/101] Start constructing positive feature vectors for road segment s_401143,401471... \n",
      "    Total number of vectors: 0\n",
      "...Completed construction of feature vectors for road segment s_401143,401471.\n"
     ]
    }
   ],
   "source": [
    "X_pos_weekday_train = []\n",
    "X_pos_weekend_train = []\n",
    "for i, seg in enumerate(road_segments):\n",
    "    B, E = seg\n",
    "    print(\"{} Start constructing positive feature vectors for road segment s_{},{}... \".format(fraction_msg(i+1, len(road_segments)), B, E))\n",
    "    progress_count = 0\n",
    "    \n",
    "    # construct segment-specific pos_times\n",
    "    pos_times = []\n",
    "    df_seg_incidents = svm_pos_timestamps_train.loc[svm_pos_timestamps_train[\"Upstream\"] == B]\n",
    "    seg_dates = df_seg_incidents['Date'].values.tolist()\n",
    "    seg_times = df_seg_incidents['Time'].values.tolist()\n",
    "    num_seg_instances = len(seg_dates)\n",
    "    for i in range(num_seg_instances):\n",
    "        pos_times.append(tuple([seg_dates[i], seg_times[i]]))\n",
    "    \n",
    "    # select the relevant training data for segment B, E \n",
    "    df_train_BE = raw_train.loc[(raw_train[\"Station ID\"] == B) | (raw_train[\"Station ID\"] == E)]\n",
    "    \n",
    "    \n",
    "    print(\"    Total number of vectors: {}\".format(num_seg_instances))\n",
    "    for pos_dt in pos_times:\n",
    "        pos_d, pos_t = pos_dt\n",
    "        feature_t = []\n",
    "        pos_dt_timestamp = pd.Timestamp(pos_d + ' ' + pos_t + ':00')\n",
    "\n",
    "        # upstream and downstream time lags\n",
    "        B_lags = []\n",
    "        for j in range(5):\n",
    "            B_lags.append(pos_dt_timestamp - dt.timedelta(minutes=j*5))\n",
    "        B_lags = list(map(lambda x: (x.strftime('%m/%d/%Y'), x.strftime('%H:%M')) , B_lags))\n",
    "        E_lags = B_lags[0:3]\n",
    "\n",
    "        # upstream features\n",
    "        for dt_lag in B_lags:\n",
    "            d_lag, t_lag = dt_lag\n",
    "            df_dt_lag = df_train_BE.loc[(df_train_BE[\"Station ID\"] == B) & (df_train_BE[\"Date\"] == d_lag) & (df_train_BE[\"Time\"] == t_lag)]\n",
    "            if df_dt_lag.empty:\n",
    "                print(d_lag, t_lag)\n",
    "            \n",
    "            speed_B_t = df_dt_lag[\"Speed\"].values[0]\n",
    "            flow_B_t = df_dt_lag[\"Flow\"].values[0]\n",
    "            occ_B_t = df_dt_lag[\"Occupancy\"].values[0]\n",
    "\n",
    "            speed_pred_B_t = df_dt_lag[\"Pred_Speed\"].values[0]\n",
    "            flow_pred_B_t = df_dt_lag[\"Pred_Flow\"].values[0]\n",
    "            occ_pred_B_t = df_dt_lag[\"Pred_Occupancy\"].values[0]\n",
    "            \n",
    "            speed_diff_B_t = df_dt_lag[\"Diff_Speed\"].values[0]\n",
    "            flow_diff_B_t = df_dt_lag[\"Diff_Flow\"].values[0]\n",
    "            occ_diff_B_t = df_dt_lag[\"Diff_Occupancy\"].values[0]\n",
    "            \n",
    "            lambda_max_B_t = df_dt_lag[\"LambdaMax\"].values[0]\n",
    "            sigma_B_t = df_dt_lag[\"Sigma\"].values[0]\n",
    "            tau_B_t = df_dt_lag[\"Tau\"].values[0]\n",
    "            impact_B_t = df_dt_lag[\"Impact\"].values[0]\n",
    "\n",
    "            feature_t.extend([speed_B_t, flow_B_t, occ_B_t, \n",
    "                              speed_pred_B_t, flow_pred_B_t, occ_pred_B_t, \n",
    "                              speed_diff_B_t, flow_diff_B_t, occ_diff_B_t, \n",
    "                              lambda_max_B_t, sigma_B_t, tau_B_t, impact_B_t])\n",
    "\n",
    "        # downstream features\n",
    "        for dt_lag in E_lags:\n",
    "            d_lag, t_lag = dt_lag\n",
    "            df_dt_lag = df_train_BE.loc[(df_train_BE[\"Station ID\"] == E) & (df_train_BE[\"Date\"] == d_lag) & (df_train_BE[\"Time\"] == t_lag)]\n",
    "\n",
    "            speed_E_t = df_dt_lag[\"Speed\"].values[0]\n",
    "            flow_E_t = df_dt_lag[\"Flow\"].values[0]\n",
    "            occ_E_t = df_dt_lag[\"Occupancy\"].values[0]\n",
    "\n",
    "            speed_pred_E_t = df_dt_lag[\"Pred_Speed\"].values[0]\n",
    "            flow_pred_E_t = df_dt_lag[\"Pred_Flow\"].values[0]\n",
    "            occ_pred_E_t = df_dt_lag[\"Pred_Occupancy\"].values[0]\n",
    "            \n",
    "            speed_diff_E_t = df_dt_lag[\"Diff_Speed\"].values[0]\n",
    "            flow_diff_E_t = df_dt_lag[\"Diff_Flow\"].values[0]\n",
    "            occ_diff_E_t = df_dt_lag[\"Diff_Occupancy\"].values[0]\n",
    "            \n",
    "            lambda_max_E_t = df_dt_lag[\"LambdaMax\"].values[0]\n",
    "            sigma_E_t = df_dt_lag[\"Sigma\"].values[0]\n",
    "            tau_E_t = df_dt_lag[\"Tau\"].values[0]\n",
    "            impact_E_t = df_dt_lag[\"Impact\"].values[0]\n",
    "            \n",
    "            feature_t.extend([speed_E_t, flow_E_t, occ_E_t, \n",
    "                              speed_pred_E_t, flow_pred_E_t, occ_pred_E_t, \n",
    "                              speed_diff_E_t, flow_diff_E_t, occ_diff_E_t, \n",
    "                              lambda_max_E_t, sigma_E_t, tau_E_t, impact_E_t])\n",
    "        \n",
    "        if date_to_day(pos_d) >= 5:\n",
    "            X_pos_weekend_train.append(feature_t)\n",
    "        else:\n",
    "            X_pos_weekday_train.append(feature_t)\n",
    "        \n",
    "        progress_count += 1\n",
    "        if progress_count % 100 == 0:\n",
    "            print(\"    {} Feature vector at date and time {} {} is done.\".format(fraction_msg(progress_count, num_seg_instances), pos_d, pos_t))\n",
    "\n",
    "print(\"...Completed construction of feature vectors for road segment s_{},{}.\".format(B, E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1644, 253)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_pos_weekday_train), len(X_pos_weekend_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos_weekday_train = [1] * len(X_pos_weekday_train)\n",
    "y_pos_weekend_train = [1] * len(X_pos_weekend_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train: Merging feature vectors together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_neg_weekday_train = np.array(X_neg_weekday_train)\n",
    "X_neg_weekend_train = np.array(X_neg_weekend_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_neg_weekday_train_balanced = X_neg_weekday_train[np.random.choice(len(X_neg_weekday_train), len(X_pos_weekday_train), replace=False)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_neg_weekday_train_balanced = [-1] * len(X_neg_weekday_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_weekday = X_neg_weekday_train_balanced + X_pos_weekday_train\n",
    "#X_train_weekday_unbalanced = np.concatenate((X_neg_weekday_train, X_pos_weekday_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_weekday = y_neg_weekday_train_balanced + y_pos_weekday_train\n",
    "#y_train_weekday_unbalanced = y_neg_weekday_train + y_pos_weekday_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(X_train_weekday_unbalanced), len(y_train_weekday_unbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3288, 3288)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_weekday), len(y_train_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_weekday[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_neg_weekend_train_balanced = X_neg_weekend_train[np.random.choice(len(X_neg_weekend_train), len(X_pos_weekend_train), replace=False)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_neg_weekend_train_balanced = [-1] * len(X_neg_weekend_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_weekend = X_neg_weekend_train_balanced + X_pos_weekend_train\n",
    "#X_train_weekend_unbalanced = np.concatenate((X_neg_weekend_train, X_pos_weekend_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_weekend = y_neg_weekend_train_balanced + y_pos_weekend_train\n",
    "#y_train_weekend_unbalanced = y_neg_weekend_train + y_pos_weekend_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(X_train_weekend_unbalanced), len(y_train_weekend_unbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 506)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_weekend), len(y_train_weekend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station ID</th>\n",
       "      <th>datetime</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>Flow</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>LambdaMax</th>\n",
       "      <th>Sigma</th>\n",
       "      <th>Tau</th>\n",
       "      <th>Impact</th>\n",
       "      <th>Pred_Speed</th>\n",
       "      <th>Pred_Flow</th>\n",
       "      <th>Pred_Occupancy</th>\n",
       "      <th>Diff_Speed</th>\n",
       "      <th>Diff_Flow</th>\n",
       "      <th>Diff_Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6532125</th>\n",
       "      <td>401471</td>\n",
       "      <td>2017-06-28 23:45:00</td>\n",
       "      <td>-1.149112</td>\n",
       "      <td>-1.207491</td>\n",
       "      <td>0.885074</td>\n",
       "      <td>06/28/2017</td>\n",
       "      <td>23:45</td>\n",
       "      <td>0.038882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064840</td>\n",
       "      <td>0.295204</td>\n",
       "      <td>-1.181611</td>\n",
       "      <td>-1.153521</td>\n",
       "      <td>-0.025416</td>\n",
       "      <td>-0.300943</td>\n",
       "      <td>0.187732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6532126</th>\n",
       "      <td>401471</td>\n",
       "      <td>2017-06-28 23:50:00</td>\n",
       "      <td>-1.217922</td>\n",
       "      <td>-1.207491</td>\n",
       "      <td>0.790361</td>\n",
       "      <td>06/28/2017</td>\n",
       "      <td>23:50</td>\n",
       "      <td>0.038882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064408</td>\n",
       "      <td>0.315483</td>\n",
       "      <td>-1.220174</td>\n",
       "      <td>-1.160351</td>\n",
       "      <td>-0.076551</td>\n",
       "      <td>0.317241</td>\n",
       "      <td>-0.712596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6532127</th>\n",
       "      <td>401471</td>\n",
       "      <td>2017-06-28 23:55:00</td>\n",
       "      <td>-1.217922</td>\n",
       "      <td>-1.283552</td>\n",
       "      <td>0.695648</td>\n",
       "      <td>06/28/2017</td>\n",
       "      <td>23:55</td>\n",
       "      <td>0.038882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063334</td>\n",
       "      <td>0.300998</td>\n",
       "      <td>-1.225277</td>\n",
       "      <td>-1.214997</td>\n",
       "      <td>-0.091591</td>\n",
       "      <td>-0.813065</td>\n",
       "      <td>0.087695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Station ID             datetime  Occupancy      Flow     Speed  \\\n",
       "6532125      401471  2017-06-28 23:45:00  -1.149112 -1.207491  0.885074   \n",
       "6532126      401471  2017-06-28 23:50:00  -1.217922 -1.207491  0.790361   \n",
       "6532127      401471  2017-06-28 23:55:00  -1.217922 -1.283552  0.695648   \n",
       "\n",
       "               Date   Time  LambdaMax  Sigma  Tau    Impact  Pred_Speed  \\\n",
       "6532125  06/28/2017  23:45   0.038882    0.0  0.0  0.064840    0.295204   \n",
       "6532126  06/28/2017  23:50   0.038882    0.0  0.0  0.064408    0.315483   \n",
       "6532127  06/28/2017  23:55   0.038882    0.0  0.0  0.063334    0.300998   \n",
       "\n",
       "         Pred_Flow  Pred_Occupancy  Diff_Speed  Diff_Flow  Diff_Occupancy  \n",
       "6532125  -1.181611       -1.153521   -0.025416  -0.300943        0.187732  \n",
       "6532126  -1.220174       -1.160351   -0.076551   0.317241       -0.712596  \n",
       "6532127  -1.225277       -1.214997   -0.091591  -0.813065        0.087695  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Speed', 'Flow', 'Occupancy', \n",
    "                 'Pred_Speed', 'Pred_Flow', 'Pred_Occupancy', \n",
    "                 'Diff_Speed', 'Diff_Flow', 'Diff_Occupancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names.extend(['LambdaMax', 'Sigma', 'Tau', 'Impact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(feature_names)\n",
    "k_B = 4\n",
    "k_E = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/101] Constructing feature vector for segment s_408907,400951...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_408907,400951.\n",
      "[2/101] Constructing feature vector for segment s_400951,400057...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400951,400057.\n",
      "[3/101] Constructing feature vector for segment s_400057,400147...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400057,400147.\n",
      "[4/101] Constructing feature vector for segment s_400147,400343...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400147,400343.\n",
      "[5/101] Constructing feature vector for segment s_400343,401560...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400343,401560.\n",
      "[6/101] Constructing feature vector for segment s_401560,400045...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401560,400045.\n",
      "[7/101] Constructing feature vector for segment s_400045,400122...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400045,400122.\n",
      "[8/101] Constructing feature vector for segment s_400122,401541...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400122,401541.\n",
      "[9/101] Constructing feature vector for segment s_401541,402281...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401541,402281.\n",
      "[10/101] Constructing feature vector for segment s_402281,402283...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402281,402283.\n",
      "[11/101] Constructing feature vector for segment s_402283,402285...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402283,402285.\n",
      "[12/101] Constructing feature vector for segment s_402285,402286...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402285,402286.\n",
      "[13/101] Constructing feature vector for segment s_402286,400088...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402286,400088.\n",
      "[14/101] Constructing feature vector for segment s_400088,402288...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400088,402288.\n",
      "[15/101] Constructing feature vector for segment s_402288,413026...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402288,413026.\n",
      "[16/101] Constructing feature vector for segment s_413026,401464...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_413026,401464.\n",
      "[17/101] Constructing feature vector for segment s_401464,401489...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401464,401489.\n",
      "[18/101] Constructing feature vector for segment s_401489,401538...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401489,401538.\n",
      "[19/101] Constructing feature vector for segment s_401538,402290...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401538,402290.\n",
      "[20/101] Constructing feature vector for segment s_402290,402292...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402290,402292.\n",
      "[21/101] Constructing feature vector for segment s_402292,401643...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402292,401643.\n",
      "[22/101] Constructing feature vector for segment s_401643,402800...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401643,402800.\n",
      "[23/101] Constructing feature vector for segment s_402800,402828...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402800,402828.\n",
      "[24/101] Constructing feature vector for segment s_402828,407219...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402828,407219.\n",
      "[25/101] Constructing feature vector for segment s_407219,402789...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_407219,402789.\n",
      "[26/101] Constructing feature vector for segment s_402789,408755...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402789,408755.\n",
      "[27/101] Constructing feature vector for segment s_408755,402802...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_408755,402802.\n",
      "[28/101] Constructing feature vector for segment s_402802,408756...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402802,408756.\n",
      "[29/101] Constructing feature vector for segment s_408756,400189...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_408756,400189.\n",
      "[30/101] Constructing feature vector for segment s_400189,400309...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400189,400309.\n",
      "[31/101] Constructing feature vector for segment s_400309,400417...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400309,400417.\n",
      "[32/101] Constructing feature vector for segment s_400417,400249...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400417,400249.\n",
      "[33/101] Constructing feature vector for segment s_400249,401639...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400249,401639.\n",
      "[34/101] Constructing feature vector for segment s_401639,400662...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401639,400662.\n",
      "[35/101] Constructing feature vector for segment s_400662,400141...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400662,400141.\n",
      "[36/101] Constructing feature vector for segment s_400141,400761...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400141,400761.\n",
      "[37/101] Constructing feature vector for segment s_400761,400490...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400761,400490.\n",
      "[38/101] Constructing feature vector for segment s_400490,401888...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400490,401888.\n",
      "[39/101] Constructing feature vector for segment s_401888,400137...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401888,400137.\n",
      "[40/101] Constructing feature vector for segment s_400137,400716...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400137,400716.\n",
      "[41/101] Constructing feature vector for segment s_400716,401545...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400716,401545.\n",
      "[42/101] Constructing feature vector for segment s_401545,401011...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401545,401011.\n",
      "[43/101] Constructing feature vector for segment s_401011,400674...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401011,400674.\n",
      "[44/101] Constructing feature vector for segment s_400674,400539...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400674,400539.\n",
      "[45/101] Constructing feature vector for segment s_400539,400534...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400539,400534.\n",
      "[46/101] Constructing feature vector for segment s_400534,401062...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400534,401062.\n",
      "[47/101] Constructing feature vector for segment s_401062,401529...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401062,401529.\n",
      "[48/101] Constructing feature vector for segment s_401529,401613...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401529,401613.\n",
      "[49/101] Constructing feature vector for segment s_401613,400536...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401613,400536.\n",
      "[50/101] Constructing feature vector for segment s_400536,400488...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400536,400488.\n",
      "[51/101] Constructing feature vector for segment s_400488,401561...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400488,401561.\n",
      "[52/101] Constructing feature vector for segment s_401561,400611...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401561,400611.\n",
      "[53/101] Constructing feature vector for segment s_400611,400928...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400611,400928.\n",
      "[54/101] Constructing feature vector for segment s_400928,400284...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400928,400284.\n",
      "[55/101] Constructing feature vector for segment s_400284,400041...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400284,400041.\n",
      "[56/101] Constructing feature vector for segment s_400041,408133...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400041,408133.\n",
      "[57/101] Constructing feature vector for segment s_408133,408135...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_408133,408135.\n",
      "[58/101] Constructing feature vector for segment s_408135,417665...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_408135,417665.\n",
      "[59/101] Constructing feature vector for segment s_417665,412637...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_417665,412637.\n",
      "[60/101] Constructing feature vector for segment s_412637,417666...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_412637,417666.\n",
      "[61/101] Constructing feature vector for segment s_417666,408134...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_417666,408134.\n",
      "[62/101] Constructing feature vector for segment s_408134,400685...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_408134,400685.\n",
      "[63/101] Constructing feature vector for segment s_400685,401003...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400685,401003.\n",
      "[64/101] Constructing feature vector for segment s_401003,400898...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401003,400898.\n",
      "[65/101] Constructing feature vector for segment s_400898,400275...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400898,400275.\n",
      "[66/101] Constructing feature vector for segment s_400275,400939...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400275,400939.\n",
      "[67/101] Constructing feature vector for segment s_400939,400180...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400939,400180.\n",
      "[68/101] Constructing feature vector for segment s_400180,400529...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400180,400529.\n",
      "[69/101] Constructing feature vector for segment s_400529,400990...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400529,400990.\n",
      "[70/101] Constructing feature vector for segment s_400990,400515...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400990,400515.\n",
      "[71/101] Constructing feature vector for segment s_400515,400252...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400515,400252.\n",
      "[72/101] Constructing feature vector for segment s_400252,400788...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400252,400788.\n",
      "[73/101] Constructing feature vector for segment s_400788,401517...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400788,401517.\n",
      "[74/101] Constructing feature vector for segment s_401517,401871...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401517,401871.\n",
      "[75/101] Constructing feature vector for segment s_401871,400574...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401871,400574.\n",
      "[76/101] Constructing feature vector for segment s_400574,401629...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400574,401629.\n",
      "[77/101] Constructing feature vector for segment s_401629,400422...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401629,400422.\n",
      "[78/101] Constructing feature vector for segment s_400422,400333...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400422,400333.\n",
      "[79/101] Constructing feature vector for segment s_400333,410363...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400333,410363.\n",
      "[80/101] Constructing feature vector for segment s_410363,400360...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_410363,400360.\n",
      "[81/101] Constructing feature vector for segment s_400360,400955...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400360,400955.\n",
      "[82/101] Constructing feature vector for segment s_400955,400495...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400955,400495.\n",
      "[83/101] Constructing feature vector for segment s_400495,400608...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400495,400608.\n",
      "[84/101] Constructing feature vector for segment s_400608,400949...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400608,400949.\n",
      "[85/101] Constructing feature vector for segment s_400949,400678...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400949,400678.\n",
      "[86/101] Constructing feature vector for segment s_400678,400341...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400678,400341.\n",
      "[87/101] Constructing feature vector for segment s_400341,400607...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400341,400607.\n",
      "[88/101] Constructing feature vector for segment s_400607,400094...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400607,400094.\n",
      "[89/101] Constructing feature vector for segment s_400094,400682...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400094,400682.\n",
      "[90/101] Constructing feature vector for segment s_400682,408138...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400682,408138.\n",
      "[91/101] Constructing feature vector for segment s_408138,400980...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_408138,400980.\n",
      "[92/101] Constructing feature vector for segment s_400980,401333...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400980,401333.\n",
      "[93/101] Constructing feature vector for segment s_401333,404746...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401333,404746.\n",
      "[94/101] Constructing feature vector for segment s_404746,401142...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_404746,401142.\n",
      "[95/101] Constructing feature vector for segment s_401142,400218...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401142,400218.\n",
      "[96/101] Constructing feature vector for segment s_400218,400983...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400218,400983.\n",
      "[97/101] Constructing feature vector for segment s_400983,400765...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400983,400765.\n",
      "[98/101] Constructing feature vector for segment s_400765,400844...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400765,400844.\n",
      "[99/101] Constructing feature vector for segment s_400844,400923...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400844,400923.\n",
      "[100/101] Constructing feature vector for segment s_400923,401143...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400923,401143.\n",
      "[101/101] Constructing feature vector for segment s_401143,401471...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401143,401471.\n"
     ]
    }
   ],
   "source": [
    "X_test_weekday, X_test_weekend = [], []\n",
    "y_test_weekday, y_test_weekend = [], []\n",
    "for seg_idx, seg in enumerate(road_segments):\n",
    "    B, E = seg\n",
    "    print(\"{} Constructing feature vector for segment s_{},{}...\".format(fraction_msg(seg_idx+1, len(road_segments)), B, E))\n",
    "    df_BE_test = raw_test.loc[((raw_test[\"Station ID\"] == B) | (raw_test[\"Station ID\"] == E))]\n",
    "    df_incidents_BE_test = svm_pos_timestamps_test.loc[svm_pos_timestamps_test[\"Upstream\"] == B]\n",
    "    incidents_BE_date = df_incidents_BE_test[\"Date\"].values\n",
    "    incidents_BE_time = df_incidents_BE_test[\"Time\"].values\n",
    "    \n",
    "    incidents_BE_dt = set()\n",
    "    for i in range(len(incidents_BE_date)):\n",
    "        incidents_BE_dt.add(incidents_BE_date[i] + ' ' + incidents_BE_time[i])\n",
    "    \n",
    "    # change to access by indices, to make program faster\n",
    "    features_BE_dict = dict()\n",
    "    features_BE_dict[B] = dict()\n",
    "    features_BE_dict[E] = dict()\n",
    "    for feature_name in feature_names:\n",
    "        features_BE_dict[B][feature_name] = df_BE_test.loc[df_BE_test[\"Station ID\"] == B][feature_name].values.tolist()\n",
    "        features_BE_dict[E][feature_name] = df_BE_test.loc[df_BE_test[\"Station ID\"] == E][feature_name].values.tolist()\n",
    "    \n",
    "    total_count = len(dates_test) * len(neg_times)\n",
    "    count = 0\n",
    "    print(\"    Total number of instances: {}\".format(total_count)) \n",
    "    \n",
    "    for i, d in enumerate(dates_test):\n",
    "        for j, t in enumerate(neg_times):\n",
    "            # construct vector Z(s_BE, dt)\n",
    "            feature_BE_t = [0.] * (k_B + k_E + 2) * num_features\n",
    "            base_idx = i * 288 + 14 + j\n",
    "            for k, feature_name in enumerate(feature_names):\n",
    "                # feature_k_B_t: [t-4, t-3, ..., t] -> need to be reversed and made consistent with order of SVM features. Same to E.\n",
    "                feature_k_B_t = features_BE_dict[B][feature_name][base_idx-k_B:base_idx+1]\n",
    "                feature_k_E_t = features_BE_dict[E][feature_name][base_idx-k_E:base_idx+1]\n",
    "                feature_k_B_t.reverse()\n",
    "                feature_k_E_t.reverse()\n",
    "                feature_k_BE_t = feature_k_B_t + feature_k_E_t\n",
    "                feature_BE_t[k:(k_B + k_E + 2)*num_features:num_features] = feature_k_BE_t\n",
    "            \n",
    "            if date_to_day(d) >= 5:\n",
    "                X_test_weekend.append(feature_BE_t)\n",
    "                # label data\n",
    "                if d + ' ' + t in incidents_BE_dt:\n",
    "                    y_test_weekend.append(1)\n",
    "                else:\n",
    "                    y_test_weekend.append(-1)\n",
    "            else:\n",
    "                X_test_weekday.append(feature_BE_t)\n",
    "                # label data\n",
    "                if d + ' ' + t in incidents_BE_dt:\n",
    "                    y_test_weekday.append(1)\n",
    "                else:\n",
    "                    y_test_weekday.append(-1)\n",
    "        count += 1\n",
    "        if count % 50 == 0:\n",
    "            print(\"    Progress: {}\" + fraction_msg(count * len(neg_times), total_count))\n",
    "    print(\"...Finished construction for segment s_{},{}.\".format(B, E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193718, 138370, 193718, 138370)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_weekday), len(X_test_weekend), len(y_test_weekday), len(y_test_weekend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM preprocessing: merge train/test and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_weekday_normalized = preprocessing.scale(X_train_weekday + X_test_weekday)\n",
    "X_weekday_normalized = X_train_weekday + X_test_weekday\n",
    "#X_train_weekday_normalized_unbalanced = preprocessing.scale(X_train_weekday_unbalanced)\n",
    "#X_test_weekday_normalized_unbalanced = preprocessing.scale(X_test_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3288, 193718)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_weekday), len(X_test_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weekday_normalized_df = pd.DataFrame(X_train_weekday + X_test_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_weekday_normalized = X_weekday_normalized[:len(X_train_weekday)]\n",
    "X_test_weekday_normalized = X_weekday_normalized[len(X_train_weekday):]\n",
    "#X_train_weekday_normalized_unbalanced = X_weekday_normalized_unbalanced[:len(X_train_weekday_unbalanced)]\n",
    "#X_test_weekday_normalized_unbalanced = X_weekday_normalized_unbalanced[len(X_train_weekday_unbalanced):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_weekday_normalized_df = pd.DataFrame(X_train_weekday_normalized)\n",
    "X_test_weekday_normalized_df = pd.DataFrame(X_test_weekday_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_weekend_normalized = preprocessing.scale(X_train_weekend + X_test_weekend)\n",
    "X_weekend_normalized = X_train_weekend + X_test_weekend\n",
    "#X_train_weekend_normalized_unbalanced = preprocessing.scale(X_train_weekend_unbalanced)\n",
    "#X_test_weekend_normalized_unbalanced = preprocessing.scale(X_test_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_weekend_normalized = X_weekend_normalized[:len(X_train_weekend)]\n",
    "X_test_weekend_normalized = X_weekend_normalized[len(X_train_weekend):]\n",
    "#X_train_weekend_normalized_unbalanced = X_weekend_normalized_unbalanced[:len(X_train_weekend_unbalanced)]\n",
    "#X_test_weekend_normalized_unbalanced = X_weekend_normalized_unbalanced[len(X_train_weekend_unbalanced):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_weekend_normalized_df = pd.DataFrame(X_train_weekend_normalized)\n",
    "X_test_weekend_normalized_df = pd.DataFrame(X_test_weekend_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [10 ** i for i in range(-4, 3)],\n",
    "    'gamma': [10 ** i for i in range(-4, 4, 2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid_search_weekday = GridSearchCV(SVC(kernel='rbf'), n_jobs=8, \n",
    "                               param_grid=param_grid, cv=5, \n",
    "                               scoring='accuracy', verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=8)]: Done 140 out of 140 | elapsed:  8.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=8,\n",
       "       param_grid={'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.0001, 0.01, 1, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=5)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_search_weekday.fit(X_train_weekday_normalized, y_train_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'gamma': 0.01}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_search_weekday.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6201338199513382"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_search_weekday.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2696"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svm_grid_search_weekday.best_estimator_.support_vectors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_search_weekday.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_weekday_pred = svm_grid_search_weekday.predict(X_train_weekday_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6362530413625304"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train_weekday_pred, y_train_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193718"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_weekday_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize prediction\n",
    "def predict_func(X, clf):\n",
    "    print(\"Process {} is predicting ...\".format(mp.current_process().pid))\n",
    "    return clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 30167 is predicting ...\n",
      "Process 30168 is predicting ...\n",
      "Process 30169 is predicting ...\n",
      "Process 30170 is predicting ...\n",
      "Process 30171 is predicting ...\n",
      "Process 30172 is predicting ...\n",
      "Process 30173 is predicting ...\n",
      "Process 30174 is predicting ...\n"
     ]
    }
   ],
   "source": [
    "pred_pool = mp.Pool(8)\n",
    "num_instances = int(np.ceil(len(X_test_weekday_normalized) / 8))\n",
    "y_test_weekday_pred_jobs = [pred_pool.apply_async(predict_func, args=(X_test_weekday_normalized[i*num_instances:(i+1)*num_instances], svm_grid_search_weekday)) for i in range(0, 8)]\n",
    "pred_pool.close()\n",
    "pred_pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_weekday_pred = []\n",
    "for y_test_weekday_pred_job in y_test_weekday_pred_jobs:\n",
    "    y_test_weekday_pred.extend(y_test_weekday_pred_job.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193718, 193718)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_weekday_normalized), len(y_test_weekday_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8336292961934358"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_weekday_pred, y_test_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dt_segments_weekday = int(len(y_test_weekday_pred) / (288-14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid_search_weekend = GridSearchCV(SVC(kernel='rbf'), n_jobs=8, \n",
    "                               param_grid=param_grid, cv=5, \n",
    "                               scoring='accuracy', verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=8)]: Done 140 out of 140 | elapsed:  1.2min finished\n",
      "/home/hylei/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=8,\n",
       "       param_grid={'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.0001, 0.01, 1, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_search_weekend.fit(X_train_weekend_normalized, y_train_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 0.1, 'gamma': 0.01}, 0.6201338199513382)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_search_weekday.best_params_, svm_grid_search_weekday.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svm_grid_search_weekend.best_estimator_.support_vectors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_search_weekend.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_weekend_pred = svm_grid_search_weekend.predict(X_train_weekend_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.733201581027668"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train_weekend_pred, y_train_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138370"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_weekend_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 30205 is predicting ...\n",
      "Process 30206 is predicting ...\n",
      "Process 30207 is predicting ...\n",
      "Process 30208 is predicting ...\n",
      "Process 30209 is predicting ...\n",
      "Process 30210 is predicting ...\n",
      "Process 30211 is predicting ...\n",
      "Process 30212 is predicting ...\n"
     ]
    }
   ],
   "source": [
    "pred_pool = mp.Pool(8)\n",
    "num_instances = int(np.ceil(len(X_test_weekend_normalized) / 8))\n",
    "y_test_weekend_pred_jobs = [pred_pool.apply_async(predict_func, args=(X_test_weekend_normalized[i*num_instances:(i+1)*num_instances], svm_grid_search_weekend)) for i in range(0, 8)]\n",
    "pred_pool.close()\n",
    "pred_pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_weekend_pred = []\n",
    "for y_test_weekend_pred_job in y_test_weekend_pred_jobs:\n",
    "    y_test_weekend_pred.extend(y_test_weekend_pred_job.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138370, 138370)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_weekend_normalized), len(y_test_weekend_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7040904820409049"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_weekend_pred, y_test_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dt_segments_weekend = int(len(y_test_weekend_pred) / (288-14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini_weekday = DecisionTreeClassifier(criterion='gini', random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gini_weekday.fit(X_train_weekday_normalized, y_train_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_weekday_pred_gini = clf_gini_weekday.predict(X_train_weekday_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train_weekday_pred_gini, y_train_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_weekday_pred_gini = clf_gini_weekday.predict(X_test_weekday_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6066550346379789"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_weekday_pred_gini, y_test_weekday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini_weekend = DecisionTreeClassifier(criterion='gini', random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gini_weekend.fit(X_train_weekend_normalized, y_train_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_weekend_pred_gini = clf_gini_weekend.predict(X_train_weekend_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train_weekend_pred_gini, y_train_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_weekend_pred_gini = clf_gini_weekend.predict(X_test_weekend_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6823733468237335"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_weekend_pred_gini, y_test_weekend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knn Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6202159840593027\n",
      "[[119658    469]\n",
      " [ 73102    489]]\n",
      "0.6423099557088139\n",
      "[[123959    490]\n",
      " [ 68801    468]]\n",
      "0.6610846694679896\n",
      "[[127607    501]\n",
      " [ 65153    457]]\n",
      "0.6743875117438751\n",
      "[[130198    515]\n",
      " [ 62562    443]]\n",
      "0.685253822566824\n",
      "[[132321    533]\n",
      " [ 60439    425]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10, 2):\n",
    "    knnClf = KNeighborsClassifier(n_neighbors=i)\n",
    "    knnClf.fit(X_train_weekday_normalized,y_train_weekday)\n",
    "    y_test_weekday_pred_knn = knnClf.predict(X_test_weekday_normalized)\n",
    "    print(accuracy_score(y_test_weekday_pred_knn, y_test_weekday))\n",
    "    print(confusion_matrix(y_test_weekday_pred_knn, y_test_weekday))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnClf = KNeighborsClassifier(n_neighbors=3, n_jobs=8)\n",
    "knnClf.fit(X_train_weekday_normalized,y_train_weekday)\n",
    "y_test_weekday_pred_knn = knnClf.predict(X_test_weekday_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(1,10):\\n    knnClf = KNeighborsClassifier(n_neighbors=i,n_jobs=8)\\n    knnClf.fit(X_train_weekday_normalized_unbalanced,y_train_weekday_unbalanced)\\n    y_test_weekday_pred_knn = knnClf.predict(X_test_weekday_normalized)\\n    print(accuracy_score(y_test_weekday_pred_knn, y_test_weekday))\\n    print(confusion_matrix(y_test_weekday_pred_knn, y_test_weekday))'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in range(1,10):\n",
    "    knnClf = KNeighborsClassifier(n_neighbors=i,n_jobs=8)\n",
    "    knnClf.fit(X_train_weekday_normalized_unbalanced,y_train_weekday_unbalanced)\n",
    "    y_test_weekday_pred_knn = knnClf.predict(X_test_weekday_normalized)\n",
    "    print(accuracy_score(y_test_weekday_pred_knn, y_test_weekday))\n",
    "    print(confusion_matrix(y_test_weekday_pred_knn, y_test_weekday))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6496061284960613\n",
      "[[89691   269]\n",
      " [48215   195]]\n",
      "0.6556912625569127\n",
      "[[90538   274]\n",
      " [47368   190]]\n",
      "0.6558430295584303\n",
      "[[90560   275]\n",
      " [47346   189]]\n",
      "0.6557346245573462\n",
      "[[90542   272]\n",
      " [47364   192]]\n",
      "0.6575196935751969\n",
      "[[90797   280]\n",
      " [47109   184]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10, 2):\n",
    "    knnClf = KNeighborsClassifier(n_neighbors=i,n_jobs=8)\n",
    "    knnClf.fit(X_train_weekend_normalized,y_train_weekend)\n",
    "    y_test_weekend_pred_knn = knnClf.predict(X_test_weekend_normalized)\n",
    "    print(accuracy_score(y_test_weekend_pred_knn, y_test_weekend))\n",
    "    print(confusion_matrix(y_test_weekend_pred_knn, y_test_weekend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnClf = KNeighborsClassifier(n_neighbors=3,n_jobs=8)\n",
    "knnClf.fit(X_train_weekend_normalized,y_train_weekend)\n",
    "y_test_weekend_pred_knn = knnClf.predict(X_test_weekend_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(1,10):\\n    knnClf = KNeighborsClassifier(n_neighbors=i,n_jobs=8)\\n    knnClf.fit(X_train_weekend_normalized_unbalanced,y_train_weekend_unbalanced)\\n    y_test_weekend_pred_knn = knnClf.predict(X_test_weekend_normalized)\\n    print(accuracy_score(y_test_weekend_pred_knn, y_test_weekend))\\n    print(confusion_matrix(y_test_weekend_pred_knn, y_test_weekend))'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in range(1,10):\n",
    "    knnClf = KNeighborsClassifier(n_neighbors=i,n_jobs=8)\n",
    "    knnClf.fit(X_train_weekend_normalized_unbalanced,y_train_weekend_unbalanced)\n",
    "    y_test_weekend_pred_knn = knnClf.predict(X_test_weekend_normalized)\n",
    "    print(accuracy_score(y_test_weekend_pred_knn, y_test_weekend))\n",
    "    print(confusion_matrix(y_test_weekend_pred_knn, y_test_weekend))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.004969910769869267"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((sum(y_test_weekday)+len(y_test_weekday))/2)/((sum(y_test_weekday)-len(y_test_weekday))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2:\n",
      "0.8816010902445823\n",
      "[[170493    669]\n",
      " [ 22267    289]]\n",
      "1, 4:\n",
      "0.8604517907473751\n",
      "[[166361    634]\n",
      " [ 26399    324]]\n",
      "6, 2:\n",
      "0.8016704694452761\n",
      "[[154916    576]\n",
      " [ 37844    382]]\n",
      "6, 4:\n",
      "0.7810270599531277\n",
      "[[150878    537]\n",
      " [ 41882    421]]\n",
      "11, 2:\n",
      "0.8193817817652463\n",
      "[[158364    593]\n",
      " [ 34396    365]]\n",
      "11, 4:\n",
      "0.7921514779215147\n",
      "[[153056    560]\n",
      " [ 39704    398]]\n",
      "16, 2:\n",
      "0.8224274460814173\n",
      "[[158960    599]\n",
      " [ 33800    359]]\n",
      "16, 4:\n",
      "0.802439628738682\n",
      "[[155056    567]\n",
      " [ 37704    391]]\n",
      "21, 2:\n",
      "0.8241825746703971\n",
      "[[159304    603]\n",
      " [ 33456    355]]\n",
      "21, 4:\n",
      "0.8090574959477178\n",
      "[[156348    577]\n",
      " [ 36412    381]]\n",
      "26, 2:\n",
      "0.8279767497083389\n",
      "[[160043    607]\n",
      " [ 32717    351]]\n",
      "26, 4:\n",
      "0.8128465088427508\n",
      "[[157089    584]\n",
      " [ 35671    374]]\n",
      "31, 2:\n",
      "0.8329117583291176\n",
      "[[161002    610]\n",
      " [ 31758    348]]\n",
      "31, 4:\n",
      "0.8166768188810539\n",
      "[[157833    586]\n",
      " [ 34927    372]]\n",
      "36, 2:\n",
      "0.8379345233793453\n",
      "[[161978    613]\n",
      " [ 30782    345]]\n",
      "36, 4:\n",
      "0.8198205639124914\n",
      "[[158437    581]\n",
      " [ 34323    377]]\n",
      "41, 2:\n",
      "0.8414396184143962\n",
      "[[162660    616]\n",
      " [ 30100    342]]\n",
      "41, 4:\n",
      "0.8238212246667836\n",
      "[[159227    596]\n",
      " [ 33533    362]]\n",
      "46, 2:\n",
      "0.8396277062534199\n",
      "[[162305    612]\n",
      " [ 30455    346]]\n",
      "46, 4:\n",
      "0.8242238718136673\n",
      "[[159302    593]\n",
      " [ 33458    365]]\n",
      "51, 2:\n",
      "0.8392044105349012\n",
      "[[162221    610]\n",
      " [ 30539    348]]\n",
      "51, 4:\n",
      "0.8238986568104152\n",
      "[[159245    599]\n",
      " [ 33515    359]]\n",
      "56, 2:\n",
      "0.8383165219545938\n",
      "[[162050    611]\n",
      " [ 30710    347]]\n",
      "56, 4:\n",
      "0.8254731103975882\n",
      "[[159552    601]\n",
      " [ 33208    357]]\n",
      "61, 2:\n",
      "0.8375783355186405\n",
      "[[161906    610]\n",
      " [ 30854    348]]\n",
      "61, 4:\n",
      "0.8220919067923477\n",
      "[[158894    598]\n",
      " [ 33866    360]]\n",
      "66, 2:\n",
      "0.8379861448084329\n",
      "[[161988    613]\n",
      " [ 30772    345]]\n",
      "66, 4:\n",
      "0.8231604703744618\n",
      "[[159099    596]\n",
      " [ 33661    362]]\n",
      "71, 2:\n",
      "0.8358645040729308\n",
      "[[161575    611]\n",
      " [ 31185    347]]\n",
      "71, 4:\n",
      "0.8202077246306487\n",
      "[[158520    589]\n",
      " [ 34240    369]]\n",
      "76, 2:\n",
      "0.8358903147874746\n",
      "[[161577    608]\n",
      " [ 31183    350]]\n",
      "76, 4:\n",
      "0.8216789353596465\n",
      "[[158806    590]\n",
      " [ 33954    368]]\n",
      "81, 2:\n",
      "0.8353792626395069\n",
      "[[161476    606]\n",
      " [ 31284    352]]\n",
      "81, 4:\n",
      "0.8212040182120401\n",
      "[[158716    592]\n",
      " [ 34044    366]]\n",
      "86, 2:\n",
      "0.8373770119451986\n",
      "[[161864    607]\n",
      " [ 30896    351]]\n",
      "86, 4:\n",
      "0.8231604703744618\n",
      "[[159099    596]\n",
      " [ 33661    362]]\n",
      "91, 2:\n",
      "0.8376609298051807\n",
      "[[161920    608]\n",
      " [ 30840    350]]\n",
      "91, 4:\n",
      "0.8234495503773527\n",
      "[[159155    596]\n",
      " [ 33605    362]]\n",
      "96, 2:\n",
      "0.8375318762324616\n",
      "[[161896    609]\n",
      " [ 30864    349]]\n",
      "96, 4:\n",
      "0.8248794639630803\n",
      "[[159437    601]\n",
      " [ 33323    357]]\n"
     ]
    }
   ],
   "source": [
    "for n in range(1, 100, 5):\n",
    "    for d in [2 ** i for i in range(1, 3)]:\n",
    "        print(\"{}, {}:\".format(n, d))\n",
    "        clf_rf_weekday = RandomForestClassifier(n_estimators=n, max_depth=d,random_state=0)\n",
    "        clf_rf_weekday.fit(X_train_weekday_normalized, y_train_weekday)\n",
    "        y_test_weekday_pred_rf = clf_rf_weekday.predict(X_test_weekday_normalized)\n",
    "        print(\"{}\".format(accuracy_score(y_test_weekday_pred_rf,y_test_weekday)))\n",
    "        print(\"{}\".format(confusion_matrix(y_test_weekday_pred_rf,y_test_weekday)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=96, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf_weekday = RandomForestClassifier(n_estimators=96)\n",
    "clf_rf_weekday.fit(X_train_weekday_normalized, y_train_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_weekday_pred_rf = clf_rf_weekday.predict(X_test_weekday_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7846715328467153"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_weekday, y_test_weekday_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10, 2:\n",
      "0.7817879598178796\n",
      "[[108079    367]\n",
      " [ 29827     97]]\n",
      "10, 4:\n",
      "0.7586615595866156\n",
      "[[104864    352]\n",
      " [ 33042    112]]\n",
      "15, 2:\n",
      "0.751232203512322\n",
      "[[103837    353]\n",
      " [ 34069    111]]\n",
      "15, 4:\n",
      "0.7705066127050662\n",
      "[[106501    350]\n",
      " [ 31405    114]]\n",
      "20, 2:\n",
      "0.7721543687215436\n",
      "[[106726    347]\n",
      " [ 31180    117]]\n",
      "20, 4:\n",
      "0.7767796487677965\n",
      "[[107368    349]\n",
      " [ 30538    115]]\n",
      "25, 2:\n",
      "0.7848883428488834\n",
      "[[108506    365]\n",
      " [ 29400     99]]\n",
      "25, 4:\n",
      "0.7968201199682012\n",
      "[[110159    367]\n",
      " [ 27747     97]]\n",
      "30, 2:\n",
      "0.8079352460793524\n",
      "[[111710    380]\n",
      " [ 26196     84]]\n",
      "30, 4:\n",
      "0.8021825540218256\n",
      "[[110904    370]\n",
      " [ 27002     94]]\n",
      "35, 2:\n",
      "0.826096697260967\n",
      "[[114231    388]\n",
      " [ 23675     76]]\n",
      "35, 4:\n",
      "0.8146635831466358\n",
      "[[112639    378]\n",
      " [ 25267     86]]\n",
      "40, 2:\n",
      "0.8236901062369011\n",
      "[[113899    389]\n",
      " [ 24007     75]]\n",
      "40, 4:\n",
      "0.8182698561826985\n",
      "[[113142    382]\n",
      " [ 24764     82]]\n",
      "45, 2:\n",
      "0.8305557563055576\n",
      "[[114855    395]\n",
      " [ 23051     69]]\n",
      "45, 4:\n",
      "0.8249909662499096\n",
      "[[114070    380]\n",
      " [ 23836     84]]\n",
      "50, 2:\n",
      "0.8288718652887187\n",
      "[[114614    387]\n",
      " [ 23292     77]]\n",
      "50, 4:\n",
      "0.8306786153067861\n",
      "[[114868    391]\n",
      " [ 23038     73]]\n",
      "55, 2:\n",
      "0.8326443593264435\n",
      "[[115138    389]\n",
      " [ 22768     75]]\n",
      "55, 4:\n",
      "0.8318782973187829\n",
      "[[115029    386]\n",
      " [ 22877     78]]\n",
      "60, 2:\n",
      "0.8332947893329479\n",
      "[[115226    387]\n",
      " [ 22680     77]]\n",
      "60, 4:\n",
      "0.8333742863337429\n",
      "[[115237    387]\n",
      " [ 22669     77]]\n",
      "65, 2:\n",
      "0.832145696321457\n",
      "[[115064    384]\n",
      " [ 22842     80]]\n",
      "65, 4:\n",
      "0.8301438173014382\n",
      "[[114783    380]\n",
      " [ 23123     84]]\n",
      "70, 2:\n",
      "0.828987497289875\n",
      "[[114628    385]\n",
      " [ 23278     79]]\n",
      "70, 4:\n",
      "0.8300932283009322\n",
      "[[114779    383]\n",
      " [ 23127     81]]\n",
      "75, 2:\n",
      "0.8236973332369734\n",
      "[[113890    379]\n",
      " [ 24016     85]]\n",
      "75, 4:\n",
      "0.8279901712799017\n",
      "[[114483    378]\n",
      " [ 23423     86]]\n",
      "80, 2:\n",
      "0.8165787381657874\n",
      "[[112905    379]\n",
      " [ 25001     85]]\n",
      "80, 4:\n",
      "0.828987497289875\n",
      "[[114622    379]\n",
      " [ 23284     85]]\n",
      "85, 2:\n",
      "0.8139770181397702\n",
      "[[112542    376]\n",
      " [ 25364     88]]\n",
      "85, 4:\n",
      "0.8309315603093156\n",
      "[[114895    383]\n",
      " [ 23011     81]]\n",
      "90, 2:\n",
      "0.8146491291464913\n",
      "[[112634    375]\n",
      " [ 25272     89]]\n",
      "90, 4:\n",
      "0.8288140492881405\n",
      "[[114597    378]\n",
      " [ 23309     86]]\n",
      "95, 2:\n",
      "0.8067644720676447\n",
      "[[111538    370]\n",
      " [ 26368     94]]\n",
      "95, 4:\n",
      "0.8272674712726747\n",
      "[[114383    378]\n",
      " [ 23523     86]]\n"
     ]
    }
   ],
   "source": [
    "for n in range(10, 100, 5):\n",
    "    for d in [2 ** i for i in range(1, 3)]:\n",
    "        print(\"{}, {}:\".format(n, d))\n",
    "        clf_rf_weekend = RandomForestClassifier(n_estimators=n, max_depth=d,random_state=0)\n",
    "        clf_rf_weekend.fit(X_train_weekend_normalized, y_train_weekend)\n",
    "        y_test_weekend_pred_rf = clf_rf_weekend.predict(X_test_weekend_normalized)\n",
    "        print(accuracy_score(y_test_weekend_pred_rf,y_test_weekend))\n",
    "        print(confusion_matrix(y_test_weekend_pred_rf,y_test_weekend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf_weekend = RandomForestClassifier(n_estimators=20)\n",
    "clf_rf_weekend.fit(X_test_weekend_normalized, y_test_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_weekend_pred_rf = clf_rf_weekend.predict(X_test_weekend_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_weekend, y_test_weekend_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = ['identity', 'logistic', 'tanh', 'relu']\n",
    "for a in activations:\n",
    "    clf_mlp_weekday = MLPClassifier(learning_rate_init=0.001, max_iter=300, activation=a)\n",
    "    clf_mlp_weekday.fit(X_train_weekday_normalized, y_train_weekday)\n",
    "    y_test_weekday_pred_mlp = clf_mlp_weekday.predict(X_test_weekday_normalized)\n",
    "    print(\"Activation method:\" + a)\n",
    "    print(accuracy_score(y_test_weekday_pred_mlp, y_test_weekday))\n",
    "    print(confusion_matrix(y_test_weekday_pred_mlp, y_test_weekday))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp_weekday = MLPClassifier(learning_rate_init=0.001, max_iter=500, activation='tanh')\n",
    "clf_mlp_weekday.fit(X_train_weekday_normalized, y_train_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_weekday_pred_mlp = clf_mlp_weekday.predict(X_test_weekday_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_weekday, y_test_weekday_pred_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = ['identity', 'logistic', 'tanh', 'relu']\n",
    "for a in activations:\n",
    "    clf_mlp_weekend = MLPClassifier(learning_rate_init=0.001, max_iter=300, activation=a)\n",
    "    clf_mlp_weekend.fit(X_train_weekend_normalized, y_train_weekend)\n",
    "    y_test_weekend_pred_mlp = clf_mlp_weekend.predict(X_test_weekend_normalized)\n",
    "    print(\"Activation method:\"+a)\n",
    "    print(accuracy_score(y_test_weekend_pred_mlp,y_test_weekend))\n",
    "    print(confusion_matrix(y_test_weekend_pred_mlp,y_test_weekend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp_weekend = MLPClassifier(learning_rate_init=0.001, max_iter=1000, activation='tanh')\n",
    "clf_mlp_weekend.fit(X_train_weekend_normalized, y_train_weekend)\n",
    "y_test_weekend_pred_mlp = clf_mlp_weekend.predict(X_test_weekend_normalized)\n",
    "print(\"Activation method:\"+'tanh')\n",
    "print(accuracy_score(y_test_weekend_pred_mlp,y_test_weekend))\n",
    "print(confusion_matrix(y_test_weekend_pred_mlp,y_test_weekend))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ns = range(1,100,5)\n",
    "for i in ns:\n",
    "    clf_ab_weekday = AdaBoostClassifier(n_estimators=i)\n",
    "    clf_ab_weekday.fit(X_train_weekday_normalized, y_train_weekday)\n",
    "    y_test_weekday_pred_ab = clf_ab_weekday.predict(X_test_weekday_normalized)\n",
    "    print(\"N estimators: \"+ str(i))\n",
    "    print(accuracy_score(y_test_weekday_pred_ab,y_test_weekday))\n",
    "    print(confusion_matrix(y_test_weekday_pred_ab,y_test_weekday))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ab_weekday = AdaBoostClassifier(n_estimators=90)\n",
    "clf_ab_weekday.fit(X_train_weekday_normalized, y_train_weekday)\n",
    "y_test_weekday_pred_ab = clf_ab_weekday.predict(X_test_weekday_normalized)\n",
    "print(\"N estimators: \"+ str(90))\n",
    "print(accuracy_score(y_test_weekday_pred_ab,y_test_weekday))\n",
    "print(confusion_matrix(y_test_weekday_pred_ab,y_test_weekday))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ns = range(1,100,5)\n",
    "for i in ns:\n",
    "    clf_ab_weekend = AdaBoostClassifier(n_estimators=i)\n",
    "    clf_ab_weekend.fit(X_train_weekend_normalized, y_train_weekend)\n",
    "    y_test_weekend_pred_ab = clf_ab_weekend.predict(X_test_weekend_normalized)\n",
    "    print(\"N estimators: \"+ str(i))\n",
    "    print(accuracy_score(y_test_weekend_pred_ab,y_test_weekend))\n",
    "    print(confusion_matrix(y_test_weekend_pred_ab,y_test_weekend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ab_weekend = AdaBoostClassifier(n_estimators=90)\n",
    "clf_ab_weekend.fit(X_train_weekend_normalized, y_train_weekend)\n",
    "y_test_weekend_pred_ab = clf_ab_weekend.predict(X_test_weekend_normalized)\n",
    "print(\"N estimators: \"+ str(90))\n",
    "print(accuracy_score(y_test_weekend_pred_ab,y_test_weekend))\n",
    "print(confusion_matrix(y_test_weekend_pred_ab,y_test_weekend))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "ns = range(1,100,5)\n",
    "for i in ns:\n",
    "    clf_gb_weekday = GradientBoostingClassifier(n_estimators=i)\n",
    "    clf_gb_weekday.fit(X_train_weekday_normalized, y_train_weekday)\n",
    "    y_test_weekday_pred_gb = clf_gb_weekday.predict(X_test_weekday_normalized)\n",
    "    print(\"N estimators: \"+ str(i))\n",
    "    print(accuracy_score(y_test_weekday_pred_gb,y_test_weekday))\n",
    "    print(confusion_matrix(y_test_weekday_pred_gb,y_test_weekday))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb_weekday = GradientBoostingClassifier(n_estimators=90)\n",
    "clf_gb_weekday.fit(X_train_weekday_normalized, y_train_weekday)\n",
    "y_test_weekday_pred_gb = clf_gb_weekday.predict(X_test_weekday_normalized)\n",
    "print(\"N estimators: \"+ str(90))\n",
    "print(accuracy_score(y_test_weekday_pred_gb,y_test_weekday))\n",
    "print(confusion_matrix(y_test_weekday_pred_gb,y_test_weekday))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ns = range(1,100,5)\n",
    "for i in ns:\n",
    "    clf_gb_weekend = GradientBoostingClassifier(n_estimators=i)\n",
    "    clf_gb_weekend.fit(X_train_weekend_normalized, y_train_weekend)\n",
    "    y_test_weekend_pred_gb = clf_gb_weekend.predict(X_test_weekend_normalized)\n",
    "    print(\"N estimators: \"+ str(i))\n",
    "    print(accuracy_score(y_test_weekend_pred_gb,y_test_weekend))\n",
    "    print(confusion_matrix(y_test_weekend_pred_gb,y_test_weekend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb_weekend = GradientBoostingClassifier(n_estimators=50)\n",
    "clf_gb_weekend.fit(X_train_weekend_normalized, y_train_weekend)\n",
    "y_test_weekend_pred_gb = clf_gb_weekend.predict(X_test_weekend_normalized)\n",
    "print(\"N estimators: \"+ str(50))\n",
    "print(accuracy_score(y_test_weekend_pred_gb,y_test_weekend))\n",
    "print(confusion_matrix(y_test_weekend_pred_gb,y_test_weekend))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rush hours: 6:30 AM - 9:00 AM (idx 78 - idx 108), 3:30 PM - 6:30 PM (idx 186 - idx 222)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rush_hours_idx = list(range(78, 109)) + list(range(186, 223))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_thresholds = [2 ** i for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection rate (DR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DR(y_test, y_test_pred, PT_thresholds, num_dt_segments):\n",
    "    DRs = []\n",
    "    for PT_threshold in PT_thresholds:\n",
    "        num_detected_incidents = 0\n",
    "        total_num_incidents = 0\n",
    "        for i in range(num_dt_segments):\n",
    "            base_idx = i * 274\n",
    "            max_base_offset = (i + 1) * 274\n",
    "            start_idx = base_idx\n",
    "            end_idx = start_idx\n",
    "            while start_idx < max_base_offset:\n",
    "                while start_idx < max_base_offset and y_test[start_idx] == -1:\n",
    "                    start_idx += 1\n",
    "                if start_idx == max_base_offset:\n",
    "                    break\n",
    "                # an incident happens\n",
    "                # time span of the incident\n",
    "                end_idx = start_idx\n",
    "                while end_idx < max_base_offset and y_test[end_idx] == 1:\n",
    "                    end_idx += 1\n",
    "                if (end_idx - start_idx >= PT_threshold):\n",
    "                    total_num_incidents += 1\n",
    "                    # the incident is detected\n",
    "                    if 1 in y_test_pred[start_idx:end_idx]:\n",
    "                        num_detected_incidents += 1\n",
    "\n",
    "                start_idx = end_idx + 1\n",
    "        DRs.append(round(num_detected_incidents / total_num_incidents * 100, 2))\n",
    "        print(\"PT_{}\".format(PT_threshold))\n",
    "        print(\"# of detected incidents: {}\".format(num_detected_incidents))\n",
    "        print(\"Total # of incidents: {}\".format(total_num_incidents))\n",
    "        print(\"Detection rate: {}\".format(DRs[-1]))\n",
    "    return DRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test_weekday + y_test_weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue predictions together\n",
    "y_test_pred_svm = y_test_weekday_pred + y_test_weekend_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test_pred_svm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRs = DR(y_test, y_test_pred_svm, PT_thresholds[0:3], num_dt_segments_weekday + num_dt_segments_weekend)\n",
    "results_str += \"SVM DR: {}\\n\".format(DRs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue predictions together\n",
    "y_test_pred_gini = list(y_test_weekday_pred_gini) + list(y_test_weekend_pred_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRs_gini = DR(y_test, y_test_pred_gini, PT_thresholds[0:3], num_dt_segments_weekday + num_dt_segments_weekend)\n",
    "results_str += \"DT DR: {}\\n\".format(DRs_gini[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knn Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_pred_knn = list(y_test_weekday_pred_knn) + list(y_test_weekend_pred_knn)\n",
    "DRs_knn = DR(y_test, y_test_pred_knn, PT_thresholds[0:3], num_dt_segments_weekday + num_dt_segments_weekend)\n",
    "results_str += \"KNN DR: {}\\n\".format(DRs_knn[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_rf = list(y_test_weekday_pred_rf) + list(y_test_weekend_pred_rf)\n",
    "DRs_rf = DR(y_test, y_test_pred_rf, PT_thresholds[0:3], num_dt_segments_weekday + num_dt_segments_weekend)\n",
    "results_str += \"RF DR: {}\\n\".format(DRs_rf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_pred_mlp = list(y_test_weekday_pred_mlp) + list(y_test_weekend_pred_mlp)\n",
    "DRs_mlp = DR(y_test, y_test_pred_mlp, PT_thresholds[0:3], num_dt_segments_weekday + num_dt_segments_weekend)\n",
    "results_str += \"MLP DR: {}\\n\".format(DRs_mlp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_ab = list(y_test_weekday_pred_ab) + list(y_test_weekend_pred_ab)\n",
    "DRs_ab = DR(y_test, y_test_pred_ab, PT_thresholds[0:3], num_dt_segments_weekday + num_dt_segments_weekend)\n",
    "results_str += \"AB DR: {}\\n\".format(DRs_ab[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_gb = list(y_test_weekday_pred_gb) + list(y_test_weekend_pred_gb)\n",
    "DRs_gb = DR(y_test, y_test_pred_gb, PT_thresholds[0:3], num_dt_segments_weekday + num_dt_segments_weekend)\n",
    "results_str += \"GB DR: {}\\n\".format(DRs_gb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean time to detect (MTTD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MTTD(y_test, y_test_pred, PT_thresholds, num_dt_segments):\n",
    "    MTTDs = []\n",
    "    for PT_threshold in PT_thresholds:\n",
    "        h = 0\n",
    "        sum_ttd = 0\n",
    "        for i in range(num_dt_segments):\n",
    "            base_idx = i * 274\n",
    "            max_base_offset = (i + 1) * 274\n",
    "            start_idx = base_idx\n",
    "            end_idx = start_idx\n",
    "            while start_idx < max_base_offset:\n",
    "                while start_idx < max_base_offset and y_test[start_idx] == -1:\n",
    "                    start_idx += 1\n",
    "                if start_idx == max_base_offset:\n",
    "                    break\n",
    "                # an incident happens\n",
    "                # time span of the incident\n",
    "                end_idx = start_idx\n",
    "                while end_idx < max_base_offset and y_test[end_idx] == 1:\n",
    "                    end_idx += 1\n",
    "\n",
    "                if end_idx - start_idx >= PT_threshold:\n",
    "                    # the incident is detected\n",
    "                    if 1 in y_test_pred[start_idx:end_idx]:\n",
    "                        h += 1\n",
    "                        incident_idx = start_idx\n",
    "                        detection_idx = incident_idx\n",
    "                        while y_test_pred[detection_idx] == -1:\n",
    "                            detection_idx += 1\n",
    "                        sum_ttd += (detection_idx - incident_idx) * 5\n",
    "\n",
    "                start_idx = end_idx + 1\n",
    "        MTTDs.append(round(sum_ttd / h, 2))\n",
    "\n",
    "        print(\"PT_{}\".format(PT_threshold))\n",
    "        print(\"Total number of detected incidents: {}\".format(h))\n",
    "        print(\"Total time of detection lags (min): {}\".format(sum_ttd))\n",
    "        print(\"Mean time to detect (MTTD): {}\".format(MTTDs[-1]))\n",
    "    return MTTDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MTTDs = MTTD(y_test, y_test_pred_svm, PT_thresholds[0:3], num_dt_segments_weekday + num_dt_segments_weekend)\n",
    "results_str += \"SVM MTTD: {}\\n\".format(MTTDs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MTTDs_gini = MTTD(y_test, y_test_pred_gini, PT_thresholds[0:3], num_dt_segments_weekday + num_dt_segments_weekend)\n",
    "results_str += \"DT MTTD: {}\\n\".format(MTTDs_gini[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knn Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MTTDs_knn = MTTD(y_test, y_test_pred_knn, PT_thresholds[0:3], num_dt_segments_weekday + num_dt_segments_weekend)\n",
    "results_str += \"KNN MTTD: {}\\n\".format(MTTDs_knn[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MTTDs_rf = MTTD(y_test, y_test_pred_rf, PT_thresholds[0:3], num_dt_segments_weekday + num_dt_segments_weekend)\n",
    "results_str += \"RF MTTD: {}\\n\".format(MTTDs_rf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MTTDs_mlp = MTTD(y_test, y_test_pred_mlp, PT_thresholds[0:3], num_dt_segments_weekday + num_dt_segments_weekend)\n",
    "results_str += \"MLP MTTD: {}\\n\".format(MTTDs_mlp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MTTDs_ab = MTTD(y_test, y_test_pred_ab, PT_thresholds[0:3], num_dt_segments_weekday + num_dt_segments_weekend)\n",
    "results_str += \"AB MTTD: {}\\n\".format(MTTDs_ab[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MTTDs_gb = MTTD(y_test, y_test_pred_gb, PT_thresholds[0:3], num_dt_segments_weekday + num_dt_segments_weekend)\n",
    "results_str += \"GB MTTD: {}\\n\".format(MTTDs_gb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False positive rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{FP}{N} = \\frac{FP}{FP + TN}, $$ where $FP$ = False positive, and $TN$ = True negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FP(y_test, y_test_pred):\n",
    "    num_false_positives = 0\n",
    "    num_true_negatives = 0\n",
    "    total_num_detections = len(y_test_pred)\n",
    "    for i in range(total_num_detections):\n",
    "        if y_test_pred[i] == 1 and y_test[i] == -1:\n",
    "            num_false_positives += 1\n",
    "        elif y_test_pred[i] == -1 and y_test[i] == -1:\n",
    "            num_true_negatives += 1\n",
    "    print(\"The false positive rate is {}.\".format(round(num_false_positives / (num_false_positives + num_true_negatives), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FP(y_test, y_test_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP(y_test, y_test_pred_gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False alarm rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_alarm_rate(y_test, y_test_pred):\n",
    "    num_false_alarms = 0\n",
    "    total_num_detections = len(y_test_pred)\n",
    "    for i in range(total_num_detections):\n",
    "        if y_test_pred[i] != y_test[i]:\n",
    "            num_false_alarms += 1\n",
    "    FAR = round(num_false_alarms / total_num_detections * 100, 4)\n",
    "    print(\"The false alarm rate is {}.\".format(FAR))\n",
    "    return FAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FAR = false_alarm_rate(y_test, y_test_pred_svm)\n",
    "results_str += \"SVM FAR: {}\\n\".format(FAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAR_gini = false_alarm_rate(y_test, y_test_pred_gini)\n",
    "results_str += \"DT FAR: {}\\n\".format(FAR_gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FAR_knn = false_alarm_rate(y_test, y_test_pred_knn)\n",
    "results_str += \"KNN FAR: {}\\n\".format(FAR_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAR_rf = false_alarm_rate(y_test, y_test_pred_rf)\n",
    "results_str += \"RF FAR: {}\\n\".format(FAR_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAR_mlp = false_alarm_rate(y_test, y_test_pred_mlp)\n",
    "results_str += \"MLP FAR: {}\\n\".format(FAR_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAR_ab = false_alarm_rate(y_test, y_test_pred_ab)\n",
    "results_str += \"AB FAR: {}\\n\".format(FAR_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAR_gb = false_alarm_rate(y_test, y_test_pred_gb)\n",
    "results_str += \"GB FAR: {}\\n\".format(FAR_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(output_path, eval_result):\n",
    "    flag = \"w\"\n",
    "    if os.path.isfile(output_path):\n",
    "        flag = \"a\"\n",
    "    \n",
    "    with open(output_path, flag) as f:\n",
    "        f.write(\"{} \\n\".format(eval_result))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result('./res.txt', results_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
