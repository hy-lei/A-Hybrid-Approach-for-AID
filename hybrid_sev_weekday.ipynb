{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/I88N-processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample dates, split train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_day(date):\n",
    "    # date: a date string in the format of \"yyyy-mm-dd\"\n",
    "    # return: an int w/t Monday being 0 and Sunday being 6.\n",
    "    if date.find('-') != -1:\n",
    "        y, m, d = date.split('-')\n",
    "    else:\n",
    "        m, d, y = date.split('/')\n",
    "    return dt.datetime(int(y), int(m), int(d)).weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.read_csv(base_path + 'available_dates.csv')\n",
    "dates = np.array(dates['0'].values.tolist())\n",
    "dates = np.array(list(map(lambda x: x.split('-')[1] + '/' + x.split('-')[2] + '/' + x.split('-')[0], dates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to sample dates in June."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = dates[(dates > '05/31/2017') & (dates < '07/01/2017')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_weekend = [date for date in dates if date_to_day(date) >= 5]\n",
    "dates_weekday = list(set(dates).difference(set(dates_weekend)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_train = dates_weekday[:14] + dates_weekend[:3]\n",
    "dates_test = dates_weekday[14:] + dates_weekend[3:]\n",
    "dates_train.sort()\n",
    "dates_test.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,\n",
       " 17,\n",
       " 12,\n",
       " ['06/02/2017', '06/03/2017', '06/04/2017'],\n",
       " ['06/01/2017', '06/07/2017', '06/08/2017'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dates), len(dates_train), len(dates_test), dates_train[0:3], dates_test[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading severity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_data = pd.read_csv(base_path + 'severity_params.csv')\n",
    "severity_data.rename(columns={'Unnamed: 0':'datetime'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sev_datetimes = severity_data['datetime'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sev_dates = []\n",
    "sev_times = []\n",
    "for x in sev_datetimes:\n",
    "    d, t = x.split(' ')\n",
    "    sev_dates.append(d)\n",
    "    sev_times.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_data['Date'] = sev_dates\n",
    "severity_data['Time'] = sev_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "severity_data = severity_data.loc[~severity_data['Date'].isin(['2017-06-15'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_max = severity_data['LambdaMax'].values\n",
    "sigma = severity_data['Sigma'].values\n",
    "tau = severity_data['Tau'].values\n",
    "impact = severity_data['Impact'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>ID</th>\n",
       "      <th>LambdaMax</th>\n",
       "      <th>Sigma</th>\n",
       "      <th>Tau</th>\n",
       "      <th>Impact</th>\n",
       "      <th>Incident</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-01 00:00:00</td>\n",
       "      <td>408907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-06-01 00:05:00</td>\n",
       "      <td>408907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>00:05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-06-01 00:10:00</td>\n",
       "      <td>408907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>00:10:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime      ID  LambdaMax  Sigma  Tau    Impact  Incident  \\\n",
       "0  2017-06-01 00:00:00  408907        NaN    NaN  NaN  0.021267       0.0   \n",
       "1  2017-06-01 00:05:00  408907        NaN    NaN  NaN  0.017058       0.0   \n",
       "2  2017-06-01 00:10:00  408907        NaN    NaN  0.0  0.015338       0.0   \n",
       "\n",
       "         Date      Time  \n",
       "0  2017-06-01  00:00:00  \n",
       "1  2017-06-01  00:05:00  \n",
       "2  2017-06-01  00:10:00  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "severity_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_max = [0 if np.isnan(x) else x for x in lambda_max]\n",
    "sigma = [0 if np.isnan(x) else x for x in sigma]\n",
    "tau = [0 if np.isnan(x) else x for x in tau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "severity_data['LambdaMax'] = lambda_max\n",
    "severity_data['Sigma'] = sigma\n",
    "severity_data['Tau'] = tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading speed, flow, occupancy, and stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(base_path + 'concat_no_holes/concat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select raw that is sampled\n",
    "raw_all = raw.loc[raw['Date'].isin(dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hylei/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/hylei/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/hylei/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/hylei/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "raw_all['LambdaMax'] = lambda_max\n",
    "raw_all['Sigma'] = sigma\n",
    "raw_all['Tau'] = tau\n",
    "raw_all['Impact'] = impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test = raw_all.loc[raw['Date'].isin(dates_test)]\n",
    "stations = np.array(raw_all['Station ID'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station ID</th>\n",
       "      <th>datetime</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>Flow</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>idx</th>\n",
       "      <th>LambdaMax</th>\n",
       "      <th>Sigma</th>\n",
       "      <th>Tau</th>\n",
       "      <th>Impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8640</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-01 00:00:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>06/01/2017</td>\n",
       "      <td>00:00</td>\n",
       "      <td>42322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8641</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-01 00:05:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>69.1</td>\n",
       "      <td>06/01/2017</td>\n",
       "      <td>00:05</td>\n",
       "      <td>42323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8642</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-01 00:10:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>68.9</td>\n",
       "      <td>06/01/2017</td>\n",
       "      <td>00:10</td>\n",
       "      <td>42324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Station ID             datetime  Occupancy  Flow  Speed        Date  \\\n",
       "8640      408907  2017-06-01 00:00:00        0.5  22.0   69.5  06/01/2017   \n",
       "8641      408907  2017-06-01 00:05:00        0.5  22.0   69.1  06/01/2017   \n",
       "8642      408907  2017-06-01 00:10:00        0.5  23.0   68.9  06/01/2017   \n",
       "\n",
       "       Time    idx  LambdaMax  Sigma  Tau    Impact  \n",
       "8640  00:00  42322        0.0    0.0  0.0  0.021267  \n",
       "8641  00:05  42323        0.0    0.0  0.0  0.017058  \n",
       "8642  00:10  42324        0.0    0.0  0.0  0.015338  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special construction of raw_train, because the dates are sampled with replacement\n",
    "raw_train = pd.DataFrame()\n",
    "duplicate_id = 0\n",
    "for i in range(0, len(dates_train)):\n",
    "    if i > 0:\n",
    "        if dates_train[i] == dates_train[i-1]:\n",
    "            duplicate_id += 1\n",
    "        else:\n",
    "            duplicate_id = 0\n",
    "    df_date = raw_all.loc[raw_all['Date'] == dates_train[i]]\n",
    "    df_date = df_date.assign(duplicateIdx=duplicate_id)\n",
    "    raw_train = raw_train.append(df_date)\n",
    "sorterIdx = dict( zip(stations, range(len(stations))) )\n",
    "raw_train['stationSorterIdx'] = raw_train['Station ID'].map(sorterIdx)\n",
    "raw_train = raw_train.sort_values(['stationSorterIdx', 'duplicateIdx', 'datetime'], ascending=[True, True, True])\n",
    "raw_train.drop(['duplicateIdx', 'stationSorterIdx', 'idx'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499392"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['Speed', 'Flow', 'Occupancy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct road segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_segments = list()\n",
    "for i in range(len(stations) - 1):\n",
    "    road_segments.append(tuple([stations[i], stations[i+1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_incidents = pd.read_csv(base_path + 'valid_incidents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_incidents_all = raw_incidents.loc[raw_incidents['Date'].isin(dates)]\n",
    "raw_incidents_train = raw_incidents_all.loc[raw_incidents_all['Date'].isin(dates_train)]\n",
    "raw_incidents_test = raw_incidents_all.loc[raw_incidents_all['Date'].isin(dates_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pos_timestamps = pd.read_csv(base_path + 'svm_pos_instances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pos_timestamps_train = svm_pos_timestamps.loc[svm_pos_timestamps['Date'].isin(dates_train)]\n",
    "svm_pos_timestamps_test = svm_pos_timestamps.loc[svm_pos_timestamps['Date'].isin(dates_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Upstream</th>\n",
       "      <th>Downstream</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>408907</td>\n",
       "      <td>400951</td>\n",
       "      <td>01/22/2017</td>\n",
       "      <td>20:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>408907</td>\n",
       "      <td>400951</td>\n",
       "      <td>01/22/2017</td>\n",
       "      <td>20:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>408907</td>\n",
       "      <td>400951</td>\n",
       "      <td>01/22/2017</td>\n",
       "      <td>20:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Upstream  Downstream        Date   Time\n",
       "0    408907      400951  01/22/2017  20:30\n",
       "1    408907      400951  01/22/2017  20:35\n",
       "2    408907      400951  01/22/2017  20:40"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_pos_timestamps.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_incident_dates_train = svm_pos_timestamps_train['Date'].unique().tolist()\n",
    "svm_normal_dates_train = list(set(dates_train).difference(svm_incident_dates_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svm_incident_dates_train), len(svm_normal_dates_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress message formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraction_msg(present, total):\n",
    "    return '[{}/{}]'.format(present, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train: TSA-DES forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DES_rmse(alpha, var_series):\n",
    "    len_series = len(var_series)\n",
    "    \n",
    "    beta = round(1. - alpha, 3)\n",
    "\n",
    "    sse = 0.\n",
    "    s1 = np.mean(var_series[:10])\n",
    "    s2 = s1\n",
    "    \n",
    "    for i in range(11, len_series - 1):\n",
    "        s1 = alpha * var_series[i] + beta * s1\n",
    "        s2 = alpha * s1 + beta * s2\n",
    "        y_next = 2 * s1 - s2 + alpha / beta * (s1 - s2)\n",
    "        sse += (var_series[i+1] - y_next) ** 2\n",
    "    \n",
    "    return np.sqrt( sse / (len_series - 12) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune best alphas for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: stations, raw training data (includ. incidents), rule to update alphas\n",
    "# output: a dictionary containing stations, and stations' best alphas\n",
    "def compute_best_alphas(stations, raw_train, raw_incidents_train, dates_train, num_grids, DES_rmse, fraction_msg):\n",
    "    best_alphas = {\n",
    "    'Station ID': [],\n",
    "    'Speed': [],\n",
    "    'Flow': [],\n",
    "    'Occupancy': []\n",
    "    }\n",
    "    pid = mp.current_process().pid\n",
    "    for i, station in enumerate(stations):\n",
    "        best_alphas['Station ID'].append(station)\n",
    "\n",
    "        # update current training station dataframe, the training data is normal day's data\n",
    "        abnormal_dates_station = raw_incidents_train.loc[(raw_incidents_train['Upstream'] == station) | (raw_incidents_train['Downstream'] == str(station))]['Date'].unique()\n",
    "        normal_dates_train = np.array(list(set(dates_train).difference(set(abnormal_dates_station))))\n",
    "        df_train_station = raw_train.loc[(raw_train['Station ID'] == station) & (raw_train['Date'].isin(normal_dates_train))]\n",
    "\n",
    "        print(\"{} {} Tuning alphas for station {}...\".format(pid, fraction_msg(i+1, len(stations)), station))\n",
    "        for var_name in var_names:\n",
    "            # print(\"    \" + var_name + \"...\")\n",
    "            var_series = df_train_station[var_name].values\n",
    "            len_series = len(var_series)\n",
    "\n",
    "            # setting up alphas\n",
    "            alphas = np.arange(num_grids) * 1. / num_grids\n",
    "\n",
    "            # save the historical best alpha by rmse\n",
    "            best_rmse = float(\"inf\")\n",
    "            best_alpha = 0.\n",
    "\n",
    "            # for each alpha, perform exponential smoothing, and compute RMSE\n",
    "            for alpha in alphas:\n",
    "                rmse = DES_rmse(alpha, var_series)\n",
    "\n",
    "                # compare, and decide whether to update best alpha\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_alpha = alpha\n",
    "\n",
    "            # finally, save the best alpha for the variable at this station\n",
    "            best_alphas[var_name].append(best_alpha)\n",
    "\n",
    "        # print trained alphas for each station\n",
    "        # print(best_alphas['Station ID'][i], best_alphas['Speed'][i], best_alphas['Flow'][i], best_alphas['Occupancy'][i])\n",
    "    print(\"Process {} has finished alpha tuning.\".format(pid))\n",
    "    return best_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29907 [1/13] Tuning alphas for station 408907...\n",
      "29908 [1/13] Tuning alphas for station 400088...\n",
      "29909 [1/13] Tuning alphas for station 408755...\n",
      "29910 [1/13] Tuning alphas for station 400137...\n",
      "29911 [1/13] Tuning alphas for station 400611...\n",
      "29912 [1/13] Tuning alphas for station 400275...\n",
      "29913 [1/13] Tuning alphas for station 400333...\n",
      "29914 [1/11] Tuning alphas for station 400980...\n",
      "29908 [2/13] Tuning alphas for station 402288...\n",
      "29907 [2/13] Tuning alphas for station 400951...\n",
      "29909 [2/13] Tuning alphas for station 402802...\n",
      "29910 [2/13] Tuning alphas for station 400716...\n",
      "29912 [2/13] Tuning alphas for station 400939...\n",
      "29911 [2/13] Tuning alphas for station 400928...\n",
      "29913 [2/13] Tuning alphas for station 410363...\n",
      "29914 [2/11] Tuning alphas for station 401333...\n",
      "29908 [3/13] Tuning alphas for station 413026...\n",
      "29907 [3/13] Tuning alphas for station 400057...\n",
      "29909 [3/13] Tuning alphas for station 408756...\n",
      "29910 [3/13] Tuning alphas for station 401545...\n",
      "29912 [3/13] Tuning alphas for station 400180...\n",
      "29914 [3/11] Tuning alphas for station 404746...\n",
      "29911 [3/13] Tuning alphas for station 400284...\n",
      "29913 [3/13] Tuning alphas for station 400360...\n",
      "29907 [4/13] Tuning alphas for station 400147...\n",
      "29908 [4/13] Tuning alphas for station 401464...\n",
      "29909 [4/13] Tuning alphas for station 400189...\n",
      "29912 [4/13] Tuning alphas for station 400529...\n",
      "29910 [4/13] Tuning alphas for station 401011...\n",
      "29913 [4/13] Tuning alphas for station 400955...\n",
      "29911 [4/13] Tuning alphas for station 400041...\n",
      "29914 [4/11] Tuning alphas for station 401142...\n",
      "29907 [5/13] Tuning alphas for station 400343...\n",
      "29909 [5/13] Tuning alphas for station 400309...\n",
      "29910 [5/13] Tuning alphas for station 400674...\n",
      "29908 [5/13] Tuning alphas for station 401489...\n",
      "29913 [5/13] Tuning alphas for station 400495...\n",
      "29912 [5/13] Tuning alphas for station 400990...\n",
      "29907 [6/13] Tuning alphas for station 401560...\n",
      "29911 [5/13] Tuning alphas for station 408133...\n",
      "29914 [5/11] Tuning alphas for station 400218...\n",
      "29909 [6/13] Tuning alphas for station 400417...\n",
      "29910 [6/13] Tuning alphas for station 400539...\n",
      "29913 [6/13] Tuning alphas for station 400608...\n",
      "29908 [6/13] Tuning alphas for station 401538...\n",
      "29914 [6/11] Tuning alphas for station 400983...\n",
      "29912 [6/13] Tuning alphas for station 400515...\n",
      "29907 [7/13] Tuning alphas for station 400045...\n",
      "29911 [6/13] Tuning alphas for station 408135...\n",
      "29909 [7/13] Tuning alphas for station 400249...\n",
      "29913 [7/13] Tuning alphas for station 400949...\n",
      "29910 [7/13] Tuning alphas for station 400534...\n",
      "29908 [7/13] Tuning alphas for station 402290...\n",
      "29907 [8/13] Tuning alphas for station 400122...\n",
      "29912 [7/13] Tuning alphas for station 400252...\n",
      "29914 [7/11] Tuning alphas for station 400765...\n",
      "29911 [7/13] Tuning alphas for station 417665...\n",
      "29909 [8/13] Tuning alphas for station 401639...\n",
      "29913 [8/13] Tuning alphas for station 400678...\n",
      "29910 [8/13] Tuning alphas for station 401062...\n",
      "29908 [8/13] Tuning alphas for station 402292...\n",
      "29912 [8/13] Tuning alphas for station 400788...\n",
      "29913 [9/13] Tuning alphas for station 400341...\n",
      "29907 [9/13] Tuning alphas for station 401541...\n",
      "29911 [8/13] Tuning alphas for station 412637...\n",
      "29909 [9/13] Tuning alphas for station 400662...\n",
      "29914 [8/11] Tuning alphas for station 400844...\n",
      "29910 [9/13] Tuning alphas for station 401529...\n",
      "29912 [9/13] Tuning alphas for station 401517...\n",
      "29908 [9/13] Tuning alphas for station 401643...\n",
      "29907 [10/13] Tuning alphas for station 402281...\n",
      "29913 [10/13] Tuning alphas for station 400607...\n",
      "29909 [10/13] Tuning alphas for station 400141...\n",
      "29914 [9/11] Tuning alphas for station 400923...\n",
      "29911 [9/13] Tuning alphas for station 417666...\n",
      "29910 [10/13] Tuning alphas for station 401613...\n",
      "29908 [10/13] Tuning alphas for station 402800...\n",
      "29912 [10/13] Tuning alphas for station 401871...\n",
      "29907 [11/13] Tuning alphas for station 402283...\n",
      "29913 [11/13] Tuning alphas for station 400094...\n",
      "29909 [11/13] Tuning alphas for station 400761...\n",
      "29914 [10/11] Tuning alphas for station 401143...\n",
      "29911 [10/13] Tuning alphas for station 408134...\n",
      "29910 [11/13] Tuning alphas for station 400536...\n",
      "29908 [11/13] Tuning alphas for station 402828...\n",
      "29913 [12/13] Tuning alphas for station 400682...\n",
      "29912 [11/13] Tuning alphas for station 400574...\n",
      "29907 [12/13] Tuning alphas for station 402285...\n",
      "29909 [12/13] Tuning alphas for station 400490...\n",
      "29914 [11/11] Tuning alphas for station 401471...\n",
      "29911 [11/13] Tuning alphas for station 400685...\n",
      "29910 [12/13] Tuning alphas for station 400488...\n",
      "29912 [12/13] Tuning alphas for station 401629...\n",
      "29913 [13/13] Tuning alphas for station 408138...\n",
      "29908 [12/13] Tuning alphas for station 407219...\n",
      "29907 [13/13] Tuning alphas for station 402286...\n",
      "29909 [13/13] Tuning alphas for station 401888...\n",
      "29910 [13/13] Tuning alphas for station 401561...\n",
      "29912 [13/13] Tuning alphas for station 400422...\n",
      "29911 [12/13] Tuning alphas for station 401003...\n",
      "Process 29914 has finished alpha tuning.\n",
      "Process 29913 has finished alpha tuning.\n",
      "29908 [13/13] Tuning alphas for station 402789...\n",
      "Process 29907 has finished alpha tuning.\n",
      "Process 29909 has finished alpha tuning.\n",
      "29911 [13/13] Tuning alphas for station 400898...\n",
      "Process 29910 has finished alpha tuning.\n",
      "Process 29912 has finished alpha tuning.\n",
      "Process 29908 has finished alpha tuning.\n",
      "Process 29911 has finished alpha tuning.\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(processes=8)\n",
    "num_grids = 100\n",
    "results = [pool.apply_async(compute_best_alphas, args=(stations[13 * i: 13 * i + 13], raw_train, raw_incidents_train, dates_train, num_grids, DES_rmse, fraction_msg)) for i in range(0, 8)]\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_results = []\n",
    "for proc in results:\n",
    "    exec_results.append(proc.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_alphas = {\n",
    "    'Station ID': [],\n",
    "    'Speed': [],\n",
    "    'Flow': [],\n",
    "    'Occupancy': []\n",
    "}\n",
    "for dict_best_alphas in exec_results:\n",
    "    for key in best_alphas.keys():\n",
    "        best_alphas[key].extend(dict_best_alphas[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Station ID': [408907,\n",
       "  400951,\n",
       "  400057,\n",
       "  400147,\n",
       "  400343,\n",
       "  401560,\n",
       "  400045,\n",
       "  400122,\n",
       "  401541,\n",
       "  402281,\n",
       "  402283,\n",
       "  402285,\n",
       "  402286,\n",
       "  400088,\n",
       "  402288,\n",
       "  413026,\n",
       "  401464,\n",
       "  401489,\n",
       "  401538,\n",
       "  402290,\n",
       "  402292,\n",
       "  401643,\n",
       "  402800,\n",
       "  402828,\n",
       "  407219,\n",
       "  402789,\n",
       "  408755,\n",
       "  402802,\n",
       "  408756,\n",
       "  400189,\n",
       "  400309,\n",
       "  400417,\n",
       "  400249,\n",
       "  401639,\n",
       "  400662,\n",
       "  400141,\n",
       "  400761,\n",
       "  400490,\n",
       "  401888,\n",
       "  400137,\n",
       "  400716,\n",
       "  401545,\n",
       "  401011,\n",
       "  400674,\n",
       "  400539,\n",
       "  400534,\n",
       "  401062,\n",
       "  401529,\n",
       "  401613,\n",
       "  400536,\n",
       "  400488,\n",
       "  401561,\n",
       "  400611,\n",
       "  400928,\n",
       "  400284,\n",
       "  400041,\n",
       "  408133,\n",
       "  408135,\n",
       "  417665,\n",
       "  412637,\n",
       "  417666,\n",
       "  408134,\n",
       "  400685,\n",
       "  401003,\n",
       "  400898,\n",
       "  400275,\n",
       "  400939,\n",
       "  400180,\n",
       "  400529,\n",
       "  400990,\n",
       "  400515,\n",
       "  400252,\n",
       "  400788,\n",
       "  401517,\n",
       "  401871,\n",
       "  400574,\n",
       "  401629,\n",
       "  400422,\n",
       "  400333,\n",
       "  410363,\n",
       "  400360,\n",
       "  400955,\n",
       "  400495,\n",
       "  400608,\n",
       "  400949,\n",
       "  400678,\n",
       "  400341,\n",
       "  400607,\n",
       "  400094,\n",
       "  400682,\n",
       "  408138,\n",
       "  400980,\n",
       "  401333,\n",
       "  404746,\n",
       "  401142,\n",
       "  400218,\n",
       "  400983,\n",
       "  400765,\n",
       "  400844,\n",
       "  400923,\n",
       "  401143,\n",
       "  401471],\n",
       " 'Speed': [0.75,\n",
       "  0.74,\n",
       "  0.8,\n",
       "  0.74,\n",
       "  0.61,\n",
       "  0.47,\n",
       "  0.69,\n",
       "  0.72,\n",
       "  0.59,\n",
       "  0.36,\n",
       "  0.31,\n",
       "  0.5,\n",
       "  0.54,\n",
       "  0.56,\n",
       "  0.56,\n",
       "  0.59,\n",
       "  0.59,\n",
       "  0.69,\n",
       "  0.31,\n",
       "  0.54,\n",
       "  0.67,\n",
       "  0.73,\n",
       "  0.54,\n",
       "  0.71,\n",
       "  0.58,\n",
       "  0.58,\n",
       "  0.68,\n",
       "  0.58,\n",
       "  0.68,\n",
       "  0.6,\n",
       "  0.6,\n",
       "  0.59,\n",
       "  0.76,\n",
       "  0.72,\n",
       "  0.58,\n",
       "  0.49,\n",
       "  0.55,\n",
       "  0.54,\n",
       "  0.63,\n",
       "  0.7,\n",
       "  0.61,\n",
       "  0.51,\n",
       "  0.68,\n",
       "  0.64,\n",
       "  0.57,\n",
       "  0.68,\n",
       "  0.61,\n",
       "  0.6,\n",
       "  0.52,\n",
       "  0.65,\n",
       "  0.7,\n",
       "  0.72,\n",
       "  0.66,\n",
       "  0.71,\n",
       "  0.69,\n",
       "  0.68,\n",
       "  0.6,\n",
       "  0.63,\n",
       "  0.64,\n",
       "  0.72,\n",
       "  0.68,\n",
       "  0.59,\n",
       "  0.62,\n",
       "  0.6,\n",
       "  0.48,\n",
       "  0.58,\n",
       "  0.68,\n",
       "  0.5,\n",
       "  0.66,\n",
       "  0.61,\n",
       "  0.5,\n",
       "  0.6,\n",
       "  0.56,\n",
       "  0.6,\n",
       "  0.64,\n",
       "  0.61,\n",
       "  0.69,\n",
       "  0.7,\n",
       "  0.68,\n",
       "  0.37,\n",
       "  0.7,\n",
       "  0.73,\n",
       "  0.69,\n",
       "  0.74,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.78,\n",
       "  0.72,\n",
       "  0.64,\n",
       "  0.57,\n",
       "  0.52,\n",
       "  0.55,\n",
       "  0.39,\n",
       "  0.45,\n",
       "  0.52,\n",
       "  0.48,\n",
       "  0.37,\n",
       "  0.51,\n",
       "  0.74,\n",
       "  0.53,\n",
       "  0.54,\n",
       "  0.38],\n",
       " 'Flow': [0.19,\n",
       "  0.21,\n",
       "  0.21,\n",
       "  0.25,\n",
       "  0.24,\n",
       "  0.13,\n",
       "  0.13,\n",
       "  0.22,\n",
       "  0.21,\n",
       "  0.19,\n",
       "  0.19,\n",
       "  0.18,\n",
       "  0.16,\n",
       "  0.14,\n",
       "  0.19,\n",
       "  0.21,\n",
       "  0.19,\n",
       "  0.22,\n",
       "  0.2,\n",
       "  0.21,\n",
       "  0.22,\n",
       "  0.21,\n",
       "  0.41,\n",
       "  0.22,\n",
       "  0.21,\n",
       "  0.24,\n",
       "  0.21,\n",
       "  0.17,\n",
       "  0.21,\n",
       "  0.21,\n",
       "  0.22,\n",
       "  0.24,\n",
       "  0.26,\n",
       "  0.23,\n",
       "  0.25,\n",
       "  0.2,\n",
       "  0.21,\n",
       "  0.2,\n",
       "  0.14,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.2,\n",
       "  0.19,\n",
       "  0.22,\n",
       "  0.23,\n",
       "  0.21,\n",
       "  0.19,\n",
       "  0.22,\n",
       "  0.19,\n",
       "  0.22,\n",
       "  0.23,\n",
       "  0.22,\n",
       "  0.23,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.23,\n",
       "  0.24,\n",
       "  0.23,\n",
       "  0.23,\n",
       "  0.23,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.28,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.27,\n",
       "  0.26,\n",
       "  0.23,\n",
       "  0.26,\n",
       "  0.27,\n",
       "  0.24,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.21,\n",
       "  0.27,\n",
       "  0.32,\n",
       "  0.22,\n",
       "  0.24,\n",
       "  0.15,\n",
       "  0.26,\n",
       "  0.26,\n",
       "  0.21,\n",
       "  0.25,\n",
       "  0.25,\n",
       "  0.23,\n",
       "  0.26,\n",
       "  0.22,\n",
       "  0.9,\n",
       "  0.18,\n",
       "  0.17,\n",
       "  0.17,\n",
       "  0.13,\n",
       "  0.14,\n",
       "  0.15,\n",
       "  0.18,\n",
       "  0.12,\n",
       "  0.22,\n",
       "  0.2,\n",
       "  0.22,\n",
       "  0.18,\n",
       "  0.51],\n",
       " 'Occupancy': [0.55,\n",
       "  0.52,\n",
       "  0.52,\n",
       "  0.41,\n",
       "  0.35,\n",
       "  0.16,\n",
       "  0.19,\n",
       "  0.41,\n",
       "  0.21,\n",
       "  0.16,\n",
       "  0.16,\n",
       "  0.27,\n",
       "  0.28,\n",
       "  0.28,\n",
       "  0.3,\n",
       "  0.32,\n",
       "  0.29,\n",
       "  0.46,\n",
       "  0.25,\n",
       "  0.36,\n",
       "  0.41,\n",
       "  0.43,\n",
       "  0.42,\n",
       "  0.34,\n",
       "  0.29,\n",
       "  0.3,\n",
       "  0.31,\n",
       "  0.24,\n",
       "  0.31,\n",
       "  0.26,\n",
       "  0.28,\n",
       "  0.34,\n",
       "  0.42,\n",
       "  0.47,\n",
       "  0.3,\n",
       "  0.26,\n",
       "  0.31,\n",
       "  0.4,\n",
       "  0.32,\n",
       "  0.4,\n",
       "  0.37,\n",
       "  0.37,\n",
       "  0.42,\n",
       "  0.35,\n",
       "  0.32,\n",
       "  0.3,\n",
       "  0.34,\n",
       "  0.28,\n",
       "  0.26,\n",
       "  0.45,\n",
       "  0.46,\n",
       "  0.47,\n",
       "  0.28,\n",
       "  0.36,\n",
       "  0.34,\n",
       "  0.36,\n",
       "  0.39,\n",
       "  0.42,\n",
       "  0.44,\n",
       "  0.39,\n",
       "  0.4,\n",
       "  0.41,\n",
       "  0.38,\n",
       "  0.31,\n",
       "  0.24,\n",
       "  0.31,\n",
       "  0.36,\n",
       "  0.28,\n",
       "  0.39,\n",
       "  0.38,\n",
       "  0.38,\n",
       "  0.42,\n",
       "  0.4,\n",
       "  0.39,\n",
       "  0.39,\n",
       "  0.34,\n",
       "  0.39,\n",
       "  0.4,\n",
       "  0.31,\n",
       "  0.23,\n",
       "  0.43,\n",
       "  0.47,\n",
       "  0.41,\n",
       "  0.39,\n",
       "  0.34,\n",
       "  0.34,\n",
       "  0.4,\n",
       "  0.33,\n",
       "  0.77,\n",
       "  0.26,\n",
       "  0.21,\n",
       "  0.23,\n",
       "  0.13,\n",
       "  0.21,\n",
       "  0.23,\n",
       "  0.38,\n",
       "  0.21,\n",
       "  0.39,\n",
       "  0.53,\n",
       "  0.45,\n",
       "  0.43,\n",
       "  0.44]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alphas_df = pd.DataFrame(best_alphas)\n",
    "best_alphas_df.to_csv(base_path + 'smaller_sample/best_alphas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the tuned alphas to predict training traffic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/102] Start time series prediction (DES) at station 408907...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 408907.\n",
      "[2/102] Start time series prediction (DES) at station 400951...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400951.\n",
      "[3/102] Start time series prediction (DES) at station 400057...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400057.\n",
      "[4/102] Start time series prediction (DES) at station 400147...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400147.\n",
      "[5/102] Start time series prediction (DES) at station 400343...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400343.\n",
      "[6/102] Start time series prediction (DES) at station 401560...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401560.\n",
      "[7/102] Start time series prediction (DES) at station 400045...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400045.\n",
      "[8/102] Start time series prediction (DES) at station 400122...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400122.\n",
      "[9/102] Start time series prediction (DES) at station 401541...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401541.\n",
      "[10/102] Start time series prediction (DES) at station 402281...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402281.\n",
      "[11/102] Start time series prediction (DES) at station 402283...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402283.\n",
      "[12/102] Start time series prediction (DES) at station 402285...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402285.\n",
      "[13/102] Start time series prediction (DES) at station 402286...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402286.\n",
      "[14/102] Start time series prediction (DES) at station 400088...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400088.\n",
      "[15/102] Start time series prediction (DES) at station 402288...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402288.\n",
      "[16/102] Start time series prediction (DES) at station 413026...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 413026.\n",
      "[17/102] Start time series prediction (DES) at station 401464...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401464.\n",
      "[18/102] Start time series prediction (DES) at station 401489...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401489.\n",
      "[19/102] Start time series prediction (DES) at station 401538...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401538.\n",
      "[20/102] Start time series prediction (DES) at station 402290...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402290.\n",
      "[21/102] Start time series prediction (DES) at station 402292...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402292.\n",
      "[22/102] Start time series prediction (DES) at station 401643...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401643.\n",
      "[23/102] Start time series prediction (DES) at station 402800...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402800.\n",
      "[24/102] Start time series prediction (DES) at station 402828...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402828.\n",
      "[25/102] Start time series prediction (DES) at station 407219...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 407219.\n",
      "[26/102] Start time series prediction (DES) at station 402789...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402789.\n",
      "[27/102] Start time series prediction (DES) at station 408755...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 408755.\n",
      "[28/102] Start time series prediction (DES) at station 402802...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 402802.\n",
      "[29/102] Start time series prediction (DES) at station 408756...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 408756.\n",
      "[30/102] Start time series prediction (DES) at station 400189...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400189.\n",
      "[31/102] Start time series prediction (DES) at station 400309...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400309.\n",
      "[32/102] Start time series prediction (DES) at station 400417...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400417.\n",
      "[33/102] Start time series prediction (DES) at station 400249...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400249.\n",
      "[34/102] Start time series prediction (DES) at station 401639...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401639.\n",
      "[35/102] Start time series prediction (DES) at station 400662...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400662.\n",
      "[36/102] Start time series prediction (DES) at station 400141...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400141.\n",
      "[37/102] Start time series prediction (DES) at station 400761...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400761.\n",
      "[38/102] Start time series prediction (DES) at station 400490...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400490.\n",
      "[39/102] Start time series prediction (DES) at station 401888...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401888.\n",
      "[40/102] Start time series prediction (DES) at station 400137...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400137.\n",
      "[41/102] Start time series prediction (DES) at station 400716...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400716.\n",
      "[42/102] Start time series prediction (DES) at station 401545...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401545.\n",
      "[43/102] Start time series prediction (DES) at station 401011...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401011.\n",
      "[44/102] Start time series prediction (DES) at station 400674...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400674.\n",
      "[45/102] Start time series prediction (DES) at station 400539...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400539.\n",
      "[46/102] Start time series prediction (DES) at station 400534...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400534.\n",
      "[47/102] Start time series prediction (DES) at station 401062...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401062.\n",
      "[48/102] Start time series prediction (DES) at station 401529...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401529.\n",
      "[49/102] Start time series prediction (DES) at station 401613...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401613.\n",
      "[50/102] Start time series prediction (DES) at station 400536...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400536.\n",
      "[51/102] Start time series prediction (DES) at station 400488...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400488.\n",
      "[52/102] Start time series prediction (DES) at station 401561...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401561.\n",
      "[53/102] Start time series prediction (DES) at station 400611...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400611.\n",
      "[54/102] Start time series prediction (DES) at station 400928...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400928.\n",
      "[55/102] Start time series prediction (DES) at station 400284...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400284.\n",
      "[56/102] Start time series prediction (DES) at station 400041...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400041.\n",
      "[57/102] Start time series prediction (DES) at station 408133...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 408133.\n",
      "[58/102] Start time series prediction (DES) at station 408135...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 408135.\n",
      "[59/102] Start time series prediction (DES) at station 417665...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 417665.\n",
      "[60/102] Start time series prediction (DES) at station 412637...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End prediction at station 412637.\n",
      "[61/102] Start time series prediction (DES) at station 417666...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 417666.\n",
      "[62/102] Start time series prediction (DES) at station 408134...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 408134.\n",
      "[63/102] Start time series prediction (DES) at station 400685...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400685.\n",
      "[64/102] Start time series prediction (DES) at station 401003...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401003.\n",
      "[65/102] Start time series prediction (DES) at station 400898...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400898.\n",
      "[66/102] Start time series prediction (DES) at station 400275...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400275.\n",
      "[67/102] Start time series prediction (DES) at station 400939...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400939.\n",
      "[68/102] Start time series prediction (DES) at station 400180...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400180.\n",
      "[69/102] Start time series prediction (DES) at station 400529...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400529.\n",
      "[70/102] Start time series prediction (DES) at station 400990...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400990.\n",
      "[71/102] Start time series prediction (DES) at station 400515...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400515.\n",
      "[72/102] Start time series prediction (DES) at station 400252...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400252.\n",
      "[73/102] Start time series prediction (DES) at station 400788...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400788.\n",
      "[74/102] Start time series prediction (DES) at station 401517...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401517.\n",
      "[75/102] Start time series prediction (DES) at station 401871...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401871.\n",
      "[76/102] Start time series prediction (DES) at station 400574...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400574.\n",
      "[77/102] Start time series prediction (DES) at station 401629...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401629.\n",
      "[78/102] Start time series prediction (DES) at station 400422...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400422.\n",
      "[79/102] Start time series prediction (DES) at station 400333...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400333.\n",
      "[80/102] Start time series prediction (DES) at station 410363...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 410363.\n",
      "[81/102] Start time series prediction (DES) at station 400360...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400360.\n",
      "[82/102] Start time series prediction (DES) at station 400955...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400955.\n",
      "[83/102] Start time series prediction (DES) at station 400495...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400495.\n",
      "[84/102] Start time series prediction (DES) at station 400608...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400608.\n",
      "[85/102] Start time series prediction (DES) at station 400949...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400949.\n",
      "[86/102] Start time series prediction (DES) at station 400678...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400678.\n",
      "[87/102] Start time series prediction (DES) at station 400341...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400341.\n",
      "[88/102] Start time series prediction (DES) at station 400607...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400607.\n",
      "[89/102] Start time series prediction (DES) at station 400094...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400094.\n",
      "[90/102] Start time series prediction (DES) at station 400682...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400682.\n",
      "[91/102] Start time series prediction (DES) at station 408138...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 408138.\n",
      "[92/102] Start time series prediction (DES) at station 400980...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400980.\n",
      "[93/102] Start time series prediction (DES) at station 401333...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401333.\n",
      "[94/102] Start time series prediction (DES) at station 404746...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 404746.\n",
      "[95/102] Start time series prediction (DES) at station 401142...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401142.\n",
      "[96/102] Start time series prediction (DES) at station 400218...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400218.\n",
      "[97/102] Start time series prediction (DES) at station 400983...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400983.\n",
      "[98/102] Start time series prediction (DES) at station 400765...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400765.\n",
      "[99/102] Start time series prediction (DES) at station 400844...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400844.\n",
      "[100/102] Start time series prediction (DES) at station 400923...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 400923.\n",
      "[101/102] Start time series prediction (DES) at station 401143...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401143.\n",
      "[102/102] Start time series prediction (DES) at station 401471...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "End prediction at station 401471.\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "# initialize prediction dictionary\n",
    "pred_dict_train = dict()\n",
    "for var_name in var_names:\n",
    "    pred_dict_train[var_name] = []\n",
    "\n",
    "for i, station in enumerate(stations):\n",
    "    print(\"{} Start time series prediction (DES) at station {}...\".format(fraction_msg(i+1, len(stations)), station))\n",
    "    df_train_station = raw_train.loc[raw_train[\"Station ID\"] == station]\n",
    "    \n",
    "    # formulate predictions of speed, flow and occupancy for the station\n",
    "    for var_name in var_names:\n",
    "        print(\"    {}...\".format(var_name))\n",
    "        var_series = df_train_station[var_name].values\n",
    "        len_series = len(var_series)\n",
    "        # initialize s1, s2, and y\n",
    "        s1 = np.mean(var_series[:10])\n",
    "        s2 = s1\n",
    "        y = [0.] * len_series\n",
    "        # get the best alpha\n",
    "        var_best_alpha = best_alphas_df.loc[best_alphas_df[\"Station ID\"] == station][var_name].values[0]\n",
    "        beta = 1. - var_best_alpha\n",
    "\n",
    "        for t in range(11, len_series - 1):\n",
    "            s1 = var_best_alpha * var_series[t] + beta * s1\n",
    "            s2 = var_best_alpha * s1 + beta * s2\n",
    "            y[t+1] = round(2 * s1 - s2 + var_best_alpha / beta * (s1 - s2), 2)\n",
    "\n",
    "        # save the predictions to a dictionary\n",
    "        pred_dict_train[var_name].extend(y)\n",
    "    print(\"End prediction at station {}.\".format(station))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = raw_train.assign(Pred_Speed=pred_dict_train['Speed'], Pred_Flow=pred_dict_train['Flow'], Pred_Occupancy=pred_dict_train['Occupancy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['Diff_Speed'] = raw_train['Speed'] - raw_train['Pred_Speed']\n",
    "raw_train['Diff_Flow'] = raw_train['Flow'] - raw_train['Pred_Flow']\n",
    "raw_train['Diff_Occupancy'] = raw_train['Occupancy'] - raw_train['Pred_Occupancy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the tuned alphas to predict testing traffic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/102] Start time series prediction (DES) at station 408907...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 408907.\n",
      "[2/102] Start time series prediction (DES) at station 400951...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400951.\n",
      "[3/102] Start time series prediction (DES) at station 400057...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400057.\n",
      "[4/102] Start time series prediction (DES) at station 400147...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400147.\n",
      "[5/102] Start time series prediction (DES) at station 400343...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400343.\n",
      "[6/102] Start time series prediction (DES) at station 401560...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401560.\n",
      "[7/102] Start time series prediction (DES) at station 400045...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400045.\n",
      "[8/102] Start time series prediction (DES) at station 400122...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400122.\n",
      "[9/102] Start time series prediction (DES) at station 401541...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401541.\n",
      "[10/102] Start time series prediction (DES) at station 402281...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402281.\n",
      "[11/102] Start time series prediction (DES) at station 402283...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402283.\n",
      "[12/102] Start time series prediction (DES) at station 402285...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402285.\n",
      "[13/102] Start time series prediction (DES) at station 402286...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402286.\n",
      "[14/102] Start time series prediction (DES) at station 400088...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400088.\n",
      "[15/102] Start time series prediction (DES) at station 402288...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402288.\n",
      "[16/102] Start time series prediction (DES) at station 413026...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 413026.\n",
      "[17/102] Start time series prediction (DES) at station 401464...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401464.\n",
      "[18/102] Start time series prediction (DES) at station 401489...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401489.\n",
      "[19/102] Start time series prediction (DES) at station 401538...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401538.\n",
      "[20/102] Start time series prediction (DES) at station 402290...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402290.\n",
      "[21/102] Start time series prediction (DES) at station 402292...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402292.\n",
      "[22/102] Start time series prediction (DES) at station 401643...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401643.\n",
      "[23/102] Start time series prediction (DES) at station 402800...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402800.\n",
      "[24/102] Start time series prediction (DES) at station 402828...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402828.\n",
      "[25/102] Start time series prediction (DES) at station 407219...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 407219.\n",
      "[26/102] Start time series prediction (DES) at station 402789...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402789.\n",
      "[27/102] Start time series prediction (DES) at station 408755...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 408755.\n",
      "[28/102] Start time series prediction (DES) at station 402802...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 402802.\n",
      "[29/102] Start time series prediction (DES) at station 408756...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 408756.\n",
      "[30/102] Start time series prediction (DES) at station 400189...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400189.\n",
      "[31/102] Start time series prediction (DES) at station 400309...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400309.\n",
      "[32/102] Start time series prediction (DES) at station 400417...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400417.\n",
      "[33/102] Start time series prediction (DES) at station 400249...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400249.\n",
      "[34/102] Start time series prediction (DES) at station 401639...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401639.\n",
      "[35/102] Start time series prediction (DES) at station 400662...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400662.\n",
      "[36/102] Start time series prediction (DES) at station 400141...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400141.\n",
      "[37/102] Start time series prediction (DES) at station 400761...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400761.\n",
      "[38/102] Start time series prediction (DES) at station 400490...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400490.\n",
      "[39/102] Start time series prediction (DES) at station 401888...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401888.\n",
      "[40/102] Start time series prediction (DES) at station 400137...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400137.\n",
      "[41/102] Start time series prediction (DES) at station 400716...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400716.\n",
      "[42/102] Start time series prediction (DES) at station 401545...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401545.\n",
      "[43/102] Start time series prediction (DES) at station 401011...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401011.\n",
      "[44/102] Start time series prediction (DES) at station 400674...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400674.\n",
      "[45/102] Start time series prediction (DES) at station 400539...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400539.\n",
      "[46/102] Start time series prediction (DES) at station 400534...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400534.\n",
      "[47/102] Start time series prediction (DES) at station 401062...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401062.\n",
      "[48/102] Start time series prediction (DES) at station 401529...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401529.\n",
      "[49/102] Start time series prediction (DES) at station 401613...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401613.\n",
      "[50/102] Start time series prediction (DES) at station 400536...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400536.\n",
      "[51/102] Start time series prediction (DES) at station 400488...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400488.\n",
      "[52/102] Start time series prediction (DES) at station 401561...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401561.\n",
      "[53/102] Start time series prediction (DES) at station 400611...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400611.\n",
      "[54/102] Start time series prediction (DES) at station 400928...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400928.\n",
      "[55/102] Start time series prediction (DES) at station 400284...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400284.\n",
      "[56/102] Start time series prediction (DES) at station 400041...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400041.\n",
      "[57/102] Start time series prediction (DES) at station 408133...\n",
      "    Speed...\n",
      "    Flow...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Occupancy...\n",
      "Finished forecasting at station 408133.\n",
      "[58/102] Start time series prediction (DES) at station 408135...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 408135.\n",
      "[59/102] Start time series prediction (DES) at station 417665...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 417665.\n",
      "[60/102] Start time series prediction (DES) at station 412637...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 412637.\n",
      "[61/102] Start time series prediction (DES) at station 417666...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 417666.\n",
      "[62/102] Start time series prediction (DES) at station 408134...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 408134.\n",
      "[63/102] Start time series prediction (DES) at station 400685...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400685.\n",
      "[64/102] Start time series prediction (DES) at station 401003...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401003.\n",
      "[65/102] Start time series prediction (DES) at station 400898...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400898.\n",
      "[66/102] Start time series prediction (DES) at station 400275...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400275.\n",
      "[67/102] Start time series prediction (DES) at station 400939...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400939.\n",
      "[68/102] Start time series prediction (DES) at station 400180...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400180.\n",
      "[69/102] Start time series prediction (DES) at station 400529...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400529.\n",
      "[70/102] Start time series prediction (DES) at station 400990...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400990.\n",
      "[71/102] Start time series prediction (DES) at station 400515...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400515.\n",
      "[72/102] Start time series prediction (DES) at station 400252...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400252.\n",
      "[73/102] Start time series prediction (DES) at station 400788...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400788.\n",
      "[74/102] Start time series prediction (DES) at station 401517...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401517.\n",
      "[75/102] Start time series prediction (DES) at station 401871...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401871.\n",
      "[76/102] Start time series prediction (DES) at station 400574...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400574.\n",
      "[77/102] Start time series prediction (DES) at station 401629...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401629.\n",
      "[78/102] Start time series prediction (DES) at station 400422...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400422.\n",
      "[79/102] Start time series prediction (DES) at station 400333...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400333.\n",
      "[80/102] Start time series prediction (DES) at station 410363...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 410363.\n",
      "[81/102] Start time series prediction (DES) at station 400360...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400360.\n",
      "[82/102] Start time series prediction (DES) at station 400955...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400955.\n",
      "[83/102] Start time series prediction (DES) at station 400495...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400495.\n",
      "[84/102] Start time series prediction (DES) at station 400608...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400608.\n",
      "[85/102] Start time series prediction (DES) at station 400949...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400949.\n",
      "[86/102] Start time series prediction (DES) at station 400678...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400678.\n",
      "[87/102] Start time series prediction (DES) at station 400341...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400341.\n",
      "[88/102] Start time series prediction (DES) at station 400607...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400607.\n",
      "[89/102] Start time series prediction (DES) at station 400094...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400094.\n",
      "[90/102] Start time series prediction (DES) at station 400682...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400682.\n",
      "[91/102] Start time series prediction (DES) at station 408138...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 408138.\n",
      "[92/102] Start time series prediction (DES) at station 400980...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400980.\n",
      "[93/102] Start time series prediction (DES) at station 401333...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401333.\n",
      "[94/102] Start time series prediction (DES) at station 404746...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 404746.\n",
      "[95/102] Start time series prediction (DES) at station 401142...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401142.\n",
      "[96/102] Start time series prediction (DES) at station 400218...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400218.\n",
      "[97/102] Start time series prediction (DES) at station 400983...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400983.\n",
      "[98/102] Start time series prediction (DES) at station 400765...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400765.\n",
      "[99/102] Start time series prediction (DES) at station 400844...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400844.\n",
      "[100/102] Start time series prediction (DES) at station 400923...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 400923.\n",
      "[101/102] Start time series prediction (DES) at station 401143...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401143.\n",
      "[102/102] Start time series prediction (DES) at station 401471...\n",
      "    Speed...\n",
      "    Flow...\n",
      "    Occupancy...\n",
      "Finished forecasting at station 401471.\n",
      "Finished forecasting for the test dataset.\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "# initialize prediction dictionary\n",
    "pred_dict_test = dict()\n",
    "for var_name in var_names:\n",
    "    pred_dict_test[var_name] = []\n",
    "\n",
    "for i, station in enumerate(stations):\n",
    "    print(\"{} Start time series prediction (DES) at station {}...\".format(fraction_msg(i+1, len(stations)), station))\n",
    "    df_test_station = raw_test.loc[raw_test[\"Station ID\"] == station]\n",
    "    \n",
    "    # formulate predictions of speed, flow and occupancy for the station\n",
    "    for var_name in var_names:\n",
    "        print(\"    {}...\".format(var_name))\n",
    "        var_series = df_test_station[var_name].values\n",
    "        len_series = len(var_series)\n",
    "        # initialize s1, s2, and y\n",
    "        s1 = np.mean(var_series[:10])\n",
    "        s2 = s1\n",
    "        y = [0.] * len_series\n",
    "        # get the best alpha\n",
    "        var_best_alpha = best_alphas_df.loc[best_alphas_df[\"Station ID\"] == station][var_name].values[0]\n",
    "        beta = 1. - var_best_alpha\n",
    "\n",
    "        num_batches = int(len_series / 288)\n",
    "        for j in range(num_batches):\n",
    "            base_idx = 288 * j\n",
    "            for t in range(base_idx + 11, base_idx + 287):\n",
    "                s1 = var_best_alpha * var_series[t] + beta * s1\n",
    "                s2 = var_best_alpha * s1 + beta * s2\n",
    "                y[t+1] = round(2 * s1 - s2 + var_best_alpha / beta * (s1 - s2), 2)\n",
    "\n",
    "        # save the predictions to a dictionary\n",
    "        pred_dict_test[var_name].extend(y)\n",
    "    print(\"Finished forecasting at station {}.\".format(station))\n",
    "print(\"Finished forecasting for the test dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test = raw_test.assign(Pred_Speed=pred_dict_test['Speed'], Pred_Flow=pred_dict_test['Flow'], Pred_Occupancy=pred_dict_test['Occupancy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test['Diff_Speed'] = raw_test['Speed'] - raw_test['Pred_Speed']\n",
    "raw_test['Diff_Flow'] = raw_test['Flow'] - raw_test['Pred_Flow']\n",
    "raw_test['Diff_Occupancy'] = raw_test['Occupancy'] - raw_test['Pred_Occupancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station ID</th>\n",
       "      <th>datetime</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>Flow</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>idx</th>\n",
       "      <th>LambdaMax</th>\n",
       "      <th>Sigma</th>\n",
       "      <th>Tau</th>\n",
       "      <th>Impact</th>\n",
       "      <th>Pred_Speed</th>\n",
       "      <th>Pred_Flow</th>\n",
       "      <th>Pred_Occupancy</th>\n",
       "      <th>Diff_Speed</th>\n",
       "      <th>Diff_Flow</th>\n",
       "      <th>Diff_Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6532125</th>\n",
       "      <td>401471</td>\n",
       "      <td>2017-06-28 23:45:00</td>\n",
       "      <td>0.7</td>\n",
       "      <td>23.0</td>\n",
       "      <td>67.3</td>\n",
       "      <td>06/28/2017</td>\n",
       "      <td>23:45</td>\n",
       "      <td>3201453</td>\n",
       "      <td>0.038882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064840</td>\n",
       "      <td>67.22</td>\n",
       "      <td>23.99</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6532126</th>\n",
       "      <td>401471</td>\n",
       "      <td>2017-06-28 23:50:00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>67.2</td>\n",
       "      <td>06/28/2017</td>\n",
       "      <td>23:50</td>\n",
       "      <td>3201454</td>\n",
       "      <td>0.038882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064408</td>\n",
       "      <td>67.29</td>\n",
       "      <td>21.89</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6532127</th>\n",
       "      <td>401471</td>\n",
       "      <td>2017-06-28 23:55:00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>19.0</td>\n",
       "      <td>67.1</td>\n",
       "      <td>06/28/2017</td>\n",
       "      <td>23:55</td>\n",
       "      <td>3201455</td>\n",
       "      <td>0.038882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063334</td>\n",
       "      <td>67.24</td>\n",
       "      <td>21.68</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-2.68</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Station ID             datetime  Occupancy  Flow  Speed        Date  \\\n",
       "6532125      401471  2017-06-28 23:45:00        0.7  23.0   67.3  06/28/2017   \n",
       "6532126      401471  2017-06-28 23:50:00        0.6  23.0   67.2  06/28/2017   \n",
       "6532127      401471  2017-06-28 23:55:00        0.6  19.0   67.1  06/28/2017   \n",
       "\n",
       "          Time      idx  LambdaMax  Sigma  Tau    Impact  Pred_Speed  \\\n",
       "6532125  23:45  3201453   0.038882    0.0  0.0  0.064840       67.22   \n",
       "6532126  23:50  3201454   0.038882    0.0  0.0  0.064408       67.29   \n",
       "6532127  23:55  3201455   0.038882    0.0  0.0  0.063334       67.24   \n",
       "\n",
       "         Pred_Flow  Pred_Occupancy  Diff_Speed  Diff_Flow  Diff_Occupancy  \n",
       "6532125      23.99            0.69        0.08      -0.99            0.01  \n",
       "6532126      21.89            0.67       -0.09       1.11           -0.07  \n",
       "6532127      21.68            0.58       -0.14      -2.68            0.02  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sd_dict = dict()\n",
    "pred_var_names = []\n",
    "diff_var_names = []\n",
    "for var in var_names:\n",
    "    mean_sd_dict[var + \"_mean\"] = []\n",
    "    mean_sd_dict[var + \"_sd\"] = []\n",
    "    pred_var_names.append(\"Pred_\" + var)\n",
    "    diff_var_names.append(\"Diff_\" + var)\n",
    "for var in pred_var_names:\n",
    "    mean_sd_dict[var + \"_mean\"] = []\n",
    "    mean_sd_dict[var + \"_sd\"] = []\n",
    "for var in diff_var_names:\n",
    "    mean_sd_dict[var + \"_mean\"] = []\n",
    "    mean_sd_dict[var + \"_sd\"] = []\n",
    "mean_sd_dict['station'] = stations\n",
    "\n",
    "for i, s in enumerate(stations):\n",
    "    s_df = raw_train[raw_train['Station ID'] == s]\n",
    "    for j, var in enumerate(var_names):\n",
    "        var_values = s_df[var].values\n",
    "        mean_sd_dict[var + \"_mean\"].append(np.mean(var_values))\n",
    "        mean_sd_dict[var + \"_sd\"].append(np.std(var_values))\n",
    "    for j, var in enumerate(pred_var_names):\n",
    "        var_values = s_df[var].values\n",
    "        mean_sd_dict[var + \"_mean\"].append(np.mean(var_values))\n",
    "        mean_sd_dict[var + \"_sd\"].append(np.std(var_values))\n",
    "    for j, var in enumerate(diff_var_names):\n",
    "        var_values = s_df[var].values\n",
    "        mean_sd_dict[var + \"_mean\"].append(np.mean(var_values))\n",
    "        mean_sd_dict[var + \"_sd\"].append(np.std(var_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_train = raw_train['Station ID'].values\n",
    "station_idx_dict = dict()\n",
    "for i, s in enumerate(stations):\n",
    "    station_idx_dict[s] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Speed', 'Flow', 'Occupancy', \n",
    "           'Pred_Speed', 'Pred_Flow', 'Pred_Occupancy', \n",
    "           'Diff_Speed', 'Diff_Flow', 'Diff_Occupancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished z-score computation: Speed.\n",
      "Finished z-score computation: Flow.\n",
      "Finished z-score computation: Occupancy.\n",
      "Finished z-score computation: Pred_Speed.\n",
      "Finished z-score computation: Pred_Flow.\n",
      "Finished z-score computation: Pred_Occupancy.\n",
      "Finished z-score computation: Diff_Speed.\n",
      "Finished z-score computation: Diff_Flow.\n",
      "Finished z-score computation: Diff_Occupancy.\n"
     ]
    }
   ],
   "source": [
    "z_dict = dict()\n",
    "for col in columns:\n",
    "    z_scores = []\n",
    "    col_train = raw_train[col].values\n",
    "    for i, val in enumerate(col_train):\n",
    "        sid = stations_train[i]\n",
    "        sidx = station_idx_dict[sid]\n",
    "        z_score = (val - mean_sd_dict[col+'_mean'][sidx]) / mean_sd_dict[col+'_sd'][sidx]\n",
    "        z_scores.append(z_score)\n",
    "    z_dict[col] = z_scores\n",
    "    print(\"Finished z-score computation: {}.\".format(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in z_dict.keys():\n",
    "    raw_train[key] = z_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished z-score computation: Speed.\n",
      "Finished z-score computation: Flow.\n",
      "Finished z-score computation: Occupancy.\n",
      "Finished z-score computation: Pred_Speed.\n",
      "Finished z-score computation: Pred_Flow.\n",
      "Finished z-score computation: Pred_Occupancy.\n",
      "Finished z-score computation: Diff_Speed.\n",
      "Finished z-score computation: Diff_Flow.\n",
      "Finished z-score computation: Diff_Occupancy.\n"
     ]
    }
   ],
   "source": [
    "stations_test = raw_test['Station ID'].values\n",
    "z_dict_test = dict()\n",
    "for col in columns:\n",
    "    z_scores = []\n",
    "    col_test = raw_test[col].values\n",
    "    for i, val in enumerate(col_test):\n",
    "        sid = stations_test[i]\n",
    "        sidx = station_idx_dict[sid]\n",
    "        z_score = (val - mean_sd_dict[col+'_mean'][sidx]) / mean_sd_dict[col+'_sd'][sidx]\n",
    "        z_scores.append(z_score)\n",
    "    z_dict_test[col] = z_scores\n",
    "    print(\"Finished z-score computation: {}.\".format(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in z_dict.keys():\n",
    "    raw_test[key] = z_dict_test[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekday/weekend Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dates = raw_train['Date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekday(weekday):\n",
    "    if weekday - 5 >= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_weekday = [is_weekday(date_to_day(date)) for date in raw_train_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['weekday'] = raw_train_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station ID</th>\n",
       "      <th>datetime</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>Flow</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>LambdaMax</th>\n",
       "      <th>Sigma</th>\n",
       "      <th>Tau</th>\n",
       "      <th>Impact</th>\n",
       "      <th>Pred_Speed</th>\n",
       "      <th>Pred_Flow</th>\n",
       "      <th>Pred_Occupancy</th>\n",
       "      <th>Diff_Speed</th>\n",
       "      <th>Diff_Flow</th>\n",
       "      <th>Diff_Occupancy</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8928</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-02 00:00:00</td>\n",
       "      <td>-0.953162</td>\n",
       "      <td>-1.373286</td>\n",
       "      <td>0.586367</td>\n",
       "      <td>06/02/2017</td>\n",
       "      <td>00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016859</td>\n",
       "      <td>-10.800253</td>\n",
       "      <td>-1.611214</td>\n",
       "      <td>-1.072184</td>\n",
       "      <td>18.179947</td>\n",
       "      <td>1.214776</td>\n",
       "      <td>0.446144</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8929</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-02 00:05:00</td>\n",
       "      <td>-0.981987</td>\n",
       "      <td>-1.341938</td>\n",
       "      <td>0.565064</td>\n",
       "      <td>06/02/2017</td>\n",
       "      <td>00:05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017058</td>\n",
       "      <td>-10.800253</td>\n",
       "      <td>-1.611214</td>\n",
       "      <td>-1.072184</td>\n",
       "      <td>18.153611</td>\n",
       "      <td>1.360780</td>\n",
       "      <td>0.382282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8930</th>\n",
       "      <td>408907</td>\n",
       "      <td>2017-06-02 00:10:00</td>\n",
       "      <td>-1.010811</td>\n",
       "      <td>-1.415085</td>\n",
       "      <td>0.586367</td>\n",
       "      <td>06/02/2017</td>\n",
       "      <td>00:10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019758</td>\n",
       "      <td>-10.800253</td>\n",
       "      <td>-1.611214</td>\n",
       "      <td>-1.072184</td>\n",
       "      <td>18.179947</td>\n",
       "      <td>1.020103</td>\n",
       "      <td>0.318421</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Station ID             datetime  Occupancy      Flow     Speed  \\\n",
       "8928      408907  2017-06-02 00:00:00  -0.953162 -1.373286  0.586367   \n",
       "8929      408907  2017-06-02 00:05:00  -0.981987 -1.341938  0.565064   \n",
       "8930      408907  2017-06-02 00:10:00  -1.010811 -1.415085  0.586367   \n",
       "\n",
       "            Date   Time  LambdaMax  Sigma  Tau    Impact  Pred_Speed  \\\n",
       "8928  06/02/2017  00:00        0.0    0.0  0.0  0.016859  -10.800253   \n",
       "8929  06/02/2017  00:05        0.0    0.0  0.0  0.017058  -10.800253   \n",
       "8930  06/02/2017  00:10        0.0    0.0  0.0  0.019758  -10.800253   \n",
       "\n",
       "      Pred_Flow  Pred_Occupancy  Diff_Speed  Diff_Flow  Diff_Occupancy  \\\n",
       "8928  -1.611214       -1.072184   18.179947   1.214776        0.446144   \n",
       "8929  -1.611214       -1.072184   18.153611   1.360780        0.382282   \n",
       "8930  -1.611214       -1.072184   18.179947   1.020103        0.318421   \n",
       "\n",
       "      weekday  \n",
       "8928        1  \n",
       "8929        1  \n",
       "8930        1  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to rescale train and test dataset with the same factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train: feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train: feature vectors - negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_times = raw_train['Time'].unique().tolist()[14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sample_dates = dates_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_incidents_sample = svm_pos_timestamps_train.loc[svm_pos_timestamps_train['Date'].isin(neg_sample_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/17] Negative feature vectors at date 06/02/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/02/2017 15:50 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/02/2017 09:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/02/2017 23:40 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/02/2017 09:25 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/02/2017 19:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[2/17] Negative feature vectors at date 06/03/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/03/2017 02:35 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/03/2017 08:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/03/2017 08:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/03/2017 09:00 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/03/2017 02:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[3/17] Negative feature vectors at date 06/04/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/04/2017 06:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/04/2017 06:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/04/2017 19:35 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/04/2017 14:05 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/04/2017 17:00 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[4/17] Negative feature vectors at date 06/05/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/05/2017 01:50 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/05/2017 15:25 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/05/2017 22:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/05/2017 11:40 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/05/2017 18:40 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[5/17] Negative feature vectors at date 06/06/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/06/2017 06:25 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/06/2017 16:05 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/06/2017 13:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/06/2017 18:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/06/2017 02:05 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[6/17] Negative feature vectors at date 06/09/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/09/2017 15:05 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/09/2017 20:05 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/09/2017 01:25 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/09/2017 03:50 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Feature vector at date and time 06/09/2017 10:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[7/17] Negative feature vectors at date 06/10/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/10/2017 12:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/10/2017 21:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/10/2017 17:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/10/2017 05:50 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/10/2017 19:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[8/17] Negative feature vectors at date 06/13/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/13/2017 17:25 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/13/2017 21:25 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/13/2017 13:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/13/2017 12:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/13/2017 11:50 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[9/17] Negative feature vectors at date 06/14/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/14/2017 08:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/14/2017 19:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/14/2017 08:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/14/2017 10:00 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/14/2017 20:05 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[10/17] Negative feature vectors at date 06/16/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/16/2017 01:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/16/2017 08:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/16/2017 17:05 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/16/2017 07:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/16/2017 23:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[11/17] Negative feature vectors at date 06/19/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/19/2017 13:35 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/19/2017 09:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/19/2017 18:35 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/19/2017 16:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/19/2017 02:35 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[12/17] Negative feature vectors at date 06/20/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/20/2017 04:00 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/20/2017 04:15 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/20/2017 13:15 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/20/2017 20:35 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/20/2017 18:15 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[13/17] Negative feature vectors at date 06/22/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/22/2017 12:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/22/2017 01:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/22/2017 06:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/22/2017 02:50 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/22/2017 19:55 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[14/17] Negative feature vectors at date 06/26/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/26/2017 17:15 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/26/2017 04:25 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/26/2017 04:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/26/2017 01:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/26/2017 04:30 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[15/17] Negative feature vectors at date 06/27/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/27/2017 22:10 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/27/2017 05:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/27/2017 07:50 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/27/2017 22:10 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/27/2017 15:10 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[16/17] Negative feature vectors at date 06/29/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/29/2017 14:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/29/2017 23:35 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/29/2017 09:45 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/29/2017 08:25 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/29/2017 07:25 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n",
      "[17/17] Negative feature vectors at date 06/30/2017:\n",
      "    [20/101] Start constructing feature vectors for road segment s_402290,402292...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/30/2017 21:20 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_402290,402292.\n",
      "    [40/101] Start constructing feature vectors for road segment s_400137,400716...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/30/2017 02:00 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400137,400716.\n",
      "    [60/101] Start constructing feature vectors for road segment s_412637,417666...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/30/2017 04:05 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_412637,417666.\n",
      "    [80/101] Start constructing feature vectors for road segment s_410363,400360...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/30/2017 02:10 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_410363,400360.\n",
      "    [100/101] Start constructing feature vectors for road segment s_400923,401143...\n",
      "        Total number of vectors: 24\n",
      "        Feature vector at date and time 06/30/2017 01:50 is done.\n",
      "    ...Completed construction of feature vectors for road segment s_400923,401143.\n"
     ]
    }
   ],
   "source": [
    "X_neg_train = []\n",
    "num_segments = len(road_segments)\n",
    "count_date = 0\n",
    "for neg_sample_date in neg_sample_dates:\n",
    "    count_date += 1\n",
    "    print(\"{} Negative feature vectors at date {}:\".format(fraction_msg(count_date, len(neg_sample_dates)), neg_sample_date))\n",
    "    \n",
    "    for i, seg in enumerate(road_segments):\n",
    "        B, E = seg\n",
    "        df_neg_train_BE = raw_train.loc[((raw_train[\"Station ID\"] == B) | (raw_train[\"Station ID\"] == E)) & (raw_train[\"Date\"] == neg_sample_date)]\n",
    "        svm_incidents_sample_BE = svm_incidents_sample.loc[svm_incidents_sample['Upstream'] == B]\n",
    "        sample_neg_times = np.random.choice(neg_times, 24)\n",
    "        \n",
    "        if (i+1) % 20 == 0:\n",
    "            print(\"    {} Start constructing feature vectors for road segment s_{},{}...\".format(fraction_msg(i+1, num_segments), B, E))\n",
    "            print(\"        Total number of vectors: {}\".format(len(sample_neg_times)))\n",
    "        \n",
    "        for neg_t in sample_neg_times:\n",
    "            # check if current time is incident time\n",
    "            if len(svm_incidents_sample_BE.loc[svm_incidents_sample_BE['Time'] == neg_t].index) != 0:\n",
    "                continue\n",
    "\n",
    "            feature_t = []\n",
    "            neg_dt_timestamp = pd.Timestamp(neg_sample_date + ' ' + neg_t + ':00')\n",
    "\n",
    "            B_lags = []\n",
    "            for j in range(5):\n",
    "                B_lags.append(neg_dt_timestamp - dt.timedelta(minutes=j*5))\n",
    "            B_lags = list(map(lambda x: x.strftime('%H:%M') , B_lags))\n",
    "            E_lags = B_lags[0:3]\n",
    "\n",
    "            # upstream features\n",
    "            for t_lag in B_lags:\n",
    "                df_dt_lag = df_neg_train_BE.loc[(df_neg_train_BE[\"Station ID\"] == B) & (df_neg_train_BE[\"Time\"] == t_lag)]\n",
    "\n",
    "                speed_B_t = df_dt_lag[\"Speed\"].values[0]\n",
    "                flow_B_t = df_dt_lag[\"Flow\"].values[0]\n",
    "                occ_B_t = df_dt_lag[\"Occupancy\"].values[0]\n",
    "\n",
    "                speed_pred_B_t = df_dt_lag[\"Pred_Speed\"].values[0]\n",
    "                flow_pred_B_t = df_dt_lag[\"Pred_Flow\"].values[0]\n",
    "                occ_pred_B_t = df_dt_lag[\"Pred_Occupancy\"].values[0]\n",
    " \n",
    "                speed_diff_B_t = df_dt_lag[\"Diff_Speed\"].values[0]\n",
    "                flow_diff_B_t = df_dt_lag[\"Diff_Flow\"].values[0]\n",
    "                occ_diff_B_t = df_dt_lag[\"Diff_Occupancy\"].values[0]\n",
    "            \n",
    "                lambda_max_B_t = df_dt_lag[\"LambdaMax\"].values[0]\n",
    "                sigma_B_t = df_dt_lag[\"Sigma\"].values[0]\n",
    "                tau_B_t = df_dt_lag[\"Tau\"].values[0]\n",
    "                impact_B_t = df_dt_lag[\"Impact\"].values[0]\n",
    "\n",
    "                weekday_B_t = df_dt_lag[\"weekday\"].values[0]\n",
    "                \n",
    "                feature_t.extend([speed_B_t, flow_B_t, occ_B_t, \n",
    "                                  speed_pred_B_t, flow_pred_B_t, occ_pred_B_t, \n",
    "                                  speed_diff_B_t, flow_diff_B_t, occ_diff_B_t, \n",
    "                                  lambda_max_B_t, sigma_B_t, tau_B_t, impact_B_t,\n",
    "                                  weekday_B_t])\n",
    "\n",
    "            # downstream features\n",
    "            for t_lag in E_lags:\n",
    "                df_dt_lag = df_neg_train_BE.loc[(df_neg_train_BE[\"Station ID\"] == E) & (df_neg_train_BE[\"Time\"] == t_lag)]\n",
    "\n",
    "                speed_E_t = df_dt_lag[\"Speed\"].values[0]\n",
    "                flow_E_t = df_dt_lag[\"Flow\"].values[0]\n",
    "                occ_E_t = df_dt_lag[\"Occupancy\"].values[0]\n",
    "\n",
    "                speed_pred_E_t = df_dt_lag[\"Pred_Speed\"].values[0]\n",
    "                flow_pred_E_t = df_dt_lag[\"Pred_Flow\"].values[0]\n",
    "                occ_pred_E_t = df_dt_lag[\"Pred_Occupancy\"].values[0]\n",
    "\n",
    "                speed_diff_E_t = df_dt_lag[\"Diff_Speed\"].values[0]\n",
    "                flow_diff_E_t = df_dt_lag[\"Diff_Flow\"].values[0]\n",
    "                occ_diff_E_t = df_dt_lag[\"Diff_Occupancy\"].values[0]\n",
    "                \n",
    "                lambda_max_E_t = df_dt_lag[\"LambdaMax\"].values[0]\n",
    "                sigma_E_t = df_dt_lag[\"Sigma\"].values[0]\n",
    "                tau_E_t = df_dt_lag[\"Tau\"].values[0]\n",
    "                impact_E_t = df_dt_lag[\"Impact\"].values[0]\n",
    "                \n",
    "                weekday_E_t = df_dt_lag[\"weekday\"].values[0]\n",
    "                \n",
    "                feature_t.extend([speed_E_t, flow_E_t, occ_E_t, \n",
    "                                  speed_pred_E_t, flow_pred_E_t, occ_pred_E_t, \n",
    "                                  speed_diff_E_t, flow_diff_E_t, occ_diff_E_t, \n",
    "                                  lambda_max_E_t, sigma_E_t, tau_E_t, impact_E_t, \n",
    "                                  weekday_E_t])\n",
    "            \n",
    "            X_neg_train.append(feature_t)\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            print(\"        Feature vector at date and time {} {} is done.\".format(neg_sample_date, neg_t))\n",
    "            print(\"    ...Completed construction of feature vectors for road segment s_{},{}.\".format(B, E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38365"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_neg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_neg_train = [-1] * len(X_neg_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train: feature vectors - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_time = raw_train['Time'].unique().tolist()[14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pos_timestamps_train = svm_pos_timestamps_train.loc[svm_pos_timestamps_train['Time'].isin(working_time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/101] Start constructing positive feature vectors for road segment s_408907,400951... \n",
      "    Total number of vectors: 0\n",
      "[2/101] Start constructing positive feature vectors for road segment s_400951,400057... \n",
      "    Total number of vectors: 37\n",
      "[3/101] Start constructing positive feature vectors for road segment s_400057,400147... \n",
      "    Total number of vectors: 42\n",
      "[4/101] Start constructing positive feature vectors for road segment s_400147,400343... \n",
      "    Total number of vectors: 38\n",
      "[5/101] Start constructing positive feature vectors for road segment s_400343,401560... \n",
      "    Total number of vectors: 24\n",
      "[6/101] Start constructing positive feature vectors for road segment s_401560,400045... \n",
      "    Total number of vectors: 14\n",
      "[7/101] Start constructing positive feature vectors for road segment s_400045,400122... \n",
      "    Total number of vectors: 13\n",
      "[8/101] Start constructing positive feature vectors for road segment s_400122,401541... \n",
      "    Total number of vectors: 5\n",
      "[9/101] Start constructing positive feature vectors for road segment s_401541,402281... \n",
      "    Total number of vectors: 38\n",
      "[10/101] Start constructing positive feature vectors for road segment s_402281,402283... \n",
      "    Total number of vectors: 0\n",
      "[11/101] Start constructing positive feature vectors for road segment s_402283,402285... \n",
      "    Total number of vectors: 45\n",
      "[12/101] Start constructing positive feature vectors for road segment s_402285,402286... \n",
      "    Total number of vectors: 0\n",
      "[13/101] Start constructing positive feature vectors for road segment s_402286,400088... \n",
      "    Total number of vectors: 11\n",
      "[14/101] Start constructing positive feature vectors for road segment s_400088,402288... \n",
      "    Total number of vectors: 0\n",
      "[15/101] Start constructing positive feature vectors for road segment s_402288,413026... \n",
      "    Total number of vectors: 9\n",
      "[16/101] Start constructing positive feature vectors for road segment s_413026,401464... \n",
      "    Total number of vectors: 0\n",
      "[17/101] Start constructing positive feature vectors for road segment s_401464,401489... \n",
      "    Total number of vectors: 0\n",
      "[18/101] Start constructing positive feature vectors for road segment s_401489,401538... \n",
      "    Total number of vectors: 3\n",
      "[19/101] Start constructing positive feature vectors for road segment s_401538,402290... \n",
      "    Total number of vectors: 8\n",
      "[20/101] Start constructing positive feature vectors for road segment s_402290,402292... \n",
      "    Total number of vectors: 0\n",
      "[21/101] Start constructing positive feature vectors for road segment s_402292,401643... \n",
      "    Total number of vectors: 81\n",
      "[22/101] Start constructing positive feature vectors for road segment s_401643,402800... \n",
      "    Total number of vectors: 4\n",
      "[23/101] Start constructing positive feature vectors for road segment s_402800,402828... \n",
      "    Total number of vectors: 0\n",
      "[24/101] Start constructing positive feature vectors for road segment s_402828,407219... \n",
      "    Total number of vectors: 34\n",
      "[25/101] Start constructing positive feature vectors for road segment s_407219,402789... \n",
      "    Total number of vectors: 0\n",
      "[26/101] Start constructing positive feature vectors for road segment s_402789,408755... \n",
      "    Total number of vectors: 14\n",
      "[27/101] Start constructing positive feature vectors for road segment s_408755,402802... \n",
      "    Total number of vectors: 0\n",
      "[28/101] Start constructing positive feature vectors for road segment s_402802,408756... \n",
      "    Total number of vectors: 0\n",
      "[29/101] Start constructing positive feature vectors for road segment s_408756,400189... \n",
      "    Total number of vectors: 94\n",
      "[30/101] Start constructing positive feature vectors for road segment s_400189,400309... \n",
      "    Total number of vectors: 14\n",
      "[31/101] Start constructing positive feature vectors for road segment s_400309,400417... \n",
      "    Total number of vectors: 9\n",
      "[32/101] Start constructing positive feature vectors for road segment s_400417,400249... \n",
      "    Total number of vectors: 23\n",
      "[33/101] Start constructing positive feature vectors for road segment s_400249,401639... \n",
      "    Total number of vectors: 27\n",
      "[34/101] Start constructing positive feature vectors for road segment s_401639,400662... \n",
      "    Total number of vectors: 8\n",
      "[35/101] Start constructing positive feature vectors for road segment s_400662,400141... \n",
      "    Total number of vectors: 0\n",
      "[36/101] Start constructing positive feature vectors for road segment s_400141,400761... \n",
      "    Total number of vectors: 0\n",
      "[37/101] Start constructing positive feature vectors for road segment s_400761,400490... \n",
      "    Total number of vectors: 3\n",
      "[38/101] Start constructing positive feature vectors for road segment s_400490,401888... \n",
      "    Total number of vectors: 5\n",
      "[39/101] Start constructing positive feature vectors for road segment s_401888,400137... \n",
      "    Total number of vectors: 110\n",
      "    [100/110] Feature vector at date and time 06/09/2017 07:25 is done.\n",
      "[40/101] Start constructing positive feature vectors for road segment s_400137,400716... \n",
      "    Total number of vectors: 95\n",
      "[41/101] Start constructing positive feature vectors for road segment s_400716,401545... \n",
      "    Total number of vectors: 9\n",
      "[42/101] Start constructing positive feature vectors for road segment s_401545,401011... \n",
      "    Total number of vectors: 24\n",
      "[43/101] Start constructing positive feature vectors for road segment s_401011,400674... \n",
      "    Total number of vectors: 12\n",
      "[44/101] Start constructing positive feature vectors for road segment s_400674,400539... \n",
      "    Total number of vectors: 6\n",
      "[45/101] Start constructing positive feature vectors for road segment s_400539,400534... \n",
      "    Total number of vectors: 12\n",
      "[46/101] Start constructing positive feature vectors for road segment s_400534,401062... \n",
      "    Total number of vectors: 10\n",
      "[47/101] Start constructing positive feature vectors for road segment s_401062,401529... \n",
      "    Total number of vectors: 0\n",
      "[48/101] Start constructing positive feature vectors for road segment s_401529,401613... \n",
      "    Total number of vectors: 0\n",
      "[49/101] Start constructing positive feature vectors for road segment s_401613,400536... \n",
      "    Total number of vectors: 9\n",
      "[50/101] Start constructing positive feature vectors for road segment s_400536,400488... \n",
      "    Total number of vectors: 75\n",
      "[51/101] Start constructing positive feature vectors for road segment s_400488,401561... \n",
      "    Total number of vectors: 100\n",
      "    [100/100] Feature vector at date and time 06/27/2017 22:40 is done.\n",
      "[52/101] Start constructing positive feature vectors for road segment s_401561,400611... \n",
      "    Total number of vectors: 0\n",
      "[53/101] Start constructing positive feature vectors for road segment s_400611,400928... \n",
      "    Total number of vectors: 7\n",
      "[54/101] Start constructing positive feature vectors for road segment s_400928,400284... \n",
      "    Total number of vectors: 73\n",
      "[55/101] Start constructing positive feature vectors for road segment s_400284,400041... \n",
      "    Total number of vectors: 0\n",
      "[56/101] Start constructing positive feature vectors for road segment s_400041,408133... \n",
      "    Total number of vectors: 0\n",
      "[57/101] Start constructing positive feature vectors for road segment s_408133,408135... \n",
      "    Total number of vectors: 0\n",
      "[58/101] Start constructing positive feature vectors for road segment s_408135,417665... \n",
      "    Total number of vectors: 4\n",
      "[59/101] Start constructing positive feature vectors for road segment s_417665,412637... \n",
      "    Total number of vectors: 0\n",
      "[60/101] Start constructing positive feature vectors for road segment s_412637,417666... \n",
      "    Total number of vectors: 7\n",
      "[61/101] Start constructing positive feature vectors for road segment s_417666,408134... \n",
      "    Total number of vectors: 11\n",
      "[62/101] Start constructing positive feature vectors for road segment s_408134,400685... \n",
      "    Total number of vectors: 25\n",
      "[63/101] Start constructing positive feature vectors for road segment s_400685,401003... \n",
      "    Total number of vectors: 0\n",
      "[64/101] Start constructing positive feature vectors for road segment s_401003,400898... \n",
      "    Total number of vectors: 41\n",
      "[65/101] Start constructing positive feature vectors for road segment s_400898,400275... \n",
      "    Total number of vectors: 15\n",
      "[66/101] Start constructing positive feature vectors for road segment s_400275,400939... \n",
      "    Total number of vectors: 9\n",
      "[67/101] Start constructing positive feature vectors for road segment s_400939,400180... \n",
      "    Total number of vectors: 0\n",
      "[68/101] Start constructing positive feature vectors for road segment s_400180,400529... \n",
      "    Total number of vectors: 0\n",
      "[69/101] Start constructing positive feature vectors for road segment s_400529,400990... \n",
      "    Total number of vectors: 0\n",
      "[70/101] Start constructing positive feature vectors for road segment s_400990,400515... \n",
      "    Total number of vectors: 0\n",
      "[71/101] Start constructing positive feature vectors for road segment s_400515,400252... \n",
      "    Total number of vectors: 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72/101] Start constructing positive feature vectors for road segment s_400252,400788... \n",
      "    Total number of vectors: 26\n",
      "[73/101] Start constructing positive feature vectors for road segment s_400788,401517... \n",
      "    Total number of vectors: 56\n",
      "[74/101] Start constructing positive feature vectors for road segment s_401517,401871... \n",
      "    Total number of vectors: 0\n",
      "[75/101] Start constructing positive feature vectors for road segment s_401871,400574... \n",
      "    Total number of vectors: 29\n",
      "[76/101] Start constructing positive feature vectors for road segment s_400574,401629... \n",
      "    Total number of vectors: 20\n",
      "[77/101] Start constructing positive feature vectors for road segment s_401629,400422... \n",
      "    Total number of vectors: 44\n",
      "[78/101] Start constructing positive feature vectors for road segment s_400422,400333... \n",
      "    Total number of vectors: 0\n",
      "[79/101] Start constructing positive feature vectors for road segment s_400333,410363... \n",
      "    Total number of vectors: 18\n",
      "[80/101] Start constructing positive feature vectors for road segment s_410363,400360... \n",
      "    Total number of vectors: 64\n",
      "[81/101] Start constructing positive feature vectors for road segment s_400360,400955... \n",
      "    Total number of vectors: 33\n",
      "[82/101] Start constructing positive feature vectors for road segment s_400955,400495... \n",
      "    Total number of vectors: 54\n",
      "[83/101] Start constructing positive feature vectors for road segment s_400495,400608... \n",
      "    Total number of vectors: 0\n",
      "[84/101] Start constructing positive feature vectors for road segment s_400608,400949... \n",
      "    Total number of vectors: 78\n",
      "[85/101] Start constructing positive feature vectors for road segment s_400949,400678... \n",
      "    Total number of vectors: 159\n",
      "    [100/159] Feature vector at date and time 06/27/2017 18:55 is done.\n",
      "[86/101] Start constructing positive feature vectors for road segment s_400678,400341... \n",
      "    Total number of vectors: 0\n",
      "[87/101] Start constructing positive feature vectors for road segment s_400341,400607... \n",
      "    Total number of vectors: 15\n",
      "[88/101] Start constructing positive feature vectors for road segment s_400607,400094... \n",
      "    Total number of vectors: 22\n",
      "[89/101] Start constructing positive feature vectors for road segment s_400094,400682... \n",
      "    Total number of vectors: 0\n",
      "[90/101] Start constructing positive feature vectors for road segment s_400682,408138... \n",
      "    Total number of vectors: 37\n",
      "[91/101] Start constructing positive feature vectors for road segment s_408138,400980... \n",
      "    Total number of vectors: 5\n",
      "[92/101] Start constructing positive feature vectors for road segment s_400980,401333... \n",
      "    Total number of vectors: 74\n",
      "[93/101] Start constructing positive feature vectors for road segment s_401333,404746... \n",
      "    Total number of vectors: 0\n",
      "[94/101] Start constructing positive feature vectors for road segment s_404746,401142... \n",
      "    Total number of vectors: 0\n",
      "[95/101] Start constructing positive feature vectors for road segment s_401142,400218... \n",
      "    Total number of vectors: 7\n",
      "[96/101] Start constructing positive feature vectors for road segment s_400218,400983... \n",
      "    Total number of vectors: 3\n",
      "[97/101] Start constructing positive feature vectors for road segment s_400983,400765... \n",
      "    Total number of vectors: 0\n",
      "[98/101] Start constructing positive feature vectors for road segment s_400765,400844... \n",
      "    Total number of vectors: 13\n",
      "[99/101] Start constructing positive feature vectors for road segment s_400844,400923... \n",
      "    Total number of vectors: 0\n",
      "[100/101] Start constructing positive feature vectors for road segment s_400923,401143... \n",
      "    Total number of vectors: 0\n",
      "[101/101] Start constructing positive feature vectors for road segment s_401143,401471... \n",
      "    Total number of vectors: 0\n",
      "...Completed construction of feature vectors for road segment s_401143,401471.\n"
     ]
    }
   ],
   "source": [
    "X_pos_train = []\n",
    "for i, seg in enumerate(road_segments):\n",
    "    B, E = seg\n",
    "    print(\"{} Start constructing positive feature vectors for road segment s_{},{}... \".format(fraction_msg(i+1, len(road_segments)), B, E))\n",
    "    progress_count = 0\n",
    "    \n",
    "    # construct segment-specific pos_times\n",
    "    pos_times = []\n",
    "    df_seg_incidents = svm_pos_timestamps_train.loc[svm_pos_timestamps_train[\"Upstream\"] == B]\n",
    "    seg_dates = df_seg_incidents['Date'].values.tolist()\n",
    "    seg_times = df_seg_incidents['Time'].values.tolist()\n",
    "    num_seg_instances = len(seg_dates)\n",
    "    for i in range(num_seg_instances):\n",
    "        pos_times.append(tuple([seg_dates[i], seg_times[i]]))\n",
    "    \n",
    "    # select the relevant training data for segment B, E \n",
    "    df_train_BE = raw_train.loc[(raw_train[\"Station ID\"] == B) | (raw_train[\"Station ID\"] == E)]\n",
    "    \n",
    "    \n",
    "    print(\"    Total number of vectors: {}\".format(num_seg_instances))\n",
    "    for pos_dt in pos_times:\n",
    "        pos_d, pos_t = pos_dt\n",
    "        feature_t = []\n",
    "        pos_dt_timestamp = pd.Timestamp(pos_d + ' ' + pos_t + ':00')\n",
    "\n",
    "        # upstream and downstream time lags\n",
    "        B_lags = []\n",
    "        for j in range(5):\n",
    "            B_lags.append(pos_dt_timestamp - dt.timedelta(minutes=j*5))\n",
    "        B_lags = list(map(lambda x: (x.strftime('%m/%d/%Y'), x.strftime('%H:%M')) , B_lags))\n",
    "        E_lags = B_lags[0:3]\n",
    "\n",
    "        # upstream features\n",
    "        for dt_lag in B_lags:\n",
    "            d_lag, t_lag = dt_lag\n",
    "            df_dt_lag = df_train_BE.loc[(df_train_BE[\"Station ID\"] == B) & (df_train_BE[\"Date\"] == d_lag) & (df_train_BE[\"Time\"] == t_lag)]\n",
    "            if df_dt_lag.empty:\n",
    "                print(d_lag, t_lag)\n",
    "            \n",
    "            speed_B_t = df_dt_lag[\"Speed\"].values[0]\n",
    "            flow_B_t = df_dt_lag[\"Flow\"].values[0]\n",
    "            occ_B_t = df_dt_lag[\"Occupancy\"].values[0]\n",
    "\n",
    "            speed_pred_B_t = df_dt_lag[\"Pred_Speed\"].values[0]\n",
    "            flow_pred_B_t = df_dt_lag[\"Pred_Flow\"].values[0]\n",
    "            occ_pred_B_t = df_dt_lag[\"Pred_Occupancy\"].values[0]\n",
    "            \n",
    "            speed_diff_B_t = df_dt_lag[\"Diff_Speed\"].values[0]\n",
    "            flow_diff_B_t = df_dt_lag[\"Diff_Flow\"].values[0]\n",
    "            occ_diff_B_t = df_dt_lag[\"Diff_Occupancy\"].values[0]\n",
    "            \n",
    "            lambda_max_B_t = df_dt_lag[\"LambdaMax\"].values[0]\n",
    "            sigma_B_t = df_dt_lag[\"Sigma\"].values[0]\n",
    "            tau_B_t = df_dt_lag[\"Tau\"].values[0]\n",
    "            impact_B_t = df_dt_lag[\"Impact\"].values[0]\n",
    "            \n",
    "            weekday_B_t = df_dt_lag[\"weekday\"].values[0]\n",
    "\n",
    "            feature_t.extend([speed_B_t, flow_B_t, occ_B_t, \n",
    "                              speed_pred_B_t, flow_pred_B_t, occ_pred_B_t, \n",
    "                              speed_diff_B_t, flow_diff_B_t, occ_diff_B_t, \n",
    "                              lambda_max_B_t, sigma_B_t, tau_B_t, impact_B_t,\n",
    "                              weekday_B_t])\n",
    "\n",
    "        # downstream features\n",
    "        for dt_lag in E_lags:\n",
    "            d_lag, t_lag = dt_lag\n",
    "            df_dt_lag = df_train_BE.loc[(df_train_BE[\"Station ID\"] == E) & (df_train_BE[\"Date\"] == d_lag) & (df_train_BE[\"Time\"] == t_lag)]\n",
    "\n",
    "            speed_E_t = df_dt_lag[\"Speed\"].values[0]\n",
    "            flow_E_t = df_dt_lag[\"Flow\"].values[0]\n",
    "            occ_E_t = df_dt_lag[\"Occupancy\"].values[0]\n",
    "\n",
    "            speed_pred_E_t = df_dt_lag[\"Pred_Speed\"].values[0]\n",
    "            flow_pred_E_t = df_dt_lag[\"Pred_Flow\"].values[0]\n",
    "            occ_pred_E_t = df_dt_lag[\"Pred_Occupancy\"].values[0]\n",
    "            \n",
    "            speed_diff_E_t = df_dt_lag[\"Diff_Speed\"].values[0]\n",
    "            flow_diff_E_t = df_dt_lag[\"Diff_Flow\"].values[0]\n",
    "            occ_diff_E_t = df_dt_lag[\"Diff_Occupancy\"].values[0]\n",
    "            \n",
    "            lambda_max_E_t = df_dt_lag[\"LambdaMax\"].values[0]\n",
    "            sigma_E_t = df_dt_lag[\"Sigma\"].values[0]\n",
    "            tau_E_t = df_dt_lag[\"Tau\"].values[0]\n",
    "            impact_E_t = df_dt_lag[\"Impact\"].values[0]\n",
    "            \n",
    "            weekday_E_t = df_dt_lag[\"weekday\"].values[0]\n",
    "            \n",
    "            feature_t.extend([speed_E_t, flow_E_t, occ_E_t, \n",
    "                              speed_pred_E_t, flow_pred_E_t, occ_pred_E_t, \n",
    "                              speed_diff_E_t, flow_diff_E_t, occ_diff_E_t,  \n",
    "                              lambda_max_E_t, sigma_E_t, tau_E_t, impact_E_t,\n",
    "                              weekday_E_t,])\n",
    "        \n",
    "        X_pos_train.append(feature_t)\n",
    "        \n",
    "        progress_count += 1\n",
    "        if progress_count % 100 == 0:\n",
    "            print(\"    {} Feature vector at date and time {} {} is done.\".format(fraction_msg(progress_count, num_seg_instances), pos_d, pos_t))\n",
    "\n",
    "print(\"...Completed construction of feature vectors for road segment s_{},{}.\".format(B, E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2065"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_pos_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos_train = [1] * len(X_pos_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train: Merging feature vectors together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_neg_train = np.array(X_neg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_neg_train_balanced = X_neg_train[np.random.choice(len(X_neg_train), len(X_pos_train), replace=False)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_neg_train_balanced = [-1] * len(X_neg_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_neg_train_balanced + X_pos_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_neg_train_balanced + y_pos_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4130, 4130)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station ID</th>\n",
       "      <th>datetime</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>Flow</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>idx</th>\n",
       "      <th>LambdaMax</th>\n",
       "      <th>Sigma</th>\n",
       "      <th>Tau</th>\n",
       "      <th>Impact</th>\n",
       "      <th>Pred_Speed</th>\n",
       "      <th>Pred_Flow</th>\n",
       "      <th>Pred_Occupancy</th>\n",
       "      <th>Diff_Speed</th>\n",
       "      <th>Diff_Flow</th>\n",
       "      <th>Diff_Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6532125</th>\n",
       "      <td>401471</td>\n",
       "      <td>2017-06-28 23:45:00</td>\n",
       "      <td>-1.146233</td>\n",
       "      <td>-1.207703</td>\n",
       "      <td>0.851742</td>\n",
       "      <td>06/28/2017</td>\n",
       "      <td>23:45</td>\n",
       "      <td>3201453</td>\n",
       "      <td>0.038882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064840</td>\n",
       "      <td>0.273968</td>\n",
       "      <td>-1.181695</td>\n",
       "      <td>-1.144224</td>\n",
       "      <td>-0.025454</td>\n",
       "      <td>-0.307671</td>\n",
       "      <td>0.085939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6532126</th>\n",
       "      <td>401471</td>\n",
       "      <td>2017-06-28 23:50:00</td>\n",
       "      <td>-1.213813</td>\n",
       "      <td>-1.207703</td>\n",
       "      <td>0.752468</td>\n",
       "      <td>06/28/2017</td>\n",
       "      <td>23:50</td>\n",
       "      <td>3201454</td>\n",
       "      <td>0.038882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064408</td>\n",
       "      <td>0.294311</td>\n",
       "      <td>-1.220799</td>\n",
       "      <td>-1.157647</td>\n",
       "      <td>-0.076512</td>\n",
       "      <td>0.319242</td>\n",
       "      <td>-0.697778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6532127</th>\n",
       "      <td>401471</td>\n",
       "      <td>2017-06-28 23:55:00</td>\n",
       "      <td>-1.213813</td>\n",
       "      <td>-1.282610</td>\n",
       "      <td>0.653193</td>\n",
       "      <td>06/28/2017</td>\n",
       "      <td>23:55</td>\n",
       "      <td>3201455</td>\n",
       "      <td>0.038882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063334</td>\n",
       "      <td>0.279781</td>\n",
       "      <td>-1.224709</td>\n",
       "      <td>-1.218050</td>\n",
       "      <td>-0.091529</td>\n",
       "      <td>-0.812187</td>\n",
       "      <td>0.183904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Station ID             datetime  Occupancy      Flow     Speed  \\\n",
       "6532125      401471  2017-06-28 23:45:00  -1.146233 -1.207703  0.851742   \n",
       "6532126      401471  2017-06-28 23:50:00  -1.213813 -1.207703  0.752468   \n",
       "6532127      401471  2017-06-28 23:55:00  -1.213813 -1.282610  0.653193   \n",
       "\n",
       "               Date   Time      idx  LambdaMax  Sigma  Tau    Impact  \\\n",
       "6532125  06/28/2017  23:45  3201453   0.038882    0.0  0.0  0.064840   \n",
       "6532126  06/28/2017  23:50  3201454   0.038882    0.0  0.0  0.064408   \n",
       "6532127  06/28/2017  23:55  3201455   0.038882    0.0  0.0  0.063334   \n",
       "\n",
       "         Pred_Speed  Pred_Flow  Pred_Occupancy  Diff_Speed  Diff_Flow  \\\n",
       "6532125    0.273968  -1.181695       -1.144224   -0.025454  -0.307671   \n",
       "6532126    0.294311  -1.220799       -1.157647   -0.076512   0.319242   \n",
       "6532127    0.279781  -1.224709       -1.218050   -0.091529  -0.812187   \n",
       "\n",
       "         Diff_Occupancy  \n",
       "6532125        0.085939  \n",
       "6532126       -0.697778  \n",
       "6532127        0.183904  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_dates = raw_test['Date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_weekday = [is_weekday(date_to_day(date)) for date in raw_test_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test['weekday'] = raw_test_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Speed', 'Flow', 'Occupancy', \n",
    "                 'Pred_Speed', 'Pred_Flow', 'Pred_Occupancy', \n",
    "                 'Diff_Speed', 'Diff_Flow', 'Diff_Occupancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names.extend(['LambdaMax', 'Sigma', 'Tau', 'Impact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names.extend(['weekday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(feature_names)\n",
    "k_B = 4\n",
    "k_E = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/101] Constructing feature vector for segment s_408907,400951...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_408907,400951.\n",
      "[2/101] Constructing feature vector for segment s_400951,400057...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400951,400057.\n",
      "[3/101] Constructing feature vector for segment s_400057,400147...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400057,400147.\n",
      "[4/101] Constructing feature vector for segment s_400147,400343...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400147,400343.\n",
      "[5/101] Constructing feature vector for segment s_400343,401560...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400343,401560.\n",
      "[6/101] Constructing feature vector for segment s_401560,400045...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401560,400045.\n",
      "[7/101] Constructing feature vector for segment s_400045,400122...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400045,400122.\n",
      "[8/101] Constructing feature vector for segment s_400122,401541...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400122,401541.\n",
      "[9/101] Constructing feature vector for segment s_401541,402281...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401541,402281.\n",
      "[10/101] Constructing feature vector for segment s_402281,402283...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402281,402283.\n",
      "[11/101] Constructing feature vector for segment s_402283,402285...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402283,402285.\n",
      "[12/101] Constructing feature vector for segment s_402285,402286...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402285,402286.\n",
      "[13/101] Constructing feature vector for segment s_402286,400088...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402286,400088.\n",
      "[14/101] Constructing feature vector for segment s_400088,402288...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400088,402288.\n",
      "[15/101] Constructing feature vector for segment s_402288,413026...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402288,413026.\n",
      "[16/101] Constructing feature vector for segment s_413026,401464...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_413026,401464.\n",
      "[17/101] Constructing feature vector for segment s_401464,401489...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401464,401489.\n",
      "[18/101] Constructing feature vector for segment s_401489,401538...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401489,401538.\n",
      "[19/101] Constructing feature vector for segment s_401538,402290...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401538,402290.\n",
      "[20/101] Constructing feature vector for segment s_402290,402292...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402290,402292.\n",
      "[21/101] Constructing feature vector for segment s_402292,401643...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402292,401643.\n",
      "[22/101] Constructing feature vector for segment s_401643,402800...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401643,402800.\n",
      "[23/101] Constructing feature vector for segment s_402800,402828...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402800,402828.\n",
      "[24/101] Constructing feature vector for segment s_402828,407219...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402828,407219.\n",
      "[25/101] Constructing feature vector for segment s_407219,402789...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_407219,402789.\n",
      "[26/101] Constructing feature vector for segment s_402789,408755...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402789,408755.\n",
      "[27/101] Constructing feature vector for segment s_408755,402802...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_408755,402802.\n",
      "[28/101] Constructing feature vector for segment s_402802,408756...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_402802,408756.\n",
      "[29/101] Constructing feature vector for segment s_408756,400189...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_408756,400189.\n",
      "[30/101] Constructing feature vector for segment s_400189,400309...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400189,400309.\n",
      "[31/101] Constructing feature vector for segment s_400309,400417...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400309,400417.\n",
      "[32/101] Constructing feature vector for segment s_400417,400249...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400417,400249.\n",
      "[33/101] Constructing feature vector for segment s_400249,401639...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400249,401639.\n",
      "[34/101] Constructing feature vector for segment s_401639,400662...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401639,400662.\n",
      "[35/101] Constructing feature vector for segment s_400662,400141...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400662,400141.\n",
      "[36/101] Constructing feature vector for segment s_400141,400761...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400141,400761.\n",
      "[37/101] Constructing feature vector for segment s_400761,400490...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400761,400490.\n",
      "[38/101] Constructing feature vector for segment s_400490,401888...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400490,401888.\n",
      "[39/101] Constructing feature vector for segment s_401888,400137...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401888,400137.\n",
      "[40/101] Constructing feature vector for segment s_400137,400716...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400137,400716.\n",
      "[41/101] Constructing feature vector for segment s_400716,401545...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400716,401545.\n",
      "[42/101] Constructing feature vector for segment s_401545,401011...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401545,401011.\n",
      "[43/101] Constructing feature vector for segment s_401011,400674...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401011,400674.\n",
      "[44/101] Constructing feature vector for segment s_400674,400539...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400674,400539.\n",
      "[45/101] Constructing feature vector for segment s_400539,400534...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400539,400534.\n",
      "[46/101] Constructing feature vector for segment s_400534,401062...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400534,401062.\n",
      "[47/101] Constructing feature vector for segment s_401062,401529...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401062,401529.\n",
      "[48/101] Constructing feature vector for segment s_401529,401613...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401529,401613.\n",
      "[49/101] Constructing feature vector for segment s_401613,400536...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401613,400536.\n",
      "[50/101] Constructing feature vector for segment s_400536,400488...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400536,400488.\n",
      "[51/101] Constructing feature vector for segment s_400488,401561...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400488,401561.\n",
      "[52/101] Constructing feature vector for segment s_401561,400611...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401561,400611.\n",
      "[53/101] Constructing feature vector for segment s_400611,400928...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400611,400928.\n",
      "[54/101] Constructing feature vector for segment s_400928,400284...\n",
      "    Total number of instances: 3288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Finished construction for segment s_400928,400284.\n",
      "[55/101] Constructing feature vector for segment s_400284,400041...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400284,400041.\n",
      "[56/101] Constructing feature vector for segment s_400041,408133...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400041,408133.\n",
      "[57/101] Constructing feature vector for segment s_408133,408135...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_408133,408135.\n",
      "[58/101] Constructing feature vector for segment s_408135,417665...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_408135,417665.\n",
      "[59/101] Constructing feature vector for segment s_417665,412637...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_417665,412637.\n",
      "[60/101] Constructing feature vector for segment s_412637,417666...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_412637,417666.\n",
      "[61/101] Constructing feature vector for segment s_417666,408134...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_417666,408134.\n",
      "[62/101] Constructing feature vector for segment s_408134,400685...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_408134,400685.\n",
      "[63/101] Constructing feature vector for segment s_400685,401003...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400685,401003.\n",
      "[64/101] Constructing feature vector for segment s_401003,400898...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401003,400898.\n",
      "[65/101] Constructing feature vector for segment s_400898,400275...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400898,400275.\n",
      "[66/101] Constructing feature vector for segment s_400275,400939...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400275,400939.\n",
      "[67/101] Constructing feature vector for segment s_400939,400180...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400939,400180.\n",
      "[68/101] Constructing feature vector for segment s_400180,400529...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400180,400529.\n",
      "[69/101] Constructing feature vector for segment s_400529,400990...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400529,400990.\n",
      "[70/101] Constructing feature vector for segment s_400990,400515...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400990,400515.\n",
      "[71/101] Constructing feature vector for segment s_400515,400252...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400515,400252.\n",
      "[72/101] Constructing feature vector for segment s_400252,400788...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400252,400788.\n",
      "[73/101] Constructing feature vector for segment s_400788,401517...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400788,401517.\n",
      "[74/101] Constructing feature vector for segment s_401517,401871...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401517,401871.\n",
      "[75/101] Constructing feature vector for segment s_401871,400574...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401871,400574.\n",
      "[76/101] Constructing feature vector for segment s_400574,401629...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400574,401629.\n",
      "[77/101] Constructing feature vector for segment s_401629,400422...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401629,400422.\n",
      "[78/101] Constructing feature vector for segment s_400422,400333...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400422,400333.\n",
      "[79/101] Constructing feature vector for segment s_400333,410363...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400333,410363.\n",
      "[80/101] Constructing feature vector for segment s_410363,400360...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_410363,400360.\n",
      "[81/101] Constructing feature vector for segment s_400360,400955...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400360,400955.\n",
      "[82/101] Constructing feature vector for segment s_400955,400495...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400955,400495.\n",
      "[83/101] Constructing feature vector for segment s_400495,400608...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400495,400608.\n",
      "[84/101] Constructing feature vector for segment s_400608,400949...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400608,400949.\n",
      "[85/101] Constructing feature vector for segment s_400949,400678...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400949,400678.\n",
      "[86/101] Constructing feature vector for segment s_400678,400341...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400678,400341.\n",
      "[87/101] Constructing feature vector for segment s_400341,400607...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400341,400607.\n",
      "[88/101] Constructing feature vector for segment s_400607,400094...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400607,400094.\n",
      "[89/101] Constructing feature vector for segment s_400094,400682...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400094,400682.\n",
      "[90/101] Constructing feature vector for segment s_400682,408138...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400682,408138.\n",
      "[91/101] Constructing feature vector for segment s_408138,400980...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_408138,400980.\n",
      "[92/101] Constructing feature vector for segment s_400980,401333...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400980,401333.\n",
      "[93/101] Constructing feature vector for segment s_401333,404746...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401333,404746.\n",
      "[94/101] Constructing feature vector for segment s_404746,401142...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_404746,401142.\n",
      "[95/101] Constructing feature vector for segment s_401142,400218...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401142,400218.\n",
      "[96/101] Constructing feature vector for segment s_400218,400983...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400218,400983.\n",
      "[97/101] Constructing feature vector for segment s_400983,400765...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400983,400765.\n",
      "[98/101] Constructing feature vector for segment s_400765,400844...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400765,400844.\n",
      "[99/101] Constructing feature vector for segment s_400844,400923...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400844,400923.\n",
      "[100/101] Constructing feature vector for segment s_400923,401143...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_400923,401143.\n",
      "[101/101] Constructing feature vector for segment s_401143,401471...\n",
      "    Total number of instances: 3288\n",
      "...Finished construction for segment s_401143,401471.\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for seg_idx, seg in enumerate(road_segments):\n",
    "    B, E = seg\n",
    "    print(\"{} Constructing feature vector for segment s_{},{}...\".format(fraction_msg(seg_idx+1, len(road_segments)), B, E))\n",
    "    df_BE_test = raw_test.loc[((raw_test[\"Station ID\"] == B) | (raw_test[\"Station ID\"] == E))]\n",
    "    df_incidents_BE_test = svm_pos_timestamps_test.loc[svm_pos_timestamps_test[\"Upstream\"] == B]\n",
    "    incidents_BE_date = df_incidents_BE_test[\"Date\"].values\n",
    "    incidents_BE_time = df_incidents_BE_test[\"Time\"].values\n",
    "    \n",
    "    incidents_BE_dt = set()\n",
    "    for i in range(len(incidents_BE_date)):\n",
    "        incidents_BE_dt.add(incidents_BE_date[i] + ' ' + incidents_BE_time[i])\n",
    "    \n",
    "    # change to access by indices, to make program faster\n",
    "    features_BE_dict = dict()\n",
    "    features_BE_dict[B] = dict()\n",
    "    features_BE_dict[E] = dict()\n",
    "    for feature_name in feature_names:\n",
    "        features_BE_dict[B][feature_name] = df_BE_test.loc[df_BE_test[\"Station ID\"] == B][feature_name].values.tolist()\n",
    "        features_BE_dict[E][feature_name] = df_BE_test.loc[df_BE_test[\"Station ID\"] == E][feature_name].values.tolist()\n",
    "    \n",
    "    total_count = len(dates_test) * len(neg_times)\n",
    "    count = 0\n",
    "    print(\"    Total number of instances: {}\".format(total_count)) \n",
    "    \n",
    "    for i, d in enumerate(dates_test):\n",
    "        for j, t in enumerate(neg_times):\n",
    "            # construct vector Z(s_BE, dt)\n",
    "            feature_BE_t = [0.] * (k_B + k_E + 2) * num_features\n",
    "            base_idx = i * 288 + 14 + j\n",
    "            for k, feature_name in enumerate(feature_names):\n",
    "                # feature_k_B_t: [t-4, t-3, ..., t] -> need to be reversed and made consistent with order of SVM features. Same to E.\n",
    "                feature_k_B_t = features_BE_dict[B][feature_name][base_idx-k_B:base_idx+1]\n",
    "                feature_k_E_t = features_BE_dict[E][feature_name][base_idx-k_E:base_idx+1]\n",
    "                feature_k_B_t.reverse()\n",
    "                feature_k_E_t.reverse()\n",
    "                feature_k_BE_t = feature_k_B_t + feature_k_E_t\n",
    "                feature_BE_t[k:(k_B + k_E + 2)*num_features:num_features] = feature_k_BE_t\n",
    "            \n",
    "            X_test.append(feature_BE_t)\n",
    "            # label data\n",
    "            if d + ' ' + t in incidents_BE_dt:\n",
    "                y_test.append(1)\n",
    "            else:\n",
    "                y_test.append(-1)\n",
    "        count += 1\n",
    "        if count % 50 == 0:\n",
    "            print(\"    Progress: {}\" + fraction_msg(count * len(neg_times), total_count))\n",
    "    print(\"...Finished construction for segment s_{},{}.\".format(B, E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(332088, 332088)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = X_normalized[:len(X_train)]\n",
    "X_test_normalized = X_normalized[len(X_train):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [10 ** i for i in range(-4, 3)],\n",
    "    'gamma': [10 ** i for i in range(-4, 4, 2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid_search = GridSearchCV(SVC(kernel='rbf'), n_jobs=8, \n",
    "                               param_grid=param_grid, cv=5, \n",
    "                               scoring='accuracy', verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=8)]: Done 140 out of 140 | elapsed: 11.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=8,\n",
       "       param_grid={'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.0001, 0.01, 1, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=5)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_search.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100, 'gamma': 0.0001}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6217917675544794"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3323"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svm_grid_search.best_estimator_.support_vectors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = svm_grid_search.predict(X_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6399515738498789"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332088"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize prediction\n",
    "def predict_func(X, clf):\n",
    "    print(\"Process {} is predicting ...\".format(mp.current_process().pid))\n",
    "    return clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 30545 is predicting ...\n",
      "Process 30546 is predicting ...\n",
      "Process 30547 is predicting ...\n",
      "Process 30548 is predicting ...\n",
      "Process 30549 is predicting ...\n",
      "Process 30550 is predicting ...\n",
      "Process 30551 is predicting ...\n",
      "Process 30552 is predicting ...\n"
     ]
    }
   ],
   "source": [
    "pred_pool = mp.Pool(8)\n",
    "num_instances = int(np.ceil(len(X_test_normalized) / 8))\n",
    "y_test_pred_jobs = [pred_pool.apply_async(predict_func, args=(X_test_normalized[i*num_instances:(i+1)*num_instances], svm_grid_search)) for i in range(0, 8)]\n",
    "pred_pool.close()\n",
    "pred_pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = []\n",
    "for y_test_pred_job in y_test_pred_jobs:\n",
    "    y_test_pred.extend(y_test_pred_job.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(332088, 332088)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_normalized), len(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8671948399219483"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dt_segments = int(len(y_test_pred) / (288-14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = DecisionTreeClassifier(criterion='gini', random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gini.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_gini = clf_gini.predict(X_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train_pred_gini, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_gini = clf_gini.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6734118667341187"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_pred_gini, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_knn = neigh.predict(X_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7300242130750605"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train_pred_knn, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_knn = neigh.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7431313386813134"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_pred_knn, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm_forest_clf = RandomForestClassifier(n_estimators=90, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdm_forest_clf.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_rdm_forest = rdm_forest_clf.predict(X_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6411622276029055"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train_pred_rdm_forest, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_rdm_forest = rdm_forest_clf.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8152266869022669"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_pred_rdm_forest, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': 90}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_boost_clf = GradientBoostingClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_boost_clf.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_gb = grad_boost_clf.predict(X_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7968523002421307"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train_pred_gb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_gb = grad_boost_clf.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.766408301414083"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_pred_gb, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_param_grid = {\n",
    "    'base_estimator': [DecisionTreeClassifier(max_depth=2)],\n",
    "    'n_estimators': [90],\n",
    "    'learning_rate': [1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_grid_search = GridSearchCV(AdaBoostClassifier(), n_jobs=8, \n",
    "                               param_grid=ada_param_grid, cv=3, \n",
    "                               scoring='accuracy', verbose=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   3 | elapsed:   20.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   3 | elapsed:   20.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "       fit_params=None, iid='warn', n_jobs=8,\n",
       "       param_grid={'base_estimator': [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')], 'n_estimators': [90], 'learning_rate': [1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=4)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_grid_search.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), 'learning_rate': 1, 'n_estimators': 90},\n",
       " 0.5513317191283293)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_grid_search.best_params_, ada_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_ada = ada_grid_search.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7022566307725663"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_test_pred_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_clf = MLPClassifier(learning_rate_init=0.001, max_iter=1000, activation='tanh')\n",
    "mlp_clf.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_mlp = mlp_clf.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6644654428946545"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_pred_mlp, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rush hours: 6:30 AM - 9:00 AM (idx 78 - idx 108), 3:30 PM - 6:30 PM (idx 186 - idx 222)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "rush_hours_idx = list(range(78, 109)) + list(range(186, 223))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_thresholds = [2 ** i for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection rate (DR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DR(y_test, y_test_pred, PT_thresholds, num_dt_segments):\n",
    "    DRs = []\n",
    "    for PT_threshold in PT_thresholds:\n",
    "        num_detected_incidents = 0\n",
    "        total_num_incidents = 0\n",
    "        for i in range(num_dt_segments):\n",
    "            base_idx = i * 274\n",
    "            max_base_offset = (i + 1) * 274\n",
    "            start_idx = base_idx\n",
    "            end_idx = start_idx\n",
    "            while start_idx < max_base_offset:\n",
    "                while start_idx < max_base_offset and y_test[start_idx] == -1:\n",
    "                    start_idx += 1\n",
    "                if start_idx == max_base_offset:\n",
    "                    break\n",
    "                # an incident happens\n",
    "                # time span of the incident\n",
    "                end_idx = start_idx\n",
    "                while end_idx < max_base_offset and y_test[end_idx] == 1:\n",
    "                    end_idx += 1\n",
    "                if (end_idx - start_idx >= PT_threshold):\n",
    "                    total_num_incidents += 1\n",
    "                    # the incident is detected\n",
    "                    if 1 in y_test_pred[start_idx:end_idx]:\n",
    "                        num_detected_incidents += 1\n",
    "\n",
    "                start_idx = end_idx + 1\n",
    "        DRs.append(round(num_detected_incidents / total_num_incidents * 100, 2))\n",
    "        print(\"PT_{}\".format(PT_threshold))\n",
    "        print(\"# of detected incidents: {}\".format(num_detected_incidents))\n",
    "        print(\"Total # of incidents: {}\".format(total_num_incidents))\n",
    "        print(\"Detection rate: {}\".format(DRs[-1]))\n",
    "    return DRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_1\n",
      "# of detected incidents: 58\n",
      "Total # of incidents: 125\n",
      "Detection rate: 46.4\n",
      "PT_2\n",
      "# of detected incidents: 57\n",
      "Total # of incidents: 119\n",
      "Detection rate: 47.9\n",
      "PT_4\n",
      "# of detected incidents: 50\n",
      "Total # of incidents: 105\n",
      "Detection rate: 47.62\n"
     ]
    }
   ],
   "source": [
    "DRs = DR(y_test, y_test_pred, PT_thresholds[0:3], num_dt_segments)\n",
    "results += \"SVM DR: {}\\n\".format(DRs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_1\n",
      "# of detected incidents: 102\n",
      "Total # of incidents: 125\n",
      "Detection rate: 81.6\n",
      "PT_2\n",
      "# of detected incidents: 99\n",
      "Total # of incidents: 119\n",
      "Detection rate: 83.19\n",
      "PT_4\n",
      "# of detected incidents: 90\n",
      "Total # of incidents: 105\n",
      "Detection rate: 85.71\n"
     ]
    }
   ],
   "source": [
    "DRs = DR(y_test, y_test_pred_gini, PT_thresholds[0:3], num_dt_segments)\n",
    "results += \"DT DR: {}\\n\".format(DRs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_1\n",
      "# of detected incidents: 87\n",
      "Total # of incidents: 125\n",
      "Detection rate: 69.6\n",
      "PT_2\n",
      "# of detected incidents: 86\n",
      "Total # of incidents: 119\n",
      "Detection rate: 72.27\n",
      "PT_4\n",
      "# of detected incidents: 82\n",
      "Total # of incidents: 105\n",
      "Detection rate: 78.1\n"
     ]
    }
   ],
   "source": [
    "DRs = DR(y_test, y_test_pred_knn, PT_thresholds[0:3], num_dt_segments)\n",
    "results += \"KNN DR: {}\\n\".format(DRs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_1\n",
      "# of detected incidents: 67\n",
      "Total # of incidents: 125\n",
      "Detection rate: 53.6\n",
      "PT_2\n",
      "# of detected incidents: 63\n",
      "Total # of incidents: 119\n",
      "Detection rate: 52.94\n",
      "PT_4\n",
      "# of detected incidents: 56\n",
      "Total # of incidents: 105\n",
      "Detection rate: 53.33\n"
     ]
    }
   ],
   "source": [
    "DRs = DR(y_test, y_test_pred_rdm_forest, PT_thresholds[0:3], num_dt_segments)\n",
    "results += \"RF DR: {}\\n\".format(DRs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_1\n",
      "# of detected incidents: 90\n",
      "Total # of incidents: 125\n",
      "Detection rate: 72.0\n",
      "PT_2\n",
      "# of detected incidents: 87\n",
      "Total # of incidents: 119\n",
      "Detection rate: 73.11\n",
      "PT_4\n",
      "# of detected incidents: 79\n",
      "Total # of incidents: 105\n",
      "Detection rate: 75.24\n"
     ]
    }
   ],
   "source": [
    "DRs = DR(y_test, y_test_pred_gb, PT_thresholds[0:3], num_dt_segments)\n",
    "results += \"GB DR: {}\\n\".format(DRs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_1\n",
      "# of detected incidents: 96\n",
      "Total # of incidents: 125\n",
      "Detection rate: 76.8\n",
      "PT_2\n",
      "# of detected incidents: 94\n",
      "Total # of incidents: 119\n",
      "Detection rate: 78.99\n",
      "PT_4\n",
      "# of detected incidents: 85\n",
      "Total # of incidents: 105\n",
      "Detection rate: 80.95\n"
     ]
    }
   ],
   "source": [
    "DRs = DR(y_test, y_test_pred_ada, PT_thresholds[0:3], num_dt_segments)\n",
    "results += \"AB DR: {}\\n\".format(DRs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_1\n",
      "# of detected incidents: 102\n",
      "Total # of incidents: 125\n",
      "Detection rate: 81.6\n",
      "PT_2\n",
      "# of detected incidents: 100\n",
      "Total # of incidents: 119\n",
      "Detection rate: 84.03\n",
      "PT_4\n",
      "# of detected incidents: 93\n",
      "Total # of incidents: 105\n",
      "Detection rate: 88.57\n"
     ]
    }
   ],
   "source": [
    "DRs = DR(y_test, y_test_pred_mlp, PT_thresholds[0:3], num_dt_segments)\n",
    "results += \"MLP DR: {}\\n\".format(DRs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean time to detect (MTTD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MTTD(y_test, y_test_pred, PT_thresholds, num_dt_segments):\n",
    "    MTTDs = []\n",
    "    for PT_threshold in PT_thresholds:\n",
    "        h = 0\n",
    "        sum_ttd = 0\n",
    "        for i in range(num_dt_segments):\n",
    "            base_idx = i * 274\n",
    "            max_base_offset = (i + 1) * 274\n",
    "            start_idx = base_idx\n",
    "            end_idx = start_idx\n",
    "            while start_idx < max_base_offset:\n",
    "                while start_idx < max_base_offset and y_test[start_idx] == -1:\n",
    "                    start_idx += 1\n",
    "                if start_idx == max_base_offset:\n",
    "                    break\n",
    "                # an incident happens\n",
    "                # time span of the incident\n",
    "                end_idx = start_idx\n",
    "                while end_idx < max_base_offset and y_test[end_idx] == 1:\n",
    "                    end_idx += 1\n",
    "\n",
    "                if end_idx - start_idx >= PT_threshold:\n",
    "                    # the incident is detected\n",
    "                    if 1 in y_test_pred[start_idx:end_idx]:\n",
    "                        h += 1\n",
    "                        incident_idx = start_idx\n",
    "                        detection_idx = incident_idx\n",
    "                        while y_test_pred[detection_idx] == -1:\n",
    "                            detection_idx += 1\n",
    "                        sum_ttd += (detection_idx - incident_idx) * 5\n",
    "\n",
    "                start_idx = end_idx + 1\n",
    "        MTTDs.append(round(sum_ttd / h, 2))\n",
    "\n",
    "        print(\"PT_{}\".format(PT_threshold))\n",
    "        print(\"Total number of detected incidents: {}\".format(h))\n",
    "        print(\"Total time of detection lags (min): {}\".format(sum_ttd))\n",
    "        print(\"Mean time to detect (MTTD): {}\".format(MTTDs[-1]))\n",
    "    return MTTDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_1\n",
      "Total number of detected incidents: 58\n",
      "Total time of detection lags (min): 350\n",
      "Mean time to detect (MTTD): 6.03\n",
      "PT_2\n",
      "Total number of detected incidents: 57\n",
      "Total time of detection lags (min): 350\n",
      "Mean time to detect (MTTD): 6.14\n",
      "PT_4\n",
      "Total number of detected incidents: 50\n",
      "Total time of detection lags (min): 335\n",
      "Mean time to detect (MTTD): 6.7\n"
     ]
    }
   ],
   "source": [
    "MTTDs = MTTD(y_test, y_test_pred, PT_thresholds[0:3], num_dt_segments)\n",
    "results += \"SVM MTTD: {}\\n\".format(MTTDs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_1\n",
      "Total number of detected incidents: 102\n",
      "Total time of detection lags (min): 765\n",
      "Mean time to detect (MTTD): 7.5\n",
      "PT_2\n",
      "Total number of detected incidents: 99\n",
      "Total time of detection lags (min): 765\n",
      "Mean time to detect (MTTD): 7.73\n",
      "PT_4\n",
      "Total number of detected incidents: 90\n",
      "Total time of detection lags (min): 750\n",
      "Mean time to detect (MTTD): 8.33\n"
     ]
    }
   ],
   "source": [
    "MTTDs = MTTD(y_test, y_test_pred_gini, PT_thresholds[0:3], num_dt_segments)\n",
    "results += \"DT MTTD: {}\\n\".format(MTTDs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_1\n",
      "Total number of detected incidents: 87\n",
      "Total time of detection lags (min): 825\n",
      "Mean time to detect (MTTD): 9.48\n",
      "PT_2\n",
      "Total number of detected incidents: 86\n",
      "Total time of detection lags (min): 825\n",
      "Mean time to detect (MTTD): 9.59\n",
      "PT_4\n",
      "Total number of detected incidents: 82\n",
      "Total time of detection lags (min): 810\n",
      "Mean time to detect (MTTD): 9.88\n"
     ]
    }
   ],
   "source": [
    "MTTDs = MTTD(y_test, y_test_pred_knn, PT_thresholds[0:3], num_dt_segments)\n",
    "results += \"KNN MTTD: {}\\n\".format(MTTDs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_1\n",
      "Total number of detected incidents: 67\n",
      "Total time of detection lags (min): 365\n",
      "Mean time to detect (MTTD): 5.45\n",
      "PT_2\n",
      "Total number of detected incidents: 63\n",
      "Total time of detection lags (min): 365\n",
      "Mean time to detect (MTTD): 5.79\n",
      "PT_4\n",
      "Total number of detected incidents: 56\n",
      "Total time of detection lags (min): 360\n",
      "Mean time to detect (MTTD): 6.43\n"
     ]
    }
   ],
   "source": [
    "MTTDs = MTTD(y_test, y_test_pred_rdm_forest, PT_thresholds[0:3], num_dt_segments)\n",
    "results += \"RF MTTD: {}\\n\".format(MTTDs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_1\n",
      "Total number of detected incidents: 90\n",
      "Total time of detection lags (min): 700\n",
      "Mean time to detect (MTTD): 7.78\n",
      "PT_2\n",
      "Total number of detected incidents: 87\n",
      "Total time of detection lags (min): 700\n",
      "Mean time to detect (MTTD): 8.05\n",
      "PT_4\n",
      "Total number of detected incidents: 79\n",
      "Total time of detection lags (min): 695\n",
      "Mean time to detect (MTTD): 8.8\n"
     ]
    }
   ],
   "source": [
    "MTTDs = MTTD(y_test, y_test_pred_gb, PT_thresholds[0:3], num_dt_segments)\n",
    "results += \"GB MTTD: {}\\n\".format(MTTDs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_1\n",
      "Total number of detected incidents: 96\n",
      "Total time of detection lags (min): 700\n",
      "Mean time to detect (MTTD): 7.29\n",
      "PT_2\n",
      "Total number of detected incidents: 94\n",
      "Total time of detection lags (min): 700\n",
      "Mean time to detect (MTTD): 7.45\n",
      "PT_4\n",
      "Total number of detected incidents: 85\n",
      "Total time of detection lags (min): 680\n",
      "Mean time to detect (MTTD): 8.0\n"
     ]
    }
   ],
   "source": [
    "MTTDs = MTTD(y_test, y_test_pred_ada, PT_thresholds[0:3], num_dt_segments)\n",
    "results += \"AB MTTD: {}\\n\".format(MTTDs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_1\n",
      "Total number of detected incidents: 102\n",
      "Total time of detection lags (min): 535\n",
      "Mean time to detect (MTTD): 5.25\n",
      "PT_2\n",
      "Total number of detected incidents: 100\n",
      "Total time of detection lags (min): 535\n",
      "Mean time to detect (MTTD): 5.35\n",
      "PT_4\n",
      "Total number of detected incidents: 93\n",
      "Total time of detection lags (min): 525\n",
      "Mean time to detect (MTTD): 5.65\n"
     ]
    }
   ],
   "source": [
    "MTTDs = MTTD(y_test, y_test_pred_mlp, PT_thresholds[0:3], num_dt_segments)\n",
    "results += \"MLP MTTD: {}\\n\".format(MTTDs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False positive rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{FP}{N} = \\frac{FP}{FP + TN}, $$ where $FP$ = False positive, and $TN$ = True negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FP(y_test, y_test_pred):\n",
    "    num_false_positives = 0\n",
    "    num_true_negatives = 0\n",
    "    total_num_detections = len(y_test_pred)\n",
    "    for i in range(total_num_detections):\n",
    "        if y_test_pred[i] == 1 and y_test[i] == -1:\n",
    "            num_false_positives += 1\n",
    "        elif y_test_pred[i] == -1 and y_test[i] == -1:\n",
    "            num_true_negatives += 1\n",
    "    print(\"The false positive rate is {}.\".format(round(num_false_positives / (num_false_positives + num_true_negatives) * 100, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The false positive rate is 13.063.\n"
     ]
    }
   ],
   "source": [
    "FP(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The false positive rate is 32.5635.\n"
     ]
    }
   ],
   "source": [
    "FP(y_test, y_test_pred_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The false positive rate is 25.5361.\n"
     ]
    }
   ],
   "source": [
    "FP(y_test, y_test_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The false positive rate is 18.3065.\n"
     ]
    }
   ],
   "source": [
    "FP(y_test, y_test_pred_rdm_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The false positive rate is 23.2156.\n"
     ]
    }
   ],
   "source": [
    "FP(y_test, y_test_pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The false positive rate is 33.4684.\n"
     ]
    }
   ],
   "source": [
    "FP(y_test, y_test_pred_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False alarm rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_alarm_rate(y_test, y_test_pred):\n",
    "    num_false_alarms = 0\n",
    "    total_num_detections = len(y_test_pred)\n",
    "    for i in range(total_num_detections):\n",
    "        if y_test_pred[i] != y_test[i]:\n",
    "            num_false_alarms += 1\n",
    "    far = round(num_false_alarms / total_num_detections * 100, 4)\n",
    "    print(\"The false alarm rate is {}.\".format(far))\n",
    "    return far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The false alarm rate is 13.2805.\n"
     ]
    }
   ],
   "source": [
    "far = false_alarm_rate(y_test, y_test_pred)\n",
    "results += \"SVM FAR: {}\\n\".format(far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The false alarm rate is 32.6588.\n"
     ]
    }
   ],
   "source": [
    "far = false_alarm_rate(y_test, y_test_pred_gini)\n",
    "results += \"DT FAR: {}\\n\".format(far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The false alarm rate is 25.6869.\n"
     ]
    }
   ],
   "source": [
    "far = false_alarm_rate(y_test, y_test_pred_knn)\n",
    "results += \"KNN FAR: {}\\n\".format(far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The false alarm rate is 18.4773.\n"
     ]
    }
   ],
   "source": [
    "far = false_alarm_rate(y_test, y_test_pred_rdm_forest)\n",
    "results += \"RF FAR: {}\\n\".format(far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The false alarm rate is 23.3592.\n"
     ]
    }
   ],
   "source": [
    "far = false_alarm_rate(y_test, y_test_pred_gb)\n",
    "results += \"GB FAR: {}\\n\".format(far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The false alarm rate is 29.7743.\n"
     ]
    }
   ],
   "source": [
    "far = false_alarm_rate(y_test, y_test_pred_ada)\n",
    "results += \"AB FAR: {}\\n\".format(far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The false alarm rate is 33.5535.\n"
     ]
    }
   ],
   "source": [
    "far = false_alarm_rate(y_test, y_test_pred_mlp)\n",
    "results += \"MLP FAR: {}\\n\".format(far)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(output_path, eval_result):\n",
    "    flag = \"w\"\n",
    "    if os.path.isfile(output_path):\n",
    "        flag = \"a\"\n",
    "    \n",
    "    with open(output_path, flag) as f:\n",
    "        f.write(\"{} \\n\".format(eval_result))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result('./res_feature.txt', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
